{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"FlowKit","text":"<p>FlowKit is a powerful flow orchestration toolkit that enables distributed task execution using a coordinator/worker architecture with Kafka as the message backplane.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Distributed Task Execution: Coordinate complex workflows across multiple workers</li> <li>Kafka-based Messaging: Reliable, scalable communication between components</li> <li>MongoDB Persistence: Durable task state and artifact storage</li> <li>Flexible DAG Support: Define complex task dependencies and fan-in/fan-out patterns</li> <li>Fault Tolerance: Built-in retry mechanisms, worker failure detection, and task recovery</li> <li>Streaming Processing: Support for streaming data through pipeline stages</li> <li>Testing Infrastructure: Comprehensive test helpers for building reliable workflows</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>import asyncio\nfrom flowkit import Coordinator, CoordinatorConfig, WorkerConfig\nfrom flowkit.worker import Worker\nfrom your_handlers import MyHandler\n\n# Start a coordinator\nasync def main():\n    # Configure and start coordinator\n    coord_config = CoordinatorConfig()\n    coordinator = Coordinator(db=your_mongo_db, cfg=coord_config)\n    await coordinator.start()\n\n    # Configure and start worker\n    worker_config = WorkerConfig(roles=[\"processor\"])\n    worker = Worker(\n        db=your_mongo_db,\n        cfg=worker_config,\n        handlers={\"processor\": MyHandler()}\n    )\n    await worker.start()\n\n    # Create a task with a simple graph\n    graph = {\n        \"nodes\": [\n            {\n                \"node_id\": \"process_data\",\n                \"type\": \"processor\",\n                \"io\": {\"input_inline\": {\"data\": \"hello world\"}}\n            }\n        ],\n        \"edges\": []\n    }\n\n    task_id = await coordinator.create_task(params={}, graph=graph)\n    print(f\"Created task: {task_id}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"#architecture-overview","title":"Architecture Overview","text":"<p>FlowKit consists of three main components:</p> <ol> <li>Coordinator: Orchestrates task execution, manages DAG scheduling, and monitors worker health</li> <li>Worker: Executes individual tasks and reports progress back to coordinators</li> <li>Message Bus: Kafka-based communication layer for reliable message delivery</li> </ol> <pre><code>graph TB\n    C[Coordinator] --&gt; K[Kafka]\n    W1[Worker 1] --&gt; K\n    W2[Worker 2] --&gt; K\n    W3[Worker N] --&gt; K\n    C --&gt; M[(MongoDB)]\n    W1 --&gt; M\n    W2 --&gt; M\n    W3 --&gt; M</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":"<ol> <li>Install FlowKit</li> <li>Follow the Quick Start guide</li> <li>Learn the basic concepts</li> <li>Explore examples</li> </ol>"},{"location":"#use-cases","title":"Use Cases","text":"<ul> <li>ETL Pipelines: Extract, transform, and load data across multiple stages</li> <li>ML Workflows: Orchestrate model training, validation, and deployment</li> <li>Data Processing: Handle large-scale data processing with fault tolerance</li> <li>Microservice Orchestration: Coordinate complex business processes</li> <li>Event-Driven Architecture: Build reactive systems with reliable message handling</li> </ul>"},{"location":"development/architecture/","title":"Architecture (WIP)","text":"<p>Coordinator/Worker/Outbox/Protocol components and flows. Placeholder.</p>"},{"location":"development/contributing/","title":"Contributing to FlowKit\u2011Async","text":"<p>Thank you for taking the time to contribute! This guide sets the ground rules for code, tests, docs, and CI so that changes stay reliable and fast.</p> <p>Short on time? See the TL;DR checklist below.</p>"},{"location":"development/contributing/#tldr-checklist","title":"TL;DR checklist","text":"<ul> <li>Use Python 3.11 or 3.12.</li> <li>Install dev deps: <code>pip install -e .[test]</code>.</li> <li>Run locally before pushing:</li> <li><code>ruff check src tests</code> (lint)</li> <li><code>black --check src tests</code> (format)</li> <li><code>mypy src</code> (types)</li> <li><code>pytest -n auto -q</code> (tests, parallel)</li> <li>Write Google\u2011style docstrings for public APIs; don\u2019t duplicate types already in annotations.</li> <li>Prefer precise typing (<code>collections.abc</code>, <code>dict[str, Any]</code> only at edges).</li> <li>Async code must be cancellation\u2011safe and non\u2011blocking.</li> <li>Keep PRs small and focused; link issues; add tests &amp; docs.</li> </ul>"},{"location":"development/contributing/#local-setup","title":"Local setup","text":"<pre><code>python -m venv .venv\nsource .venv/bin/activate            # Windows: .venv\\Scripts\\activate\npython -m pip install --upgrade pip\npip install -e .[test]\n# optional but recommended\npip install pre-commit &amp;&amp; pre-commit install\n</code></pre>"},{"location":"development/contributing/#useful-commands","title":"Useful commands","text":"<pre><code>ruff check src tests\nblack --check src tests\nmypy src\npytest -n auto -q\npytest -n auto --count=10 -q  # stress/flakiness check\nmkdocs serve                   # docs preview on http://localhost:8000\n</code></pre>"},{"location":"development/contributing/#branching-commits-and-prs","title":"Branching, commits, and PRs","text":"<ul> <li>Branch names: <code>feature/&lt;slug&gt;</code>, <code>fix/&lt;slug&gt;</code>, <code>chore/&lt;slug&gt;</code>.</li> <li>Keep PRs &lt; ~400 lines of diff when possible; split large changes.</li> <li>Reference issues (<code>Fixes #123</code>) and include tests.</li> <li>Commit messages: Conventional Commits recommended (not strictly required):</li> <li><code>feat(worker): cooperative cancellation for adapter</code></li> <li><code>fix(coordinator): fence duplicate lease renewals</code></li> <li><code>test(chaos): restart coordinator mid-stream</code></li> <li><code>docs(getting-started): clarify quickstart prerequisites</code></li> </ul>"},{"location":"development/contributing/#code-style","title":"Code style","text":""},{"location":"development/contributing/#docstrings-google-style","title":"Docstrings: Google style","text":"<p>Use Google\u2011style docstrings for public modules, classes, and functions. Don\u2019t repeat types already expressed in annotations.</p> <pre><code>from typing import Iterable\n\ndef stable_hash(items: Iterable[str]) -&gt; str:\n    \"\"\"Return a content\u2011stable hex digest for an iterable of strings.\n\n    Args:\n        items: Strings to hash in order.\n\n    Returns:\n        Hexadecimal SHA1 digest.\n    \"\"\"\n    ...\n</code></pre> <p>Sections you may use: Args, Returns, Raises, Yields, Examples, Notes. Keep docstrings concise and focused on behavior and invariants rather than implementation.</p>"},{"location":"development/contributing/#typing-policy","title":"Typing policy","text":"<ul> <li>Target Python 3.11+ syntax (<code>list[str]</code>, <code>X | None</code>, <code>typing.Self</code>).</li> <li>Prefer <code>collections.abc</code> for abstract containers: <code>Iterable</code>, <code>Mapping</code>, <code>Sequence</code>.</li> <li>Use <code>dict[str, Any]</code> only at system boundaries (I/O, JSON, DB). Convert to typed structures internally.</li> <li>For protocol payloads and wire messages use Pydantic v2 models (e.g., <code>Envelope</code>) to validate and serialize.</li> <li>For internal immutable data consider <code>@dataclass(frozen=True)</code>.</li> <li>Avoid <code>Any</code> unless there is a hard boundary. If used, document why.</li> <li>Always annotate async functions and return types explicitly.</li> </ul>"},{"location":"development/contributing/#formatting-linting","title":"Formatting &amp; linting","text":"<ul> <li>Black enforces formatting; Ruff enforces lint rules.</li> <li>Ruff is the source of truth for style checks; follow <code>ruff.toml</code> in the repo.</li> <li>Keep imports sorted (Ruff\u2019s <code>I</code> rules) and avoid unused code (<code>F401</code>, etc.).</li> <li>Lines should generally be \u2264 120 characters (CI enforces lints; docs may be stricter).</li> </ul>"},{"location":"development/contributing/#logging","title":"Logging","text":"<ul> <li>Prefer structured logs. Workers use <code>_json_log(clock, **kv)</code> for consistent, parseable output.</li> <li>Tests may use <code>tests.helpers.dbg()</code> for human\u2011readable traces.</li> </ul>"},{"location":"development/contributing/#async-cancellation","title":"Async &amp; cancellation","text":"<ul> <li>Never block the event loop. Use <code>await</code> for I/O and <code>asyncio.to_thread</code> for CPU\u2011bound work (sparingly).</li> <li>Handle <code>asyncio.CancelledError</code> by propagating (re\u2011raise) unless you are performing cleanup.</li> <li>Long\u2011running loops must check cancel flags (<code>ctx.cancel_flag.is_set()</code>) and yield (<code>await asyncio.sleep(\u2026)</code>).</li> <li>When catching broad exceptions, classify and surface them via handler error APIs (see <code>RoleHandler.classify_error</code>).</li> </ul>"},{"location":"development/contributing/#tests","title":"Tests","text":"<p>We use pytest with asyncio, randomized order, and parallelism.</p>"},{"location":"development/contributing/#quick-start","title":"Quick start","text":"<pre><code>pytest -n auto -q\n</code></pre>"},{"location":"development/contributing/#conventions","title":"Conventions","text":"<ul> <li>Name files <code>test_*.py</code> and mark async tests with <code>@pytest.mark.asyncio</code>.</li> <li>Use the helpers in <code>tests.helpers</code>:</li> <li>Graph helpers: <code>prime_graph</code>, <code>wait_task_finished</code>, <code>wait_node_running</code>, <code>node_by_id</code>.</li> <li>In-memory DB &amp; bus fixtures for isolated tests.</li> <li>Chaos/resiliency scenarios are marked with <code>@pytest.mark.chaos</code> and may use small sleeps; keep them deterministic otherwise.</li> <li>Every new feature/change should include unit tests; integration/chaos tests as applicable.</li> </ul>"},{"location":"development/contributing/#coverage","title":"Coverage","text":"<p>Coverage is reported in CI. Try to keep coverage steady or improving; focus on critical paths (coordinator scheduling, worker heartbeats, adapters, and cancellation).</p>"},{"location":"development/contributing/#ci-what-must-pass","title":"CI (what must pass)","text":"<p>The repository ships a CI workflow that runs on Linux with Python 3.11 and 3.12:</p> <ul> <li>Lint: <code>ruff</code>, <code>black --check</code>, <code>mypy</code>.</li> <li>Tests: <code>pytest -n auto --count=10</code> with timeouts and JUnit + coverage reports.</li> <li>Optional flake-hunt mode can repeat tests in random order to chase flakes.</li> <li>Docs build must succeed (<code>mkdocs build --strict</code>) in the docs workflow.</li> </ul> <p>If CI is red, please reproduce locally and push a fix or mark a flaky test with <code>@pytest.mark.flaky(reruns=...)</code> as a temporary measure with a tracking issue.</p>"},{"location":"development/contributing/#documentation","title":"Documentation","text":"<p>Docs live under <code>docs/</code> and are built with MkDocs (Material theme).</p> <ul> <li>Keep reference pages in sync when changing public APIs (protocol messages, coordinator/worker interfaces).</li> <li>Prefer short, task\u2011oriented pages. Put long rationale into <code>development/architecture.md</code>.</li> <li>Run <code>mkdocs serve</code> locally to preview. Do not commit the built <code>site/</code> folder; GitHub Actions deploys Pages automatically.</li> </ul>"},{"location":"development/contributing/#security-secrets","title":"Security &amp; secrets","text":"<ul> <li>Never commit credentials or tokens. Use environment variables in examples.</li> <li>Treat logs as potentially sensitive; avoid dumping full payloads unless sanitized.</li> </ul>"},{"location":"development/contributing/#design-principles-projectspecific","title":"Design principles (project\u2011specific)","text":"<ul> <li>Explicit states: transitions go through <code>RunState</code> and are visible in the task document.</li> <li>Exactly\u2011once intent for outbox/bus: deduplicate via <code>fp</code> (fingerprint), idempotent Kafka producers where applicable.</li> <li>Cooperative cancellation: make workers stop promptly and safely when signaled (signals topic, DB flags).</li> <li>Resilient batching: partial artifacts per batch, final \u201ccomplete\u201d artifact on finalize.</li> </ul>"},{"location":"development/contributing/#getting-help","title":"Getting help","text":"<ul> <li>Open a Draft PR early to discuss design.</li> <li>Use Discussions/Issues for proposals and bug reports.</li> <li>Ping maintainers in the PR if you need a review on a deadline.</li> </ul> <p>Thanks again for contributing \u2764\ufe0f</p>"},{"location":"development/github-pages/","title":"GitHub Pages Deployment Guide","text":"<p>This page explains reliable ways to publish the documentation site to GitHub Pages for the <code>valvarl/flowkit-async</code> project. It uses MkDocs (Material theme) and the official actions/deploy-pages workflow.</p> <p>If you are working in a fork, replace <code>valvarl</code> and <code>flowkit-async</code> with your own GitHub user/org and repository name.</p>"},{"location":"development/github-pages/#1-automatic-deployment-with-github-actions-recommended","title":"1) Automatic deployment with GitHub Actions (recommended)","text":"<p>The workflow below builds the site with MkDocs and publishes it to GitHub Pages on each push to the default branch. It also supports manual runs via workflow_dispatch.</p>"},{"location":"development/github-pages/#onetime-repo-settings","title":"One\u2011time repo settings","text":"<ol> <li>Open Settings \u2192 Pages.</li> <li>Under Build and deployment, set Source = GitHub Actions.</li> </ol>"},{"location":"development/github-pages/#add-workflow-githubworkflowsdocsyml","title":"Add workflow: <code>.github/workflows/docs.yml</code>","text":"<pre><code>name: Deploy Documentation\n\non:\n  push:\n    branches: [ main, master ]\n    paths:\n      - \"docs/**\"\n      - \"mkdocs.y*ml\"\n      - \"src/**\"\n      - \"pyproject.toml\"\n      - \"requirements*.txt\"\n  workflow_dispatch: {}\n\npermissions:\n  contents: read\n  pages: write\n  id-token: write\n\nconcurrency:\n  group: \"pages\"\n  cancel-in-progress: true\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Setup Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: \"3.12\"\n\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          # Install your package (if your docs import it) and doc toolchain\n          pip install -e .\n          pip install mkdocs mkdocs-material mkdocstrings[python]\n\n      - name: Build with MkDocs\n        run: |\n          mkdocs build --clean --strict\n\n      - name: Configure Pages\n        uses: actions/configure-pages@v5\n\n      - name: Upload artifact\n        uses: actions/upload-pages-artifact@v3\n        with:\n          path: ./site\n\n  deploy:\n    environment:\n      name: github-pages\n      url: ${{ steps.deployment.outputs.page_url }}\n    needs: build\n    runs-on: ubuntu-latest\n    steps:\n      - name: Deploy to GitHub Pages\n        id: deployment\n        uses: actions/deploy-pages@v4\n</code></pre> <p>After the first successful run, your docs will be available at:</p> <pre><code>https://valvarl.github.io/flowkit-async/\n</code></pre> <p>Tip: If your default branch is <code>develop</code> or <code>main</code> is protected, adjust the <code>on.push.branches</code> list accordingly.</p>"},{"location":"development/github-pages/#2-manual-deployment-alternative","title":"2) Manual deployment (alternative)","text":"<p>If you prefer to push directly to the <code>gh-pages</code> branch without Actions:</p>"},{"location":"development/github-pages/#prerequisites","title":"Prerequisites","text":"<pre><code>pip install mkdocs mkdocs-material mkdocstrings[python]\n</code></pre>"},{"location":"development/github-pages/#deploy","title":"Deploy","text":"<pre><code># Build and deploy to gh-pages (will create the branch if missing)\nmkdocs gh-deploy\n\n# With a custom message\nmkdocs gh-deploy -m \"Update documentation\"\n\n# Force re-deploy the current build (use with care)\nmkdocs gh-deploy --force\n</code></pre> <p>In Settings \u2192 Pages, set Source = Deploy from a branch and choose <code>gh-pages</code> / <code>/ (root)</code>.</p> <p>You should not use both the GitHub Actions deployment and <code>gh-deploy</code> at the same time. Pick one strategy to avoid confusion.</p>"},{"location":"development/github-pages/#3-local-development","title":"3) Local development","text":"<p>Use MkDocs\u2019 live-reload server while writing docs:</p> <pre><code># Start dev server on :8000\nmkdocs serve\n\n# Custom host/port\nmkdocs serve --dev-addr=0.0.0.0:8080\n\n# Watch extra paths (e.g., your package code)\nmkdocs serve --watch src --watch docs\n</code></pre> <p>The site will be available at http://localhost:8000 and will reload on changes.</p> <p>Build locally the same way Actions does:</p> <pre><code>mkdocs build --clean --strict\n</code></pre>"},{"location":"development/github-pages/#4-mkdocs-configuration-template","title":"4) MkDocs configuration template","text":"<p>Edit <code>mkdocs.yml</code> (or <code>mkdocs.yaml</code>) as needed. A minimal example aligned with this repository:</p> <pre><code>site_name: FlowKit Async\nsite_url: https://valvarl.github.io/flowkit-async\nrepo_url: https://github.com/valvarl/flowkit-async\nrepo_name: valvarl/flowkit-async\n\ntheme:\n  name: material\n  features:\n    - navigation.tracking\n    - navigation.expand\n    - content.code.copy\n\nnav:\n  - Home: index.md\n  - Getting Started:\n      - Quickstart: getting-started/quickstart.md\n  - Reference:\n      - Core: reference/core.md\n\nmarkdown_extensions:\n  - admonition\n  - toc:\n      permalink: true\n  - tables\n  - codehilite\n\nplugins:\n  - search\n  - mkdocstrings:\n      default_handler: python\n</code></pre> <p>Keep <code>site_url</code> in sync with your actual Pages URL. For forks, replace the owner/repo.</p>"},{"location":"development/github-pages/#custom-domain-optional","title":"Custom domain (optional)","text":"<ol> <li>Add a <code>CNAME</code> file inside the <code>docs/</code> directory so MkDocs copies it into <code>site/</code>:    <pre><code>echo \"docs.yourdomain.com\" &gt; docs/CNAME\n</code></pre></li> <li>Create a DNS CNAME record pointing <code>docs.yourdomain.com</code> \u2192 <code>valvarl.github.io</code> (or your username).</li> <li>Set <code>site_url</code> in <code>mkdocs.yml</code>:    <pre><code>site_url: https://docs.yourdomain.com\n</code></pre></li> </ol>"},{"location":"development/github-pages/#5-troubleshooting","title":"5) Troubleshooting","text":"<p>Pages shows 404 or outdated content - Make sure Settings \u2192 Pages \u2192 Source is set to GitHub Actions (for the workflow above) or <code>gh-pages</code> branch (for manual <code>gh-deploy</code>). - Check the latest workflow run logs; verify that <code>upload-pages-artifact</code> and <code>deploy-pages</code> completed successfully. - Confirm that <code>mkdocs build</code> produced <code>site/</code> with your expected pages.</p> <p>Build fails in Actions - Reproduce locally with:   <pre><code>mkdocs build --clean --strict\n</code></pre> - Ensure all doc dependencies are installed; if your docstrings import your package, install it with <code>pip install -e .</code>. - Validate configuration:   <pre><code>mkdocs config\n</code></pre></p> <p>Permission errors when deploying - The workflow must request:   <pre><code>permissions:\n  contents: read\n  pages: write\n  id-token: write\n</code></pre> - Also verify repo settings allow GitHub Pages for this branch/workflow.</p> <p>Link or image issues - Use relative links within <code>docs/</code> (MkDocs resolves them during build). - Place images under <code>docs/images/</code> and reference them relatively: <code>![Alt](images/diagram.png)</code>.</p>"},{"location":"development/github-pages/#6-best-practices","title":"6) Best practices","text":"<ol> <li>Preview locally before pushing:    <pre><code>mkdocs serve\n</code></pre></li> <li>Run strict builds in CI to catch warnings as errors:    <pre><code>mkdocs build --strict\n</code></pre></li> <li>Keep docs changes small and focused; review them in PRs.</li> <li>Avoid secrets in docs and examples; prefer environment variables where needed.</li> <li>Optimize images (lossless) to keep the site fast.</li> </ol>"},{"location":"development/github-pages/#7-url-recap","title":"7) URL recap","text":"<ul> <li>Site root: <code>https://valvarl.github.io/flowkit-async/</code></li> <li>If you add sections like \u201cGetting Started\u201d or \u201cReference\u201d, they will appear under that root according to your <code>nav</code> in <code>mkdocs.yml</code>.</li> </ul>"},{"location":"development/testing/","title":"Test Suite Reference","text":"<p>This page is generated from pytest docstrings at build time. If you see this placeholder, enable the generator plugin and rebuild docs.</p>"},{"location":"development/testing/#how-it-works","title":"How it works","text":"<p>We use mkdocs-gen-files to scan <code>tests/</code> and pull docstrings from test modules, classes, and functions, then render them as documentation. See <code>docs/_scripts/gen_tests_doc.py</code>.</p>"},{"location":"development/testing/#writing-good-test-docstrings","title":"Writing good test docstrings","text":"<pre><code>def test_example():\n    \"\"\"\n    Brief summary explaining what the test validates and why.\n    Mention key behaviors, invariants, or failure modes.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"examples/complex-workflows/","title":"Complex Workflows","text":"<p>This page shows composable patterns for larger pipelines. Below is a compact example that uses task variables to set SLA knobs, stream processing early, and collect metrics \u2014 without touching worker code.</p>"},{"location":"examples/complex-workflows/#vars-driven-sla-with-streaming-fan-in","title":"Vars-driven SLA with streaming fan-in","text":"<pre><code>{\n  \"nodes\": [\n    {\"node_id\": \"indexer\", \"type\": \"indexer\"},\n    {\"node_id\": \"proc\", \"type\": \"processor\", \"depends_on\": [\"indexer\"],\n      \"io\": {\"start_when\": \"first_batch\"}},\n    {\"node_id\": \"update_sla\", \"type\": \"coordinator_fn\", \"depends_on\": [\"indexer\"],\n      \"io\": {\"fn\": \"vars.merge\", \"fn_args\": {\"data\": {\"sla\": {\"max_delay\": 500}, \"flags\": {\"ab\": true}}}}},\n    {\"node_id\": \"agg_metrics\", \"type\": \"coordinator_fn\", \"depends_on\": [\"proc\"],\n      \"io\": {\"fn\": \"metrics.aggregate\", \"fn_args\": {\"node_id\": \"proc\", \"mode\": \"sum\"}}}\n  ],\n  \"edges\": [\n    [\"indexer\", \"proc\"],\n    [\"indexer\", \"update_sla\"],\n    [\"proc\", \"agg_metrics\"]\n  ],\n  \"edges_ex\": [\n    {\"from\": \"indexer\", \"to\": \"proc\", \"mode\": \"async\", \"trigger\": \"on_batch\"}\n  ]\n}\n</code></pre> <p>What happens</p> <ul> <li><code>update_sla</code> seeds <code>coordinator.vars.sla.max_delay</code> and a feature flag</li> <li><code>proc</code> can start as soon as <code>indexer</code> produces the first batch</li> <li><code>agg_metrics</code> aggregates raw metrics from <code>proc</code> into <code>graph.nodes[].stats</code></li> </ul>"},{"location":"examples/complex-workflows/#inspecting-coordinatorvars","title":"Inspecting <code>coordinator.vars</code>","text":"<p>A coordinator function (or an external diagnostic tool) can read the task document to act on <code>vars</code>:</p> <pre><code>task = await db.tasks.find_one({\"id\": task_id}, {\"coordinator\": 1})\nvars = ((task or {}).get(\"coordinator\") or {}).get(\"vars\") or {}\nassert isinstance(vars, dict)\n</code></pre>"},{"location":"examples/complex-workflows/#cleaning-up-vars","title":"Cleaning up vars","text":"<p>You can remove keys and prune empty objects with <code>vars.unset</code>:</p> <pre><code>{\n  \"node_id\": \"cleanup_vars\",\n  \"type\": \"coordinator_fn\",\n  \"io\": {\n    \"fn\": \"vars.unset\",\n    \"fn_args\": {\"keys\": [\"flags.ab\", \"sla.max_delay\"]}\n  }\n}\n</code></pre> <p>Empty parent objects are removed in a second pass by the adapter.</p>"},{"location":"examples/simple-pipeline/","title":"Simple Pipeline Example","text":"<p>This example demonstrates a basic three-stage ETL pipeline: Extract \u2192 Transform \u2192 Load.</p>"},{"location":"examples/simple-pipeline/#complete-example","title":"Complete Example","text":"<pre><code>import asyncio\nfrom typing import AsyncIterator\nfrom motor.motor_asyncio import AsyncIOMotorClient\n\nfrom flowkit import Coordinator, CoordinatorConfig, WorkerConfig\nfrom flowkit.worker import Worker\nfrom flowkit.worker.handlers.base import Handler, Batch, BatchResult, RunContext\n\n# Mock data source\nSAMPLE_DATA = [\n    {\"id\": 1, \"name\": \"Alice\", \"age\": 30, \"city\": \"New York\"},\n    {\"id\": 2, \"name\": \"Bob\", \"age\": 25, \"city\": \"San Francisco\"},\n    {\"id\": 3, \"name\": \"Charlie\", \"age\": 35, \"city\": \"Chicago\"},\n    {\"id\": 4, \"name\": \"Diana\", \"age\": 28, \"city\": \"Seattle\"},\n    {\"id\": 5, \"name\": \"Eve\", \"age\": 32, \"city\": \"Boston\"},\n]\n\nclass ExtractHandler(Handler):\n    \"\"\"Extract data from source.\"\"\"\n\n    async def iter_batches(self, input_data) -&gt; AsyncIterator[Batch]:\n        batch_size = input_data.get(\"input_inline\", {}).get(\"batch_size\", 2)\n\n        for i in range(0, len(SAMPLE_DATA), batch_size):\n            batch_data = SAMPLE_DATA[i:i + batch_size]\n            yield Batch(\n                batch_uid=f\"extract_batch_{i // batch_size}\",\n                payload={\"records\": batch_data}\n            )\n\n    async def process_batch(self, batch: Batch, ctx: RunContext) -&gt; BatchResult:\n        records = batch.payload[\"records\"]\n        print(f\"\ud83d\udce5 Extracted {len(records)} records\")\n\n        # Simulate extraction work\n        await asyncio.sleep(0.1)\n\n        return BatchResult(\n            success=True,\n            metrics={\"records_extracted\": len(records)},\n            artifacts_ref={\"data\": records, \"stage\": \"extract\"}\n        )\n\nclass TransformHandler(Handler):\n    \"\"\"Transform extracted data.\"\"\"\n\n    async def iter_batches(self, input_data) -&gt; AsyncIterator[Batch]:\n        # Use input adapter to pull from extract stage\n        input_args = input_data.get(\"input_inline\", {}).get(\"input_args\", {})\n        from_nodes = input_args.get(\"from_nodes\", [])\n\n        # In real implementation, this would pull from artifacts DB\n        # For demo, we'll simulate pulling transformed data\n        if \"extract\" in from_nodes:\n            # Simulate pulling from artifacts\n            for i in range(3):  # Mock 3 batches from extract\n                yield Batch(\n                    batch_uid=f\"transform_batch_{i}\",\n                    payload={\"batch_id\": i, \"from_extract\": True}\n                )\n\n    async def process_batch(self, batch: Batch, ctx: RunContext) -&gt; BatchResult:\n        print(f\"\ud83d\udd04 Transforming batch {batch.batch_uid}\")\n\n        # Simulate transformation: add computed fields\n        transformed_data = {\n            \"batch_id\": batch.payload.get(\"batch_id\"),\n            \"transformation\": \"added_full_name_and_category\",\n            \"processed_at\": ctx.clock.now_dt().isoformat()\n        }\n\n        await asyncio.sleep(0.15)  # Simulate work\n\n        return BatchResult(\n            success=True,\n            metrics={\"records_transformed\": 2},  # Simulated\n            artifacts_ref={\"transformed_data\": transformed_data, \"stage\": \"transform\"}\n        )\n\nclass LoadHandler(Handler):\n    \"\"\"Load transformed data to destination.\"\"\"\n\n    async def iter_batches(self, input_data) -&gt; AsyncIterator[Batch]:\n        # Pull from transform stage\n        input_args = input_data.get(\"input_inline\", {}).get(\"input_args\", {})\n        from_nodes = input_args.get(\"from_nodes\", [])\n\n        if \"transform\" in from_nodes:\n            # Simulate final loading batch\n            yield Batch(\n                batch_uid=\"load_final\",\n                payload={\"load_all\": True, \"from_transform\": True}\n            )\n\n    async def process_batch(self, batch: Batch, ctx: RunContext) -&gt; BatchResult:\n        print(f\"\ud83d\udcbe Loading data to destination\")\n\n        # Simulate loading to database/warehouse\n        await asyncio.sleep(0.2)\n\n        return BatchResult(\n            success=True,\n            metrics={\"records_loaded\": 5, \"load_time_ms\": 200},\n            artifacts_ref={\"load_status\": \"completed\", \"stage\": \"load\"}\n        )\n\nasync def run_etl_pipeline():\n    \"\"\"Run the complete ETL pipeline.\"\"\"\n\n    # Database setup\n    client = AsyncIOMotorClient(\"mongodb://localhost:27017\")\n    db = client.flowkit_examples\n\n    # Configuration\n    coord_config = CoordinatorConfig(\n        kafka_bootstrap=\"localhost:9092\",\n        worker_types=[\"extractor\", \"transformer\", \"loader\"],\n        scheduler_tick_sec=0.1  # Fast scheduling for demo\n    )\n\n    worker_config = WorkerConfig(\n        kafka_bootstrap=\"localhost:9092\",\n        roles=[\"extractor\", \"transformer\", \"loader\"]\n    )\n\n    # Start coordinator\n    coordinator = Coordinator(db=db, cfg=coord_config)\n    await coordinator.start()\n    print(\"\ud83d\ude80 Coordinator started\")\n\n    # Start worker with all handlers\n    worker = Worker(\n        db=db,\n        cfg=worker_config,\n        handlers={\n            \"extractor\": ExtractHandler(),\n            \"transformer\": TransformHandler(),\n            \"loader\": LoadHandler()\n        }\n    )\n    await worker.start()\n    print(\"\ud83d\ude80 Worker started\")\n\n    try:\n        # Define ETL graph\n        graph = {\n            \"nodes\": [\n                {\n                    \"node_id\": \"extract\",\n                    \"type\": \"extractor\",\n                    \"depends_on\": [],\n                    \"io\": {\n                        \"input_inline\": {\"batch_size\": 2}\n                    }\n                },\n                {\n                    \"node_id\": \"transform\",\n                    \"type\": \"transformer\",\n                    \"depends_on\": [\"extract\"],\n                    \"io\": {\n                        \"start_when\": \"first_batch\",  # Start as soon as extract produces data\n                        \"input_inline\": {\n                            \"input_adapter\": \"pull.from_artifacts\",\n                            \"input_args\": {\"from_nodes\": [\"extract\"]}\n                        }\n                    }\n                },\n                {\n                    \"node_id\": \"load\",\n                    \"type\": \"loader\",\n                    \"depends_on\": [\"transform\"],\n                    \"fan_in\": \"all\",  # Wait for transform to complete\n                    \"io\": {\n                        \"input_inline\": {\n                            \"input_adapter\": \"pull.from_artifacts\",\n                            \"input_args\": {\"from_nodes\": [\"transform\"]}\n                        }\n                    }\n                }\n            ],\n            \"edges\": [\n                [\"extract\", \"transform\"],\n                [\"transform\", \"load\"]\n            ]\n        }\n\n        # Create and run task\n        task_id = await coordinator.create_task(\n            params={\"pipeline_type\": \"etl_demo\"},\n            graph=graph\n        )\n        print(f\"\ud83d\udccb Created ETL task: {task_id}\")\n\n        # Monitor progress\n        print(\"\\\n\u23f3 Waiting for pipeline completion...\")\n        for i in range(30):  # Wait up to 30 seconds\n            task_doc = await db.tasks.find_one({\"id\": task_id})\n            status = task_doc.get(\"status\")\n\n            if status == \"finished\":\n                print(f\"\u2705 Pipeline completed successfully!\")\n                break\n            elif status == \"failed\":\n                print(f\"\u274c Pipeline failed\")\n                break\n\n            await asyncio.sleep(1)\n\n        # Show final results\n        task_doc = await db.tasks.find_one({\"id\": task_id})\n        nodes = {n[\"node_id\"]: n[\"status\"] for n in task_doc[\"graph\"][\"nodes\"]}\n        print(f\"\\\n\ud83d\udcca Final node statuses: {nodes}\")\n\n        # Show artifacts\n        artifacts = await db.artifacts.find({\"task_id\": task_id}).to_list(10)\n        print(f\"\ud83d\udce6 Total artifacts created: {len(artifacts)}\")\n\n        # Show metrics\n        metrics = await db.metrics_raw.find({\"task_id\": task_id}).to_list(20)\n        total_extracted = sum(m.get(\"metrics\", {}).get(\"records_extracted\", 0) for m in metrics)\n        total_transformed = sum(m.get(\"metrics\", {}).get(\"records_transformed\", 0) for m in metrics)\n        total_loaded = sum(m.get(\"metrics\", {}).get(\"records_loaded\", 0) for m in metrics)\n\n        print(f\"\\\n\ud83d\udcc8 Pipeline Metrics:\")\n        print(f\"  Records extracted: {total_extracted}\")\n        print(f\"  Records transformed: {total_transformed}\")\n        print(f\"  Records loaded: {total_loaded}\")\n\n    finally:\n        # Cleanup\n        await worker.stop()\n        await coordinator.stop()\n        client.close()\n        print(\"\\\n\u2705 Pipeline shutdown complete\")\n\nif __name__ == \"__main__\":\n    asyncio.run(run_etl_pipeline())\n</code></pre>"},{"location":"examples/simple-pipeline/#key-concepts-demonstrated","title":"Key Concepts Demonstrated","text":""},{"location":"examples/simple-pipeline/#1-handler-implementation","title":"1. Handler Implementation","text":"<p>Each stage implements the <code>Handler</code> interface with: - <code>iter_batches()</code>: Generates batches for processing - <code>process_batch()</code>: Processes individual batches - Returns <code>BatchResult</code> with success status and metrics</p>"},{"location":"examples/simple-pipeline/#2-input-adapters","title":"2. Input Adapters","text":"<p>The transform and load stages use <code>pull.from_artifacts</code> to get data from upstream stages:</p> <pre><code>\"input_inline\": {\n    \"input_adapter\": \"pull.from_artifacts\",\n    \"input_args\": {\"from_nodes\": [\"extract\"]}\n}\n</code></pre>"},{"location":"examples/simple-pipeline/#3-streaming-processing","title":"3. Streaming Processing","text":"<p>The transform stage starts as soon as extract produces its first batch:</p> <pre><code>\"io\": {\n    \"start_when\": \"first_batch\"\n}\n</code></pre>"},{"location":"examples/simple-pipeline/#4-dag-dependencies","title":"4. DAG Dependencies","text":"<p>The graph clearly defines the flow: extract \u2192 transform \u2192 load</p>"},{"location":"examples/simple-pipeline/#5-metrics-collection","title":"5. Metrics Collection","text":"<p>Each handler reports metrics that are aggregated for monitoring</p>"},{"location":"examples/simple-pipeline/#running-the-example","title":"Running the Example","text":"<ol> <li> <p>Start Kafka and MongoDB: <pre><code># Terminal 1: Kafka\ndocker run -d --name kafka -p 9092:9092 confluentinc/cp-kafka:latest\n\n# Terminal 2: MongoDB\ndocker run -d --name mongodb -p 27017:27017 mongo:6.0\n</code></pre></p> </li> <li> <p>Run the pipeline: <pre><code>python simple_pipeline.py\n</code></pre></p> </li> </ol> <p>Expected output: <pre><code>\ud83d\ude80 Coordinator started\n\ud83d\ude80 Worker started\n\ud83d\udccb Created ETL task: abc123...\n\u23f3 Waiting for pipeline completion...\n\ud83d\udce5 Extracted 2 records\n\ud83d\udce5 Extracted 2 records\n\ud83d\udce5 Extracted 1 records\n\ud83d\udd04 Transforming batch transform_batch_0\n\ud83d\udd04 Transforming batch transform_batch_1\n\ud83d\udd04 Transforming batch transform_batch_2\n\ud83d\udcbe Loading data to destination\n\u2705 Pipeline completed successfully!\n\ud83d\udcca Final node statuses: {'extract': 'finished', 'transform': 'finished', 'load': 'finished'}\n\ud83d\udce6 Total artifacts created: 7\n\ud83d\udcc8 Pipeline Metrics:\n  Records extracted: 5\n  Records transformed: 6\n  Records loaded: 5\n\u2705 Pipeline shutdown complete\n</code></pre></p>"},{"location":"examples/simple-pipeline/#next-steps","title":"Next Steps","text":"<ul> <li>Complex Workflows - Multi-path DAGs with fan-in/fan-out</li> <li>Testing Scenarios - How to test your pipelines</li> <li>Error Handling - Robust error handling patterns</li> </ul>"},{"location":"examples/testing/","title":"Testing Scenarios (WIP)","text":"<p>Chaos tests, resilience cases, concurrency &amp; lease checks. Placeholder.</p>"},{"location":"getting-started/concepts/","title":"Basic Concepts","text":"<p>Understanding FlowKit's core concepts will help you build robust, scalable workflows.</p>"},{"location":"getting-started/concepts/#core-components","title":"Core Components","text":""},{"location":"getting-started/concepts/#coordinator","title":"Coordinator","text":"<p>The Coordinator is the brain of FlowKit. It:</p> <ul> <li>Schedules tasks based on DAG dependencies</li> <li>Monitors worker health and task progress</li> <li>Handles task retries and failure recovery</li> <li>Manages task state transitions</li> <li>Coordinates multiple workers across different types</li> </ul> <pre><code>from flowkit import Coordinator, CoordinatorConfig\n\ncoordinator = Coordinator(\n    db=mongodb_instance,\n    cfg=CoordinatorConfig(worker_types=[\"indexer\", \"processor\"])\n)\n</code></pre>"},{"location":"getting-started/concepts/#worker","title":"Worker","text":"<p>A Worker executes individual tasks. Workers:</p> <ul> <li>Pull tasks from Kafka topics</li> <li>Execute custom handler logic</li> <li>Report progress and results back to coordinators</li> <li>Handle task cancellation and cleanup</li> <li>Support multiple roles/task types</li> </ul> <pre><code>from flowkit.worker import Worker, WorkerConfig\n\nworker = Worker(\n    db=mongodb_instance,\n    cfg=WorkerConfig(roles=[\"processor\"]),\n    handlers={\"processor\": MyProcessorHandler()}\n)\n</code></pre>"},{"location":"getting-started/concepts/#task-graph-dag","title":"Task Graph (DAG)","text":"<p>Tasks are organized into Directed Acyclic Graphs (DAGs) that define:</p> <ul> <li>Nodes: Individual processing steps</li> <li>Edges: Dependencies between nodes</li> <li>Fan-in/Fan-out: How data flows between stages</li> </ul> <pre><code>graph = {\n    \"nodes\": [\n        {\"node_id\": \"extract\", \"type\": \"extractor\", \"depends_on\": []},\n        {\"node_id\": \"transform\", \"type\": \"transformer\", \"depends_on\": [\"extract\"]},\n        {\"node_id\": \"load\", \"type\": \"loader\", \"depends_on\": [\"transform\"]}\n    ],\n    \"edges\": [[\"extract\", \"transform\"], [\"transform\", \"load\"]]\n}\n</code></pre>"},{"location":"getting-started/concepts/#key-concepts","title":"Key Concepts","text":""},{"location":"getting-started/concepts/#orchestrator-first-contract","title":"Orchestrator-first contract","text":"<p>For input routing, the Coordinator is the single source of truth. If <code>cmd.input_inline.input_adapter</code> is present, the worker must use it. Otherwise, it may use a handler-suggested adapter or fall back to <code>iter_batches</code>.</p> <p>This eliminates ambiguity and makes runs reproducible.</p>"},{"location":"getting-started/concepts/#handlers","title":"Handlers","text":"<p>Handlers define the actual processing logic for each task type:</p> <pre><code>from flowkit.worker.handlers.base import RoleHandler, Batch, BatchResult, FinalizeResult\n\nclass MyHandler(RoleHandler):\n    async def iter_batches(self, input_data) -&gt; AsyncIterator[Batch]:\n        # Generate batches from input\n        for item in input_data:\n            yield Batch(batch_uid=item.id, payload=item.data)\n\n    async def process_batch(self, batch: Batch, ctx: RunContext) -&gt; BatchResult:\n        # Process single batch\n        result = process_data(batch.payload)\n        return BatchResult(success=True, artifacts_ref=result)\n\n    async def finalize(self, ctx: RunContext) -&gt; FinalizeResult:\n        # Clean up after all batches processed\n        return FinalizeResult(metrics={\"total_processed\": ctx.processed_count})\n</code></pre>"},{"location":"getting-started/concepts/#artifacts","title":"Artifacts","text":"<p>Artifacts are the data outputs from each processing stage:</p> <ul> <li>Stored in MongoDB with metadata</li> <li>Can be partial (streaming) or complete</li> <li>Referenced by downstream stages</li> <li>Include processing metrics and status</li> </ul>"},{"location":"getting-started/concepts/#input-adapters","title":"Input Adapters","text":"<p>Input Adapters define how data flows between stages:</p> <pre><code>{\n    \"input_inline\": {\n    \"input_adapter\": \"pull.from_artifacts\",\n    \"input_args\": {\"from_nodes\": [\"upstream_node\"]}\n    }\n}\n</code></pre> <pre><code>{\n    \"input_inline\": {\n    \"input_adapter\": \"pull.from_artifacts.rechunk:size\",\n    \"input_args\": {\"from_nodes\": [\"upstream\"], \"size\": 10}\n    }\n}\n</code></pre> <p>Deterministic rechunking</p> <ul> <li>If <code>meta_list_key</code> is provided and <code>meta[meta_list_key]</code> is a list \u2192 chunk that list to <code>size</code>.</li> <li>Otherwise, each artifact meta is treated as one logical item (<code>items=[meta]</code>).</li> <li>No heuristics over domain keys (no <code>skus|items|\u2026</code> guessing).</li> </ul> <p>Aliases</p> <ul> <li><code>from_node</code> (single) is accepted and normalized to <code>from_nodes=[...]</code>.</li> </ul> <p>Empty upstream</p> <p>Valid routes with no data complete normally (<code>count=0</code>).</p>"},{"location":"getting-started/concepts/#task-states","title":"Task States","text":"<p>Tasks progress through several states:</p> <ul> <li>queued: Waiting to be scheduled</li> <li>running: Currently being processed</li> <li>deferred: Temporarily paused (e.g., for retry)</li> <li>finished: Completed successfully</li> <li>failed: Terminated with error</li> <li>cancelling: Being cancelled</li> </ul>"},{"location":"getting-started/concepts/#flow-control-patterns","title":"Flow Control Patterns","text":""},{"location":"getting-started/concepts/#fan-in-strategies","title":"Fan-in Strategies","text":"<p>Control when a node starts based on dependencies:</p> <pre><code>{\n    \"node_id\": \"combiner\",\n    \"depends_on\": [\"node1\", \"node2\", \"node3\"],\n    \"fan_in\": \"all\"\n    # \"fan_in\": \"any\"\n    # \"fan_in\": \"count:2\"\n}\n</code></pre>"},{"location":"getting-started/concepts/#streaming-vs-batch-processing","title":"Streaming vs Batch Processing","text":"<pre><code># Start processing as soon as first batch is available\n{\n    \"node_id\": \"processor\",\n    \"io\": {\"start_when\": \"first_batch\"}\n}\n\n# Wait for upstream to completely finish\n{\n    \"node_id\": \"processor\",\n    \"io\": {\"start_when\": \"ready\"}  # default\n}\n</code></pre>"},{"location":"getting-started/concepts/#coordinator-functions","title":"Coordinator Functions","text":"<p>Execute logic directly in the coordinator (no worker needed):</p> <pre><code>{\n    \"node_id\": \"merger\",\n    \"type\": \"coordinator_fn\",\n    \"io\": {\n        \"fn\": \"merge.generic\",\n        \"fn_args\": {\n            \"from_nodes\": [\"node1\", \"node2\"],\n            \"target\": {\"key\": \"merged_result\"}\n        }\n    }\n}\n</code></pre>"},{"location":"getting-started/concepts/#error-handling","title":"Error Handling","text":""},{"location":"getting-started/concepts/#retry-policies","title":"Retry Policies","text":"<pre><code>{\n    \"node_id\": \"flaky_task\",\n    \"retry_policy\": {\n        \"max\": 3,\n        \"backoff_sec\": 300,\n        \"permanent_on\": [\"bad_input\", \"schema_mismatch\"]\n    }\n}\n</code></pre>"},{"location":"getting-started/concepts/#error-classification","title":"Error Classification","text":"<p>Handlers can classify errors as permanent or transient:</p> <pre><code>def classify_error(self, error: Exception) -&gt; tuple[str, bool]:\n    if isinstance(error, ValidationError):\n        return \"validation_failed\", True  # Permanent - don't retry\n    else:\n        return \"temporary_failure\", False  # Transient - retry\n</code></pre> <p>Worker core also normalizes common configuration errors: - unknown adapter \u2192 <code>bad_input_adapter</code> (permanent), bad/missing args \u2192 <code>bad_input_args</code> (permanent).</p>"},{"location":"getting-started/concepts/#message-flow","title":"Message Flow","text":"<ol> <li>Coordinator publishes task commands to Kafka topics</li> <li>Workers consume from role-specific topics (e.g., <code>cmd.processor.v1</code>)</li> <li>Workers publish status updates to status topics</li> <li>Coordinator consumes status updates and updates task state</li> <li>Artifacts are stored in MongoDB for inter-stage communication</li> </ol>"},{"location":"getting-started/concepts/#scalability-patterns","title":"Scalability Patterns","text":""},{"location":"getting-started/concepts/#horizontal-scaling","title":"Horizontal Scaling","text":"<ul> <li>Run multiple coordinators (they coordinate through MongoDB)</li> <li>Run multiple workers of the same type for parallel processing</li> <li>Workers auto-discover tasks and distribute load</li> </ul>"},{"location":"getting-started/concepts/#partitioning","title":"Partitioning","text":"<ul> <li>Use Kafka partitioning for parallel processing</li> <li>Workers process partitions independently</li> <li>Results are merged by downstream stages</li> </ul>"},{"location":"getting-started/concepts/#resource-management","title":"Resource Management","text":"<ul> <li>Configure worker capacity limits</li> <li>Set concurrency limits per worker type</li> <li>Use backpressure mechanisms for flow control</li> </ul>"},{"location":"getting-started/concepts/#next-steps","title":"Next Steps","text":"<ul> <li>Set up Coordinators</li> <li>Create Custom Workers</li> <li>Design Task Graphs</li> <li>Handle Errors Gracefully</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.11+</li> <li>Apache Kafka</li> <li>MongoDB</li> <li>Docker (optional, for development)</li> </ul>"},{"location":"getting-started/installation/#install-flowkit","title":"Install FlowKit","text":""},{"location":"getting-started/installation/#from-pypi","title":"From PyPI","text":"<pre><code>pip install flowkit\n</code></pre>"},{"location":"getting-started/installation/#from-source","title":"From Source","text":"<pre><code>git clone https://github.com/your-org/flowkit.git\ncd flowkit\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>For development with all dependencies:</p> <pre><code>git clone https://github.com/your-org/flowkit.git\ncd flowkit\npip install -e \".[dev,test]\"\n</code></pre>"},{"location":"getting-started/installation/#infrastructure-setup","title":"Infrastructure Setup","text":""},{"location":"getting-started/installation/#apache-kafka","title":"Apache Kafka","text":"<p>FlowKit requires Kafka for message coordination. You can run Kafka locally using Docker:</p> <pre><code># Start Zookeeper and Kafka\ndocker run -d --name zookeeper -p 2181:2181 zookeeper:3.8\ndocker run -d --name kafka -p 9092:9092 \\\n    --link zookeeper:zookeeper \\\n    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \\\n    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092 \\\n    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \\\n    confluentinc/cp-kafka:latest\n</code></pre> <p>Or use the provided Docker Compose file:</p> <pre><code>cd flowkit\ndocker-compose up -d kafka\n</code></pre>"},{"location":"getting-started/installation/#mongodb","title":"MongoDB","text":"<p>FlowKit uses MongoDB for task state and artifact persistence:</p> <pre><code># Run MongoDB with Docker\ndocker run -d --name mongodb -p 27017:27017 mongo:6.0\n</code></pre> <p>Or with Docker Compose:</p> <pre><code>cd flowkit\ndocker-compose up -d mongodb\n</code></pre>"},{"location":"getting-started/installation/#configuration","title":"Configuration","text":"<p>Create configuration files for your coordinator and workers:</p>"},{"location":"getting-started/installation/#coordinator-configuration-coordinatorjson","title":"Coordinator Configuration (<code>coordinator.json</code>)","text":"<pre><code>{\n    \"kafka_bootstrap\": \"localhost:9092\",\n    \"worker_types\": [\"indexer\", \"processor\", \"analyzer\"],\n    \"heartbeat_soft_sec\": 300,\n    \"heartbeat_hard_sec\": 3600,\n    \"lease_ttl_sec\": 60\n}\n</code></pre>"},{"location":"getting-started/installation/#worker-configuration-workerjson","title":"Worker Configuration (<code>worker.json</code>)","text":"<pre><code>{\n    \"kafka_bootstrap\": \"localhost:9092\",\n    \"roles\": [\"processor\"],\n    \"worker_id\": null,\n    \"lease_ttl_sec\": 60,\n    \"hb_interval_sec\": 20\n}\n</code></pre>"},{"location":"getting-started/installation/#verification","title":"Verification","text":"<p>Test your installation:</p> <pre><code>import flowkit\nprint(f\"FlowKit version: {flowkit.__version__}\")\n\n# Test coordinator and worker imports\nfrom flowkit import Coordinator, CoordinatorConfig, WorkerConfig\nfrom flowkit.worker import Worker\nprint(\"FlowKit installed successfully!\")\n</code></pre>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>This guide will walk you through creating your first FlowKit pipeline in just a few minutes.</p>"},{"location":"getting-started/quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>FlowKit installed (Installation Guide)</li> <li>Kafka running on <code>localhost:9092</code></li> <li>MongoDB running on <code>localhost:27017</code></li> </ul>"},{"location":"getting-started/quickstart/#step-1-create-a-simple-handler","title":"Step 1: Create a Simple Handler","text":"<p>First, create a custom handler that will process your data:</p> <pre><code># handlers.py\nimport asyncio\nfrom typing import AsyncIterator\nfrom flowkit.worker.handlers.base import Handler, Batch, BatchResult, RunContext\n\nclass EchoHandler(Handler):\n    \"\"\"A simple handler that echoes input data.\"\"\"\n\n    async def iter_batches(self, input_data) -&gt; AsyncIterator[Batch]:\n        \"\"\"Generate batches from input data.\"\"\"\n        data = input_data.get(\"input_inline\", {}).get(\"message\", \"Hello World\")\n        yield Batch(\n            batch_uid=\"batch_1\",\n            payload={\"message\": data, \"processed_at\": \"now\"}\n        )\n\n    async def process_batch(self, batch: Batch, ctx: RunContext) -&gt; BatchResult:\n        \"\"\"Process a single batch.\"\"\"\n        message = batch.payload.get(\"message\", \"\")\n        processed_message = f\"Echo: {message.upper()}\"\n\n        print(f\"Processing: {processed_message}\")\n\n        # Simulate some work\n        await asyncio.sleep(0.1)\n\n        return BatchResult(\n            success=True,\n            metrics={\"messages_processed\": 1},\n            artifacts_ref={\"output\": processed_message}\n        )\n</code></pre>"},{"location":"getting-started/quickstart/#step-2-set-up-database-connection","title":"Step 2: Set Up Database Connection","text":"<pre><code># database.py\nfrom motor.motor_asyncio import AsyncIOMotorClient\n\nasync def get_database():\n    \"\"\"Get MongoDB database connection.\"\"\"\n    client = AsyncIOMotorClient(\"mongodb://localhost:27017\")\n    return client.flowkit_db\n</code></pre>"},{"location":"getting-started/quickstart/#step-3-create-your-first-pipeline","title":"Step 3: Create Your First Pipeline","text":"<pre><code># pipeline.py\nimport asyncio\nfrom flowkit import Coordinator, CoordinatorConfig, WorkerConfig\nfrom flowkit.worker import Worker\nfrom handlers import EchoHandler\nfrom database import get_database\n\nasync def run_pipeline():\n    # Get database connection\n    db = await get_database()\n\n    # Configure coordinator\n    coord_config = CoordinatorConfig(\n        kafka_bootstrap=\"localhost:9092\",\n        worker_types=[\"echo\"]\n    )\n\n    # Configure worker\n    worker_config = WorkerConfig(\n        kafka_bootstrap=\"localhost:9092\",\n        roles=[\"echo\"]\n    )\n\n    # Start coordinator\n    coordinator = Coordinator(db=db, cfg=coord_config)\n    await coordinator.start()\n    print(\"\u2705 Coordinator started\")\n\n    # Start worker\n    worker = Worker(\n        db=db,\n        cfg=worker_config,\n        handlers={\"echo\": EchoHandler()}\n    )\n    await worker.start()\n    print(\"\u2705 Worker started\")\n\n    try:\n        # Create a simple task graph\n        graph = {\n            \"nodes\": [\n                {\n                    \"node_id\": \"echo_task\",\n                    \"type\": \"echo\",\n                    \"depends_on\": [],\n                    \"io\": {\n                        \"input_inline\": {\n                            \"message\": \"Hello FlowKit!\"\n                        }\n                    }\n                }\n            ],\n            \"edges\": []\n        }\n\n        # Create and execute task\n        task_id = await coordinator.create_task(\n            params={\"pipeline_name\": \"quickstart\"},\n            graph=graph\n        )\n        print(f\"\u2705 Created task: {task_id}\")\n\n        # Wait for completion (in real apps, you'd monitor differently)\n        await asyncio.sleep(5)\n\n        # Check task status\n        task_doc = await db.tasks.find_one({\"id\": task_id})\n        print(f\"\u2705 Task status: {task_doc['status']}\")\n\n        # Check artifacts\n        artifacts = await db.artifacts.find({\"task_id\": task_id}).to_list(10)\n        print(f\"\u2705 Artifacts created: {len(artifacts)}\")\n\n    finally:\n        # Clean shutdown\n        await worker.stop()\n        await coordinator.stop()\n        print(\"\u2705 Pipeline shutdown complete\")\n\nif __name__ == \"__main__\":\n    asyncio.run(run_pipeline())\n</code></pre>"},{"location":"getting-started/quickstart/#step-4-run-your-pipeline","title":"Step 4: Run Your Pipeline","text":"<pre><code>python pipeline.py\n</code></pre> <p>Expected output: <pre><code>\u2705 Coordinator started\n\u2705 Worker started\n\u2705 Created task: 12345678-1234-1234-1234-123456789abc\nProcessing: Echo: HELLO FLOWKIT!\n\u2705 Task status: finished\n\u2705 Artifacts created: 1\n\u2705 Pipeline shutdown complete\n</code></pre></p>"},{"location":"getting-started/quickstart/#step-5-create-a-multi-stage-pipeline","title":"Step 5: Create a Multi-Stage Pipeline","text":"<p>Now let's create a more complex pipeline with multiple stages:</p> <pre><code># complex_pipeline.py\nimport asyncio\nfrom flowkit import Coordinator, CoordinatorConfig, WorkerConfig\nfrom flowkit.worker import Worker\nfrom handlers import EchoHandler\nfrom database import get_database\n\nclass ProcessorHandler(Handler):\n    \"\"\"Processes data from previous stage.\"\"\"\n\n    async def iter_batches(self, input_data) -&gt; AsyncIterator[Batch]:\n        # Use input adapter to pull from artifacts\n        input_adapter = input_data.get(\"input_inline\", {}).get(\"input_adapter\")\n        if input_adapter == \"pull.from_artifacts\":\n            # This would pull from upstream artifacts\n            # For simplicity, we'll simulate\n            yield Batch(\n                batch_uid=\"process_batch_1\",\n                payload={\"data\": \"processed_data\", \"stage\": \"processor\"}\n            )\n\n    async def process_batch(self, batch: Batch, ctx: RunContext) -&gt; BatchResult:\n        data = batch.payload.get(\"data\", \"\")\n        result = f\"Processed: {data}\"\n\n        print(f\"Stage 2 - {result}\")\n        await asyncio.sleep(0.1)\n\n        return BatchResult(\n            success=True,\n            metrics={\"items_processed\": 1},\n            artifacts_ref={\"result\": result}\n        )\n\nasync def run_complex_pipeline():\n    db = await get_database()\n\n    coord_config = CoordinatorConfig(\n        kafka_bootstrap=\"localhost:9092\",\n        worker_types=[\"echo\", \"processor\"]\n    )\n\n    worker_config = WorkerConfig(\n        kafka_bootstrap=\"localhost:9092\",\n        roles=[\"echo\", \"processor\"]\n    )\n\n    coordinator = Coordinator(db=db, cfg=coord_config)\n    await coordinator.start()\n\n    worker = Worker(\n        db=db,\n        cfg=worker_config,\n        handlers={\n            \"echo\": EchoHandler(),\n            \"processor\": ProcessorHandler()\n        }\n    )\n    await worker.start()\n\n    try:\n        # Multi-stage graph\n        graph = {\n            \"nodes\": [\n                {\n                    \"node_id\": \"stage1\",\n                    \"type\": \"echo\",\n                    \"depends_on\": [],\n                    \"io\": {\"input_inline\": {\"message\": \"Input data\"}}\n                },\n                {\n                    \"node_id\": \"stage2\",\n                    \"type\": \"processor\",\n                    \"depends_on\": [\"stage1\"],\n                    \"io\": {\n                        \"input_inline\": {\n                            \"input_adapter\": \"pull.from_artifacts\",\n                            \"input_args\": {\"from_nodes\": [\"stage1\"]}\n                        }\n                    }\n                }\n            ],\n            \"edges\": [[\"stage1\", \"stage2\"]]  # stage1 -&gt; stage2\n        }\n\n        task_id = await coordinator.create_task(params={}, graph=graph)\n        print(f\"\u2705 Multi-stage task created: {task_id}\")\n\n        await asyncio.sleep(10)  # Wait for completion\n\n        task_doc = await db.tasks.find_one({\"id\": task_id})\n        print(f\"\u2705 Final status: {task_doc['status']}\")\n\n    finally:\n        await worker.stop()\n        await coordinator.stop()\n\nif __name__ == \"__main__\":\n    asyncio.run(run_complex_pipeline())\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Basic Concepts</li> <li>Explore Task Graphs in detail</li> <li>Check out more Examples</li> <li>Read about Error Handling</li> </ul>"},{"location":"guide/configuration/","title":"Configuration (WIP)","text":"<p>Coordinator/Worker settings, environment overrides, and defaults. Placeholder.</p>"},{"location":"guide/coordinators/","title":"Coordinators","text":"<p>Coordinators are the orchestration layer of FlowKit, responsible for managing task execution, scheduling DAG nodes, and monitoring worker health.</p>"},{"location":"guide/coordinators/#overview","title":"Overview","text":"<p>A Coordinator:</p> <ul> <li>Schedules Tasks: Determines when nodes in a DAG are ready to execute</li> <li>Manages Workers: Tracks worker health and capacity</li> <li>Handles Failures: Implements retry policies and failure recovery</li> <li>Coordinates State: Maintains task state in MongoDB</li> <li>Routes Messages: Uses Kafka for reliable communication with workers</li> </ul>"},{"location":"guide/coordinators/#basic-setup","title":"Basic Setup","text":"<pre><code>from flowkit import Coordinator, CoordinatorConfig\nfrom motor.motor_asyncio import AsyncIOMotorClient\n\nasync def setup_coordinator():\n    # Database connection\n    client = AsyncIOMotorClient(\"mongodb://localhost:27017\")\n    db = client.flowkit\n\n    # Configuration\n    config = CoordinatorConfig(\n        kafka_bootstrap=\"localhost:9092\",\n        worker_types=[\"processor\", \"analyzer\", \"loader\"]\n    )\n\n    # Create and start coordinator\n    coordinator = Coordinator(db=db, cfg=config)\n    await coordinator.start()\n\n    return coordinator\n</code></pre>"},{"location":"guide/coordinators/#configuration-options","title":"Configuration Options","text":""},{"location":"guide/coordinators/#core-settings","title":"Core Settings","text":"<pre><code>config = CoordinatorConfig(\n    # Kafka configuration\n    kafka_bootstrap=\"localhost:9092\",\n    worker_types=[\"indexer\", \"processor\", \"analyzer\"],\n\n    # Topic naming patterns\n    topic_cmd_fmt=\"cmd.{type}.v1\",\n    topic_status_fmt=\"status.{type}.v1\",\n\n    # Timing settings (seconds)\n    heartbeat_soft_sec=300,      # Soft heartbeat timeout\n    heartbeat_hard_sec=3600,     # Hard heartbeat timeout\n    lease_ttl_sec=45,            # Worker lease duration\n    scheduler_tick_sec=1.0,      # Scheduling frequency\n\n    # Retry and backoff\n    cancel_grace_sec=30,         # Grace period for cancellation\n    outbox_max_retry=12,         # Max retry attempts\n    outbox_backoff_min_ms=250,   # Min backoff time\n    outbox_backoff_max_ms=60000, # Max backoff time\n)\n</code></pre>"},{"location":"guide/coordinators/#loading-from-file","title":"Loading from File","text":"<pre><code># Load from JSON file\nconfig = CoordinatorConfig.load(\"configs/coordinator.json\")\n\n# With overrides\nconfig = CoordinatorConfig.load(\n    \"configs/coordinator.json\",\n    overrides={\"scheduler_tick_sec\": 0.5}\n)\n\n# Environment variables\n# KAFKA_BOOTSTRAP_SERVERS, WORKER_TYPES are automatically loaded\n</code></pre>"},{"location":"guide/coordinators/#task-management","title":"Task Management","text":""},{"location":"guide/coordinators/#creating-tasks","title":"Creating Tasks","text":"<pre><code>async def create_pipeline_task(coordinator):\n    graph = {\n        \"nodes\": [\n            {\n                \"node_id\": \"extract\",\n                \"type\": \"extractor\",\n                \"depends_on\": [],\n                \"io\": {\"input_inline\": {\"source\": \"database\"}}\n            },\n            {\n                \"node_id\": \"transform\",\n                \"type\": \"processor\",\n                \"depends_on\": [\"extract\"],\n                \"io\": {\n                    \"input_inline\": {\n                        \"input_adapter\": \"pull.from_artifacts\",\n                        \"input_args\": {\"from_nodes\": [\"extract\"]}\n                    }\n                }\n            }\n        ],\n        \"edges\": [[\"extract\", \"transform\"]]\n    }\n\n    task_id = await coordinator.create_task(\n        params={\"pipeline_name\": \"etl_demo\"},\n        graph=graph\n    )\n\n    return task_id\n</code></pre>"},{"location":"guide/coordinators/#task-lifecycle","title":"Task Lifecycle","text":"<p>Tasks progress through these states:</p> <ol> <li>queued \u2192 Task created, waiting to be scheduled</li> <li>running \u2192 At least one node is executing</li> <li>finished \u2192 All nodes completed successfully</li> <li>failed \u2192 Task failed permanently</li> <li>deferred \u2192 Temporarily paused for retry</li> </ol>"},{"location":"guide/coordinators/#monitoring-tasks","title":"Monitoring Tasks","text":"<pre><code>async def monitor_task(coordinator, task_id):\n    # Get task document from database\n    task_doc = await coordinator.db.tasks.find_one({\"id\": task_id})\n\n    # Check overall status\n    print(f\"Task status: {task_doc['status']}\")\n\n    # Check individual node statuses\n    for node in task_doc[\"graph\"][\"nodes\"]:\n        print(f\"Node {node['node_id']}: {node['status']}\")\n\n    # Check artifacts\n    artifacts = await coordinator.db.artifacts.find(\n        {\"task_id\": task_id}\n    ).to_list(100)\n    print(f\"Artifacts created: {len(artifacts)}\")\n</code></pre>"},{"location":"guide/coordinators/#scheduling-behavior","title":"Scheduling Behavior","text":""},{"location":"guide/coordinators/#node-readiness","title":"Node Readiness","text":"<p>The coordinator schedules nodes when:</p> <ol> <li>Dependencies satisfied: All <code>depends_on</code> nodes are finished</li> <li>Fan-in condition met: Based on <code>fan_in</code> strategy</li> <li>Not already running: Node isn't currently being processed</li> <li>Retry conditions met: If deferred, retry time has passed</li> </ol>"},{"location":"guide/coordinators/#fan-in-strategies","title":"Fan-in Strategies","text":"<pre><code># Wait for all dependencies (default)\n{\n    \"node_id\": \"combiner\",\n    \"depends_on\": [\"node1\", \"node2\", \"node3\"],\n    \"fan_in\": \"all\"\n}\n\n# Start when any dependency completes\n{\n    \"node_id\": \"processor\",\n    \"depends_on\": [\"node1\", \"node2\"],\n    \"fan_in\": \"any\"\n}\n\n# Start when N dependencies complete\n{\n    \"node_id\": \"analyzer\",\n    \"depends_on\": [\"node1\", \"node2\", \"node3\"],\n    \"fan_in\": \"count:2\"\n}\n</code></pre>"},{"location":"guide/coordinators/#streaming-execution","title":"Streaming Execution","text":"<pre><code># Start as soon as upstream produces first batch\n{\n    \"node_id\": \"processor\",\n    \"depends_on\": [\"extractor\"],\n    \"io\": {\"start_when\": \"first_batch\"}\n}\n\n# Wait for upstream to completely finish (default)\n{\n    \"node_id\": \"loader\",\n    \"depends_on\": [\"processor\"],\n    \"io\": {\"start_when\": \"ready\"}\n}\n</code></pre>"},{"location":"guide/coordinators/#coordinator-functions","title":"Coordinator Functions","text":"<p>Coordinators can execute logic directly without workers:</p> <pre><code>{\n    \"node_id\": \"merger\",\n    \"type\": \"coordinator_fn\",\n    \"depends_on\": [\"extract1\", \"extract2\"],\n    \"io\": {\n        \"fn\": \"merge.generic\",\n        \"fn_args\": {\n            \"from_nodes\": [\"extract1\", \"extract2\"],\n            \"target\": {\"node_id\": \"merger\"}\n        }\n    }\n}\n</code></pre> <p>Built-in coordinator functions include (non-exhaustive):</p> <ul> <li><code>vars.set</code> \u2014 set multiple keys under <code>coordinator.vars</code> atomically</li> <li><code>vars.merge</code> \u2014 deep-merge dictionaries into <code>coordinator.vars</code></li> <li><code>vars.incr</code> \u2014 atomically increment a numeric leaf under <code>coordinator.vars</code></li> <li><code>vars.unset</code> \u2014 remove one or more keys and prune empty subtrees</li> <li><code>merge.generic</code> \u2014 merge artifacts from multiple nodes and mark a target artifact complete</li> <li><code>metrics.aggregate</code> \u2014 aggregate raw metrics and store them on a node</li> <li><code>noop</code> \u2014 no-op (useful in tests)</li> </ul> <p>See Task Variables below for full usage and guardrails.</p>"},{"location":"guide/coordinators/#task-variables-coordinatorvars","title":"Task Variables (<code>coordinator.vars</code>)","text":"<p>A task-scoped key\u2013value store lives at <code>tasks.coordinator.vars</code>. It allows lightweight, durable state that later coordinator decisions (or coordinator functions) can read and update without changing workers.</p>"},{"location":"guide/coordinators/#why-its-useful","title":"Why it\u2019s useful","text":"<ul> <li>Record thresholds, counters, feature flags, SLA knobs</li> <li>Let a coordinator function make data-driven choices (e.g., skip an expensive branch if <code>vars.qps &lt; 10</code>)</li> <li>Keep routing state in the task document instead of pushing it into workers</li> </ul>"},{"location":"guide/coordinators/#adapters","title":"Adapters","text":"<p>All adapters are available as coordinator functions (see examples). They are idempotent and rely on MongoDB atomic operators.</p>"},{"location":"guide/coordinators/#varssettask_id-str-kv-dict","title":"<code>vars.set(task_id: str, **kv) -&gt; dict</code>","text":"<ul> <li>Accepts dotted keys or nested dicts:   <pre><code># dotted + nested in the same call\n{\"kv\": {\"plain\": 7, \"sla.max_delay\": 500, \"flags\": {\"ab\": True}}}\n</code></pre></li> <li>Optional <code>block_sensitive=True</code> (either top-level kwarg or inside <code>kv</code>) rejects suspicious values (see Sensitive value detection).</li> <li>Performs a single atomic <code>$set</code> across all paths.</li> <li>Returns: <code>{\"ok\": True, \"touched\": N}</code></li> </ul>"},{"location":"guide/coordinators/#varsmergetask_id-str-kv-dict","title":"<code>vars.merge(task_id: str, **kv) -&gt; dict</code>","text":"<ul> <li>Deep-merge dicts into <code>coordinator.vars</code>; non-dict leaves overwrite.</li> <li>Accepts <code>{\"data\": {...}}</code> or <code>{\"kv\": {...}}</code>.</li> <li>Optional <code>block_sensitive=True</code> to reject suspicious values.</li> <li>Returns: <code>{\"ok\": True, \"touched\": N}</code></li> </ul>"},{"location":"guide/coordinators/#varsincrtask_id-str-key-str-by-intfloat-1-dict","title":"<code>vars.incr(task_id: str, *, key: str, by: int|float = 1) -&gt; dict</code>","text":"<ul> <li>Atomically increments a numeric leaf under <code>coordinator.vars.&lt;key&gt;</code> (using <code>$inc</code>).</li> <li>Creates the leaf if it\u2019s missing.</li> <li>Validates that existing value is numeric; rejects <code>NaN</code>/<code>\u00b1Inf</code>.</li> <li>Returns: <code>{\"ok\": True, \"key\": key, \"by\": by}</code></li> </ul>"},{"location":"guide/coordinators/#varsunsettask_id-str-paths-str-keys-iterablestr-none-none-dict","title":"<code>vars.unset(task_id: str, *paths: str, keys: Iterable[str] | None = None) -&gt; dict</code>","text":"<ul> <li>Removes one or more dotted keys under <code>coordinator.vars</code>.</li> <li>After unsetting, empty parent objects are pruned in a second pass.</li> <li>Returns: <code>{\"ok\": True, \"touched\": N}</code></li> </ul>"},{"location":"guide/coordinators/#limits-validation-defaults","title":"Limits &amp; validation (defaults)","text":"Limit Default Max keys per operation 256 Max nested depth (dots across full path, incl. <code>coordinator.vars.</code>) 16 Max path length (characters) 512 Max segment length (characters) 128 Max value size (strings/bytes, UTF\u20118) 64 KiB <p>Additional rules:</p> <ul> <li>Segments must be non-empty strings, must not start with <code>$</code>, contain <code>.</code> or NUL (<code>\\x00</code>).</li> <li>Non-dict leaves overwrite on merge.</li> <li>Strings are measured in UTF\u20118 bytes for size checks.</li> </ul>"},{"location":"guide/coordinators/#sensitive-value-detection","title":"Sensitive value detection","text":"<p>A built-in lightweight detector flags secret-like values. It never logs values (only counts). Heuristics include:</p> <ul> <li>Entropy \u2265 3.2 bits/char and length \u2265 20</li> <li>JWT/PEM/AWS/Google/Slack/Bearer token formats</li> <li>Long base64-like strings</li> </ul> <p>Behavior:</p> <ul> <li>If <code>block_sensitive=True</code> and any key looks sensitive \u2192 AdapterError</li> <li>Otherwise write proceeds and logs record <code>sensitive_hits</code></li> </ul>"},{"location":"guide/coordinators/#structured-logging","title":"Structured logging","text":"<p>Each adapter emits a structured log record:</p> <ul> <li>Event names: <code>coord.vars.set</code>, <code>coord.vars.merge</code>, <code>coord.vars.incr</code>, <code>coord.vars.unset</code>, <code>coord.vars.unset.prune</code></li> <li>Fields: <code>task_id</code>, <code>keys</code> (sorted), <code>n</code>, <code>sensitive_hits</code> (where applicable)</li> </ul> <p>Example (conceptual):</p> <pre><code>INFO coord.vars.set task_id=... keys=['coordinator.vars.a', 'coordinator.vars.b.c'] n=2 sensitive_hits=0\n</code></pre>"},{"location":"guide/coordinators/#using-adapters-in-a-coordinator_fn","title":"Using adapters in a coordinator_fn","text":"<pre><code># Node that seeds vars\n{\n  \"node_id\": \"seed_vars\",\n  \"type\": \"coordinator_fn\",\n  \"io\": {\n    \"fn\": \"vars.set\",\n    \"fn_args\": {\n      \"kv\": {\n        \"sla.max_delay\": 500,\n        \"flags\": {\"ab\": true, \"beta\": false}\n      }\n    }\n  }\n}\n\n# Node that bumps a counter\n{\n  \"node_id\": \"count_pages\",\n  \"type\": \"coordinator_fn\",\n  \"depends_on\": [\"seed_vars\"],\n  \"io\": {\n    \"fn\": \"vars.incr\",\n    \"fn_args\": {\"key\": \"counters.pages\", \"by\": 2}\n  }\n}\n</code></pre> <p>Later coordinator functions can read <code>tasks.coordinator.vars</code> directly from MongoDB to make decisions.</p>"},{"location":"guide/coordinators/#error-handling","title":"Error Handling","text":""},{"location":"guide/coordinators/#retry-policies","title":"Retry Policies","text":"<pre><code>{\n    \"node_id\": \"flaky_processor\",\n    \"retry_policy\": {\n        \"max\": 3,                    # Maximum retry attempts\n        \"backoff_sec\": 300,          # Backoff between retries\n        \"permanent_on\": [            # Errors that shouldn't retry\n            \"bad_input\",\n            \"schema_mismatch\"\n        ]\n    }\n}\n</code></pre>"},{"location":"guide/coordinators/#cascade-cancellation","title":"Cascade Cancellation","text":"<p>When a node fails permanently, the coordinator can cancel downstream nodes:</p> <pre><code># This happens automatically for permanent failures\n# You can also trigger manual cancellation:\n\nawait coordinator.cascade_cancel(\n    task_id=\"abc123\",\n    reason=\"upstream_failure\"\n)\n</code></pre>"},{"location":"guide/coordinators/#scaling-coordinators","title":"Scaling Coordinators","text":""},{"location":"guide/coordinators/#multiple-coordinators","title":"Multiple Coordinators","text":"<p>You can run multiple coordinators for high availability:</p> <pre><code># Coordinator 1\ncoordinator1 = Coordinator(db=db, cfg=config, worker_types=[\"processor\"])\n\n# Coordinator 2\ncoordinator2 = Coordinator(db=db, cfg=config, worker_types=[\"analyzer\"])\n\n# They coordinate through MongoDB and don't conflict\nawait coordinator1.start()\nawait coordinator2.start()\n</code></pre>"},{"location":"guide/coordinators/#load-balancing","title":"Load Balancing","text":"<p>Coordinators automatically distribute work:</p> <ul> <li>Task scheduling is coordinated through MongoDB</li> <li>Workers are discovered dynamically</li> <li>No single point of failure</li> </ul>"},{"location":"guide/coordinators/#monitoring-and-observability","title":"Monitoring and Observability","text":""},{"location":"guide/coordinators/#health-checks","title":"Health Checks","text":"<pre><code>async def check_coordinator_health(coordinator):\n    # Check if coordinator is running\n    if not coordinator._running:\n        return \"stopped\"\n\n    # Check active workers\n    workers = await coordinator.db.worker_registry.find(\n        {\"status\": \"online\"}\n    ).to_list(100)\n\n    # Check pending tasks\n    pending = await coordinator.db.tasks.count_documents(\n        {\"status\": {\"$in\": [\"queued\", \"running\"]}}\n    )\n\n    return {\n        \"status\": \"healthy\",\n        \"active_workers\": len(workers),\n        \"pending_tasks\": pending\n    }\n</code></pre>"},{"location":"guide/coordinators/#metrics-collection","title":"Metrics Collection","text":"<pre><code>async def collect_coordinator_metrics(coordinator):\n    # Task metrics\n    tasks_by_status = await coordinator.db.tasks.aggregate([\n        {\"$group\": {\"_id\": \"$status\", \"count\": {\"$sum\": 1}}}\n    ]).to_list(10)\n\n    # Worker metrics\n    workers_by_status = await coordinator.db.worker_registry.aggregate([\n        {\"$group\": {\"_id\": \"$status\", \"count\": {\"$sum\": 1}}}\n    ]).to_list(10)\n\n    # Recent events\n    from datetime import datetime, timedelta\n    recent_events = await coordinator.db.worker_events.count_documents({\n        \"created_at\": {\"$gte\": datetime.utcnow() - timedelta(hours=1)}\n    })\n\n    return {\n        \"tasks\": {x[\"_id\"]: x[\"count\"] for x in tasks_by_status},\n        \"workers\": {x[\"_id\"]: x[\"count\"] for x in workers_by_status},\n        \"recent_events\": recent_events\n    }\n</code></pre>"},{"location":"guide/coordinators/#best-practices","title":"Best Practices","text":""},{"location":"guide/coordinators/#configuration-management","title":"Configuration Management","text":"<ol> <li> <p>Use environment-specific configs:    <pre><code>config = CoordinatorConfig.load(f\"configs/coordinator.{env}.json\")\n</code></pre></p> </li> <li> <p>Tune timing parameters based on your workload:</p> </li> <li>Short <code>scheduler_tick_sec</code> for low-latency</li> <li> <p>Longer <code>heartbeat_soft_sec</code> for stable workloads</p> </li> <li> <p>Set appropriate worker types to match your pipeline needs</p> </li> </ol>"},{"location":"guide/coordinators/#task-design","title":"Task Design","text":"<ol> <li>Keep DAGs focused: Don't create overly complex graphs</li> <li>Use streaming for large datasets</li> <li>Implement proper retry policies for reliability</li> <li>Add monitoring for long-running tasks</li> </ol>"},{"location":"guide/coordinators/#error-handling_1","title":"Error Handling","text":"<ol> <li>Classify errors properly in handlers</li> <li>Set reasonable retry limits to avoid infinite loops</li> <li>Monitor failure patterns to identify systemic issues</li> <li>Implement circuit breakers for external dependencies</li> </ol>"},{"location":"guide/coordinators/#performance","title":"Performance","text":"<ol> <li>Scale coordinators horizontally for high throughput</li> <li>Tune Kafka settings for your message volume</li> <li>Monitor MongoDB performance under load</li> <li>Use indexing for large task collections</li> </ol>"},{"location":"guide/coordinators/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guide/coordinators/#common-issues","title":"Common Issues","text":"<p>Coordinator not starting: - Check Kafka connectivity - Verify MongoDB connection - Ensure topics are created</p> <p>Tasks not being scheduled: - Check worker availability - Verify DAG dependencies are correct - Look for failed discovery queries</p> <p>High memory usage: - Tune outbox retention settings - Clean up old task documents - Monitor worker event collection size</p> <p>Slow scheduling: - Reduce <code>scheduler_tick_sec</code> - Add MongoDB indexes - Optimize DAG complexity</p>"},{"location":"guide/coordinators/#next-steps","title":"Next Steps","text":"<ul> <li>Configure Workers to process your tasks</li> <li>Design Task Graphs for complex workflows</li> <li>Handle Errors gracefully</li> <li>Monitor Performance in production</li> </ul>"},{"location":"guide/error-handling/","title":"Error handling &amp; retries","text":"<p>FlowKit separates configuration/programming errors (permanent) from operational errors (transient) so the Coordinator can retry only when it makes sense.</p>"},{"location":"guide/error-handling/#classification-rules","title":"Classification rules","text":""},{"location":"guide/error-handling/#worker-side-normalization","title":"Worker-side normalization","text":"<ul> <li>Unknown input adapter \u2192 <code>TASK_FAILED</code>, <code>reason_code=\"bad_input_adapter\"</code>, <code>permanent=True</code>.</li> <li>Missing/invalid adapter args \u2192 <code>TASK_FAILED</code>, <code>reason_code=\"bad_input_args\"</code>, <code>permanent=True</code>.</li> </ul> <p>Handlers can also classify errors using <code>RoleHandler.classify_error(exc)</code>:</p> <pre><code>def classify_error(self, exc: BaseException) -&gt; tuple[str, bool]:\n    if isinstance(exc, (TypeError, ValueError, KeyError)):\n        return (\"bad_input\", True)         # permanent\n    return (\"unexpected_error\", False)     # transient\n</code></pre> <p>Use this default or refine it for your domain.</p>"},{"location":"guide/error-handling/#coordinator-retry-policy","title":"Coordinator retry policy","text":"<p>Coordinator retries only transient failures according to per-node policy (max attempts, backoff, etc.). Permanent failures finalize the node immediately.</p>"},{"location":"guide/error-handling/#visibility-observability","title":"Visibility &amp; observability","text":"<p>Workers emit structured logs for input routing and failures:</p> <ul> <li><code>worker.adapter.selected</code> \u2192 adapter name, normalized args, and <code>io_source</code> (<code>cmd|handler</code>).</li> <li><code>worker.adapter.handler_conflict_ignored</code> \u2192 when cmd overrides handler suggestion.</li> <li><code>TASK_FAILED</code> envelopes include <code>reason_code</code>, <code>permanent</code>, and <code>error</code> message.</li> </ul> <p>This makes misconfiguration obvious during pre-flight instead of failing deep inside background tasks.</p>"},{"location":"guide/error-handling/#empty-upstream-is-not-an-error","title":"Empty upstream is not an error","text":"<p>If routes/args are valid but upstream has no data, the worker completes normally (<code>count=0</code>). This prevents flapping retries on empty streams.</p>"},{"location":"guide/error-handling/#examples","title":"Examples","text":"<p>Unknown adapter in cmd <pre><code>{\n    \"io\": {\"input_inline\": {\"input_adapter\": \"does.not.exist\", \"input_args\": {}}}\n}\n</code></pre> \u2192 <code>TASK_FAILED</code> / <code>bad_input_adapter</code> (permanent).</p> <p>Missing args for rechunk <pre><code>{\n    \"io\": {\n    \"input_inline\": {\n        \"input_adapter\": \"pull.from_artifacts.rechunk:size\",\n        \"input_args\": {\"from_nodes\": [\"u\"]}\n    }\n    }\n}\n</code></pre> If your product policy requires <code>meta_list_key</code>, enforce it at the coordinator (strict mode). The worker core is lenient by default and will treat each artifact meta as a single logical item.</p>"},{"location":"guide/error-handling/#tips","title":"Tips","text":"<ul> <li>Keep classification stable: don\u2019t flip an error from transient to permanent across releases without a migration plan.</li> <li>Attach actionable text in <code>error</code> field for operator runbooks.</li> </ul>"},{"location":"guide/graphs/","title":"Task Graphs","text":"<p>This page describes FlowKit DAGs: nodes, edges, fan\u2011in modes, streaming rules \u2014 and how to use task variables (<code>coordinator.vars</code>) to enable data\u2011driven behavior.</p>"},{"location":"guide/graphs/#nodes","title":"Nodes","text":"<p>A graph consists of nodes with unique <code>node_id</code> and a <code>type</code>:</p> <ul> <li>Worker nodes (<code>type</code> = your worker type, e.g. <code>indexer</code>, <code>processor</code>, <code>analyzer</code>)</li> <li>Coordinator functions (<code>type</code> = <code>coordinator_fn</code>) which run small orchestration steps inside the coordinator process</li> </ul> <p>Minimal node example:</p> <pre><code>{\n  \"node_id\": \"processor\",\n  \"type\": \"processor\",\n  \"depends_on\": [\"indexer\"]\n}\n</code></pre>"},{"location":"guide/graphs/#edges","title":"Edges","text":"<p>Use <code>edges: [[\"src\", \"dst\"], ...]</code> to express simple dependencies. A node becomes ready when its fan\u2011in condition is satisfied (see below).</p>"},{"location":"guide/graphs/#extended-edges-edges_ex","title":"Extended edges (<code>edges_ex</code>)","text":"<p>You can add additional routing metadata with <code>edges_ex</code> entries:</p> <pre><code>{\n  \"edges\": [[\"indexer\", \"processor\"]],\n  \"edges_ex\": [\n    {\"from\": \"indexer\", \"to\": \"processor\", \"mode\": \"async\", \"trigger\": \"on_batch\"}\n  ]\n}\n</code></pre> <ul> <li><code>mode: \"async\"</code> \u2014 allows downstream to start before upstream completes</li> <li><code>trigger: \"on_batch\"</code> \u2014 pair with <code>io.start_when = \"first_batch\"</code> on the child for streaming start</li> </ul> <p>The coordinator still enforces normal readiness checks; <code>edges_ex</code> is a hint that complements them.</p>"},{"location":"guide/graphs/#fanin-modes","title":"Fan\u2011in modes","text":"<p>A node can delay start until enough parents finish:</p> <pre><code>{ \"node_id\": \"combiner\", \"depends_on\": [\"a\", \"b\", \"c\"], \"fan_in\": \"all\" }   // default\n{ \"node_id\": \"processor\", \"depends_on\": [\"a\", \"b\"], \"fan_in\": \"any\" }\n{ \"node_id\": \"analyzer\", \"depends_on\": [\"a\", \"b\", \"c\"], \"fan_in\": \"count:2\" }\n</code></pre>"},{"location":"guide/graphs/#streaming","title":"Streaming","text":"<p>Start a node as soon as the upstream emits its first batch:</p> <pre><code>{ \"node_id\": \"proc\", \"depends_on\": [\"indexer\"], \"io\": {\"start_when\": \"first_batch\"} }\n</code></pre> <p>If omitted, the default is <code>ready</code> (start after parents finish).</p>"},{"location":"guide/graphs/#coordinator-functions-in-graphs","title":"Coordinator functions in graphs","text":"<p>Coordinator functions are nodes with <code>type: \"coordinator_fn\"</code>. They execute one of the registered adapters by name via <code>io.fn</code> with arguments in <code>io.fn_args</code>.</p> <pre><code>{\n  \"node_id\": \"seed_vars\",\n  \"type\": \"coordinator_fn\",\n  \"io\": {\n    \"fn\": \"vars.set\",\n    \"fn_args\": { \"kv\": { \"flags.ab\": true, \"sla.max_delay\": 500 } }\n  }\n}\n</code></pre> <p>See Coordinators \u2192 Task Variables for all adapters.</p>"},{"location":"guide/graphs/#using-coordinatorvars-for-datadriven-routing","title":"Using <code>coordinator.vars</code> for data\u2011driven routing","text":"<p><code>coordinator.vars</code> is a task\u2011scoped key\u2013value bag stored on the task document. Typical patterns:</p> <ul> <li>Feature flags / AB routing: <code>flags.ab = True</code> to route lighter branch in later coordinator steps</li> <li>SLA knobs: <code>sla.max_delay</code>, <code>sla.quality_threshold</code></li> <li>Counters and volumes: <code>counters.pages += 1</code></li> </ul>"},{"location":"guide/graphs/#pattern-gate-a-branch-with-a-coordinator-function","title":"Pattern: gate a branch with a coordinator function","text":"<ol> <li>Seed routing signals:</li> </ol> <pre><code>{ \"node_id\": \"seed_vars\", \"type\": \"coordinator_fn\",\n  \"io\": {\"fn\": \"vars.merge\", \"fn_args\": {\"data\": {\"sla\": {\"max_delay\": 500}, \"flags\": {\"ab\": true}}}} }\n</code></pre> <ol> <li>Downstream coordinator function checks <code>tasks.coordinator.vars</code> and decides what to do (e.g., set additional flags, update metrics, or schedule light\u2011weight work).</li> </ol> <p>The base scheduler reacts to dependencies, fan\u2011in, and streaming rules. Using <code>coordinator.vars</code> lets your coordinator\u2011side logic remain pure data. Upcoming features (see EPIC: Dynamic DAG) make conditional edges first\u2011class.</p>"},{"location":"guide/graphs/#example-streaming-with-early-processing","title":"Example: streaming with early processing","text":"<pre><code>{\n  \"nodes\": [\n    {\"node_id\": \"indexer\", \"type\": \"indexer\"},\n    {\"node_id\": \"proc\", \"type\": \"processor\", \"depends_on\": [\"indexer\"],\n      \"io\": {\"start_when\": \"first_batch\"}},\n    {\"node_id\": \"seed_vars\", \"type\": \"coordinator_fn\", \"depends_on\": [\"indexer\"],\n      \"io\": {\"fn\": \"vars.incr\", \"fn_args\": {\"key\": \"counters.pages\", \"by\": 2}}}\n  ],\n  \"edges\": [[\"indexer\", \"proc\"], [\"indexer\", \"seed_vars\"]],\n  \"edges_ex\": [\n    {\"from\": \"indexer\", \"to\": \"proc\", \"mode\": \"async\", \"trigger\": \"on_batch\"}\n  ]\n}\n</code></pre> <ul> <li><code>proc</code> can start on the first upstream batch</li> <li><code>seed_vars</code> adjusts a task counter side\u2011by\u2011side</li> </ul>"},{"location":"guide/workers/","title":"Workers","text":"<p>Workers are the execution engine of FlowKit. They execute handler code, stream batches, and report status back to the Coordinator.</p> <p>This page documents the orchestrator-first contract, input adapter precedence, deterministic rechunking, and error visibility that make worker behavior predictable and easy to operate.</p>"},{"location":"guide/workers/#overview","title":"Overview","text":"<p>A Worker:</p> <ul> <li>Executes tasks via user-provided <code>RoleHandler</code>s.</li> <li>Streams input either through input adapters (pulling from upstream artifacts) or via the handler\u2019s own <code>iter_batches</code> fallback.</li> <li>Reports progress to Kafka status topics.</li> <li>Handles cancellation quickly and cooperatively.</li> <li>Persists artifacts and emits metrics.</li> </ul>"},{"location":"guide/workers/#quick-start","title":"Quick start","text":"<pre><code>from flowkit.worker.runner import Worker, WorkerConfig\nfrom flowkit.worker.handlers.base import RoleHandler, Batch, BatchResult\n\nclass MyHandler(RoleHandler):\n    role = \"processor\"\n\n    async def iter_batches(self, loaded):\n        yield Batch(batch_uid=\"b1\", payload={\"items\": [1, 2, 3]})\n\n    async def process_batch(self, batch, ctx):\n        return BatchResult(success=True, metrics={\"count\": len(batch.payload.get(\"items\", []))})\n\nworker = Worker(db=my_db, cfg=WorkerConfig(roles=[\"processor\"]), handlers={\"processor\": MyHandler()})\nawait worker.start()\n</code></pre>"},{"location":"guide/workers/#input-routing-the-orchestrator-first-contract","title":"Input routing: the orchestrator-first contract","text":"<p>There is one source of truth for the input route: the Coordinator.</p> <p>Adapter selection precedence</p> <ol> <li>If the Coordinator provides <code>cmd.input_inline.input_adapter</code> \u2192 the worker must use it.</li> <li>Else, the worker may use a handler-proposed adapter from <code>load_input()</code> if it is known.</li> <li>Else, the worker falls back to <code>handler.iter_batches()</code>.</li> </ol> <p>Additional rules:</p> <ul> <li>Unknown adapter in cmd \u2192 permanent <code>TASK_FAILED</code> with reason <code>bad_input_adapter</code>.</li> <li>Missing/invalid adapter args \u2192 permanent <code>TASK_FAILED</code> with reason <code>bad_input_args</code> (no hidden <code>TypeError</code>).</li> <li>We accept the alias <code>from_node</code> and normalize it to <code>from_nodes=[...]</code>.</li> <li>The core is domain-agnostic: it never guesses schema keys like <code>skus</code> or <code>items</code>.</li> </ul>"},{"location":"guide/workers/#example-coordinator-wins-over-handler","title":"Example: Coordinator wins over handler","text":"<p><pre><code>{\n  \"io\": {\n    \"start_when\": \"first_batch\",\n    \"input_inline\": {\n      \"input_adapter\": \"pull.from_artifacts.rechunk:size\",\n      \"input_args\": {\"from_nodes\": [\"u\"], \"size\": 2, \"meta_list_key\": \"skus\"}\n    }\n  }\n}\n</code></pre> If a handler returns a different route from <code>load_input()</code>, it is ignored and the route above is used.</p>"},{"location":"guide/workers/#adapters","title":"Adapters","text":""},{"location":"guide/workers/#pullfrom_artifacts","title":"<code>pull.from_artifacts</code>","text":"<p>Pull partial upstream artifacts as they are produced.</p> <pre><code>{\n  \"io\": {\n    \"input_inline\": {\n      \"input_adapter\": \"pull.from_artifacts\",\n      \"input_args\": {\"from_nodes\": [\"u\"], \"poll_ms\": 50}\n    }\n  }\n}\n</code></pre>"},{"location":"guide/workers/#pullfrom_artifactsrechunksize","title":"<code>pull.from_artifacts.rechunk:size</code>","text":"<p>Deterministically reshape upstream artifact meta to fixed-size slices.</p>"},{"location":"guide/workers/#key-semantics-deterministic-schema-agnostic","title":"Key semantics (deterministic, schema-agnostic)","text":"<ul> <li><code>meta_list_key</code> is optional.</li> <li>If provided and <code>meta[meta_list_key]</code> is a list \u2192 chunk that list.</li> <li>If not provided (or key is not a list) \u2192 treat each artifact meta as a single logical item (<code>items=[meta]</code>).</li> <li>No heuristics over domain keys. No guessing <code>skus|items|\u2026</code>.</li> <li>In strict installations, you may require <code>meta_list_key</code> via a product policy flag (outside of worker core).</li> </ul> <pre><code>{\n  \"io\": {\n    \"input_inline\": {\n      \"input_adapter\": \"pull.from_artifacts.rechunk:size\",\n      \"input_args\": {\n        \"from_node\": \"u\",\n        \"size\": 3,\n        \"poll_ms\": 25,\n        \"meta_list_key\": \"skus\"\n      }\n    }\n  }\n}\n</code></pre> <p>Empty upstream is not an error. If the route/args are valid but there is no data, the node finishes normally (<code>count=0</code>).</p>"},{"location":"guide/workers/#handler-interface","title":"Handler interface","text":"<pre><code>from typing import AsyncIterator\nfrom flowkit.worker.handlers.base import RoleHandler, Batch, BatchResult, FinalizeResult\n\nclass MyRole(RoleHandler):\n    role = \"probe\"\n\n    async def init(self, run_info: dict):  # optional\n        pass\n\n    async def load_input(self, input_ref, input_inline):\n        # May return a *suggestion* for adapter when cmd doesn't provide one\n        return {\"input_ref\": input_ref or {}, \"input_inline\": input_inline or {}}\n\n    async def iter_batches(self, loaded) -&gt; AsyncIterator[Batch]:\n        # Used only when no adapter is selected\n        yield Batch(batch_uid=\"b1\", payload={\"items\": [1, 2]})\n\n    async def process_batch(self, batch: Batch, ctx) -&gt; BatchResult:\n        return BatchResult(success=True, metrics={\"count\": len(batch.payload.get(\"items\", []))})\n\n    async def finalize(self, ctx) -&gt; FinalizeResult | None:\n        return FinalizeResult(metrics={})\n\n    def classify_error(self, exc: BaseException) -&gt; tuple[str, bool]:\n        # Default behavior in core:\n        #   TypeError/ValueError/KeyError -&gt; permanent (\"bad_input\")\n        #   otherwise -&gt; transient (\"unexpected_error\")\n        if isinstance(exc, (TypeError, ValueError, KeyError)):\n            return (\"bad_input\", True)\n        return (\"unexpected_error\", False)\n</code></pre>"},{"location":"guide/workers/#validation-observability","title":"Validation &amp; observability","text":"<p>The worker validates adapter names and arguments before streaming:</p> <ul> <li>Unknown adapter \u2192 <code>TASK_FAILED</code> (<code>bad_input_adapter</code>, permanent).</li> <li>Bad/missing args \u2192 <code>TASK_FAILED</code> (<code>bad_input_args</code>, permanent).</li> <li>Structured logs include:</li> <li><code>adapter.selected</code> (name, args, source=<code>cmd|handler</code>)</li> <li><code>handler_conflict_ignored</code> when cmd overrides handler suggestion</li> </ul>"},{"location":"guide/workers/#cancellation","title":"Cancellation","text":"<p>Workers check <code>ctx.cancel_flag</code> between batches and within long operations and transition to <code>cancelling</code> quickly on:</p> <ol> <li>Coordinator CANCEL signal</li> <li>DB cancel flags</li> <li>Lease/heartbeat policy</li> <li>Graceful shutdown</li> </ol>"},{"location":"guide/workers/#metrics-artifacts","title":"Metrics &amp; artifacts","text":"<p>Handlers return metrics with each <code>BatchResult</code>. The worker upserts partial artifact rows and marks completion at the end, attaching final metrics.</p>"},{"location":"guide/workers/#configuration-essentials","title":"Configuration (essentials)","text":"<pre><code>from flowkit.worker.runner import WorkerConfig\n\ncfg = WorkerConfig(\n    kafka_bootstrap=\"kafka:9092\",\n    roles=[\"indexer\", \"probe\"],\n    lease_ttl_sec=60,\n    hb_interval_sec=20,\n    pull_poll_ms_default=300,\n    pull_empty_backoff_ms_max=4000,\n)\n</code></pre> <p>Load with overrides:</p> <pre><code>WorkerConfig.load(path=\"configs/worker.default.json\", overrides={\"roles\": [\"probe\"]})\n</code></pre>"},{"location":"guide/workers/#best-practices","title":"Best practices","text":"<ul> <li>Keep handlers stateless; put state/offsets in artifacts or DB.</li> <li>Classify errors carefully; return permanent for configuration mistakes.</li> <li>Don\u2019t encode domain semantics in core\u2014use <code>meta_list_key</code> where needed.</li> <li>Prefer <code>start_when=first_batch</code> for true streaming, otherwise wait for upstream completion.</li> </ul>"},{"location":"guide/workers/#see-also","title":"See also","text":"<ul> <li>Reference \u2192 Worker</li> <li>Guide \u2192 Error handling</li> <li>Getting started \u2192 Concepts</li> </ul>"},{"location":"reference/bus/","title":"Bus API Reference","text":""},{"location":"reference/bus/#kafka-bus","title":"Kafka Bus","text":""},{"location":"reference/bus/#flowkit.bus.kafka","title":"flowkit.bus.kafka","text":""},{"location":"reference/bus/#flowkit.bus.kafka-classes","title":"Classes","text":""},{"location":"reference/bus/#flowkit.bus.kafka.KafkaBus","title":"KafkaBus","text":"<pre><code>KafkaBus(cfg: CoordinatorConfig)\n</code></pre> <p>Thin wrapper around AIOKafka with a minimal reply correlator (by corr_id).</p> Source code in <code>src/flowkit/bus/kafka.py</code> <pre><code>def __init__(self, cfg: CoordinatorConfig) -&gt; None:\n    self.cfg = cfg\n    self._producer: AIOKafkaProducer | None = None\n    self._consumers: list[AIOKafkaConsumer] = []\n    self._replies: dict[str, list[Envelope]] = {}\n    self._reply_events: dict[str, asyncio.Event] = {}\n    self.bootstrap = cfg.kafka_bootstrap\n</code></pre>"},{"location":"reference/coordinator/","title":"Coordinator API Reference","text":"<p>This page combines API introspection with a practical overview of coordinator adapters and task variables.</p>"},{"location":"reference/coordinator/#task-variables-adapters-overview","title":"Task Variables &amp; Adapters (overview)","text":"<p>Task variables live under <code>tasks.coordinator.vars</code>. Adapters are exposed as coordinator functions and can also be imported and called directly in Python.</p>"},{"location":"reference/coordinator/#quick-reference","title":"Quick reference","text":"Adapter Signature (simplified) Semantics <code>vars.set</code> <code>vars_set(task_id, **kv)</code> Set multiple dotted/nested keys atomically (<code>$set</code>). Optional <code>block_sensitive=True</code>. <code>vars.merge</code> <code>vars_merge(task_id, **kv)</code> Deep-merge dicts into <code>coordinator.vars</code>; non-dict leaves overwrite. Optional <code>block_sensitive=True</code>. <code>vars.incr</code> <code>vars_incr(task_id, *, key, by=1)</code> Atomic numeric increment on <code>coordinator.vars.&lt;key&gt;</code> (<code>$inc</code>). Creates the leaf if missing. <code>vars.unset</code> <code>vars_unset(task_id, *paths, keys=None)</code> Remove one or more keys; prunes empty subtrees afterwards. <code>merge.generic</code> <code>merge_generic(task_id, from_nodes, target)</code> Merge metadata across nodes, mark target artifact complete. <code>metrics.aggregate</code> <code>metrics_aggregate(task_id, node_id, mode=\"sum\")</code> Aggregate raw metrics, write to node stats. <code>noop</code> <code>noop(task_id, **_)</code> No-op helper useful for tests."},{"location":"reference/coordinator/#limits-validation","title":"Limits &amp; validation","text":"<ul> <li>Max 256 keys per operation</li> <li>Max depth \u2248 16 dot segments across the full path (including <code>coordinator.vars.</code>)</li> <li>Max path length 512 chars; max segment length 128 chars</li> <li>Max string/bytes value size: 64 KiB (UTF\u20118 for strings)</li> <li>Key segments cannot start with <code>$</code>, contain <code>.</code> or NUL (<code>\\x00</code>)</li> </ul>"},{"location":"reference/coordinator/#sensitive-value-detection","title":"Sensitive value detection","text":"<p>Adapters can block or flag sensitive-looking values (tokens, PEM/JWT, high-entropy blobs). Pass <code>block_sensitive=True</code> to make the adapter fail fast, otherwise the write succeeds and logs <code>sensitive_hits</code> &gt; 0.</p>"},{"location":"reference/coordinator/#coordinator-class","title":"Coordinator Class","text":""},{"location":"reference/coordinator/#flowkit.coordinator.runner.Coordinator","title":"flowkit.coordinator.runner.Coordinator","text":"<pre><code>Coordinator(*, db, cfg: CoordinatorConfig | None = None, worker_types: list[str] | None = None, clock: Clock | None = None, adapters: dict[str, Any] | None = None, sensitive_detector: Any | None = None)\n</code></pre> <p>Orchestrates DAG execution across workers via Kafka topics. <code>db</code> is injected (e.g., Motor client). <code>clock</code> is injectable for tests.</p> Source code in <code>src/flowkit/coordinator/runner.py</code> <pre><code>def __init__(\n    self,\n    *,\n    db,\n    cfg: CoordinatorConfig | None = None,\n    worker_types: list[str] | None = None,\n    clock: Clock | None = None,\n    adapters: dict[str, Any] | None = None,\n    sensitive_detector: Any | None = None,\n) -&gt; None:\n    self.db = db\n    self.cfg = copy.deepcopy(cfg) if cfg is not None else CoordinatorConfig.load()\n    if worker_types:\n        self.cfg.worker_types = list(worker_types)\n\n    self.clock: Clock = clock or SystemClock()\n    self.bus = KafkaBus(self.cfg)\n    self.outbox = OutboxDispatcher(db=db, bus=self.bus, cfg=self.cfg, clock=self.clock)\n    self.adapters = adapters or dict(default_adapters(db=db, clock=self.clock, detector=sensitive_detector))\n\n    self._tasks: set[asyncio.Task] = set()\n    self._running = False\n\n    self._announce_consumer: AIOKafkaConsumer | None = None\n    self._status_consumers: dict[str, AIOKafkaConsumer] = {}\n    self._query_reply_consumer: AIOKafkaConsumer | None = None\n\n    self._gid = f\"coord.{uuid.uuid4().hex[:6]}\"\n\n    # logging\n    self.log = get_logger(\"coordinator\")\n    bind_context(role=\"coordinator\", gid=self._gid)\n    self.log.debug(\"coordinator.init\", event=\"coord.init\")\n</code></pre>"},{"location":"reference/coordinator/#flowkit.coordinator.runner.Coordinator-functions","title":"Functions","text":""},{"location":"reference/coordinator/#flowkit.coordinator.runner.Coordinator.start","title":"start  <code>async</code>","text":"<pre><code>start() -&gt; None\n</code></pre> Source code in <code>src/flowkit/coordinator/runner.py</code> <pre><code>async def start(self) -&gt; None:\n    # Helpful config print (silent by default unless tests enable stdout)\n    cfg_dump: Any\n    if hasattr(self.cfg, \"model_dump\"):  # pydantic v2\n        cfg_dump = self.cfg.model_dump()\n    elif hasattr(self.cfg, \"dict\"):  # pydantic v1\n        cfg_dump = self.cfg.dict()\n    else:\n        cfg_dump = getattr(self.cfg, \"__dict__\", str(self.cfg))\n    self.log.debug(\"coordinator.start\", event=\"coord.start\", cfg=cfg_dump)\n\n    await self._ensure_indexes()\n    await self.bus.start()\n    await self._start_consumers()\n    await self.outbox.start()\n    self._running = True\n    self._spawn(self._scheduler_loop(), name=\"scheduler\")\n    self._spawn(self._heartbeat_monitor(), name=\"hb-monitor\")\n    self._spawn(self._finalizer_loop(), name=\"finalizer\")\n    self._spawn(self._resume_inflight(), name=\"resume-inflight\")\n    self.log.debug(\"coordinator.started\", event=\"coord.started\", gid=self._gid)\n</code></pre>"},{"location":"reference/coordinator/#flowkit.coordinator.runner.Coordinator.stop","title":"stop  <code>async</code>","text":"<pre><code>stop() -&gt; None\n</code></pre> Source code in <code>src/flowkit/coordinator/runner.py</code> <pre><code>async def stop(self) -&gt; None:\n    self._running = False\n    for t in list(self._tasks):\n        t.cancel()\n    self._tasks.clear()\n    with swallow(\n        logger=self.log, code=\"outbox.stop\", msg=\"outbox stop failed\", level=logging.ERROR, expected=False\n    ):\n        await self.outbox.stop()\n    with swallow(logger=self.log, code=\"bus.stop\", msg=\"bus stop failed\", level=logging.ERROR, expected=False):\n        await self.bus.stop()\n    self.log.debug(\"coordinator.stopped\", event=\"coord.stopped\")\n</code></pre>"},{"location":"reference/coordinator/#flowkit.coordinator.runner.Coordinator.create_task","title":"create_task  <code>async</code>","text":"<pre><code>create_task(*, params: dict[str, Any], graph: dict[str, Any]) -&gt; str\n</code></pre> Source code in <code>src/flowkit/coordinator/runner.py</code> <pre><code>async def create_task(self, *, params: dict[str, Any], graph: dict[str, Any]) -&gt; str:\n    task_id = str(uuid.uuid4())\n    graph.setdefault(\"nodes\", [])\n    graph.setdefault(\"edges\", [])\n    graph.setdefault(\"edges_ex\", [])\n    doc = TaskDoc(\n        id=task_id,\n        pipeline_id=task_id,\n        status=RunState.queued,\n        params=params,\n        graph=graph,\n        status_history=[{\"from\": None, \"to\": RunState.queued, \"at\": self.clock.now_dt()}],\n        started_at=self.clock.now_dt().isoformat(),\n        last_event_recv_ms=self.clock.now_ms(),\n    ).model_dump(mode=\"json\")\n    await self.db.tasks.insert_one(doc)\n    self.log.debug(\"task.created\", event=\"coord.task.created\", task_id=task_id)\n    return task_id\n</code></pre>"},{"location":"reference/coordinator/#adapters-api-via-mkdocstrings","title":"Adapters (API via mkdocstrings)","text":""},{"location":"reference/coordinator/#flowkit.coordinator.adapters","title":"flowkit.coordinator.adapters","text":""},{"location":"reference/coordinator/#flowkit.coordinator.adapters-classes","title":"Classes","text":""},{"location":"reference/coordinator/#flowkit.coordinator.adapters.CoordinatorAdapters","title":"CoordinatorAdapters","text":"<pre><code>CoordinatorAdapters(*, db, clock: Clock | None = None, detector: SensitiveDetectorProto | Callable[[str, Any], bool] | str | None = None)\n</code></pre> <p>Coordinator-facing helpers that operate via the injected <code>db</code>. Side effects should be idempotent.</p> Source code in <code>src/flowkit/coordinator/adapters.py</code> <pre><code>def __init__(\n    self,\n    *,\n    db,\n    clock: Clock | None = None,\n    detector: SensitiveDetectorProto | Callable[[str, Any], bool] | str | None = None,\n) -&gt; None:\n    self.db = db\n    self.clock = clock or SystemClock()\n    self.log = get_logger(\"coord.adapters\")\n    self._limits = {\n        \"max_paths_per_op\": 256,\n        \"max_key_depth\": 16,  # max number of dots in the stored path\n        \"max_path_len\": 512,\n        \"max_value_bytes\": 64 * 1024,\n        \"max_seg_len\": 128,\n    }\n    if isinstance(detector, str):\n        detector = _import_from_string(detector)\n    self._detector = _DetectorShim(detector) if detector is not None else _SensitiveDetector()\n</code></pre>"},{"location":"reference/coordinator/#flowkit.coordinator.adapters.CoordinatorAdapters-functions","title":"Functions","text":""},{"location":"reference/coordinator/#flowkit.coordinator.adapters.CoordinatorAdapters.vars_set","title":"vars_set  <code>async</code>","text":"<pre><code>vars_set(task_id: str, **kv: Any) -&gt; dict[str, Any]\n</code></pre> <p>Set multiple keys atomically under coordinator.vars. Accepts dotted keys via {\"kv\": {\"sla.max_delay\": 500, \"qps\": 7}} or nested dict via {\"kv\": {\"sla\": {\"max_delay\": 500}}}. Direct kwargs are accepted for identifier-safe keys.</p> Source code in <code>src/flowkit/coordinator/adapters.py</code> <pre><code>async def vars_set(self, task_id: str, **kv: Any) -&gt; dict[str, Any]:\n    \"\"\"\n    Set multiple keys atomically under coordinator.vars.\n    Accepts dotted keys via {\"kv\": {\"sla.max_delay\": 500, \"qps\": 7}}\n    or nested dict via {\"kv\": {\"sla\": {\"max_delay\": 500}}}.\n    Direct kwargs are accepted for identifier-safe keys.\n    \"\"\"\n    block_sensitive_flag = bool(kv.pop(\"block_sensitive\", False))\n\n    if \"kv\" in kv and isinstance(kv[\"kv\"], dict):\n        data = dict(kv[\"kv\"])  # copy to avoid caller mutation\n        block_sensitive = bool(data.pop(\"block_sensitive\", False)) or block_sensitive_flag\n    else:\n        data = {k: v for k, v in kv.items() if k not in (\"block_sensitive\",)}\n        block_sensitive = block_sensitive_flag\n\n    set_doc: dict[str, Any] = {}\n    for k, v in list(data.items()):\n        if isinstance(v, dict) and \".\" not in k:\n            set_doc.update(self._flatten_for_set(v, prefix=f\"coordinator.vars.{k}\"))\n            continue\n        for seg in k.split(\".\"):\n            self._validate_key_segment(seg)\n        set_doc[f\"coordinator.vars.{k}\"] = v\n\n    if not set_doc:\n        return {\"ok\": True, \"touched\": 0}\n\n    self._validate_paths_and_sizes(set_doc)\n\n    suspicious = [p for p, v in set_doc.items() if self._detector.is_sensitive(p, v)]\n    if suspicious and block_sensitive:\n        raise AdapterError(f\"vars.set blocked: {len(suspicious)} sensitive-looking values\")\n\n    await self.db.tasks.update_one(\n        {\"id\": task_id},\n        {\"$set\": set_doc, \"$currentDate\": {\"updated_at\": True}},\n    )\n\n    keys = sorted(set_doc.keys())\n    self.log.info(\n        \"vars.set\",\n        event=\"coord.vars.set\",\n        task_id=task_id,\n        keys=keys,\n        n=len(keys),\n        sensitive_hits=len(suspicious),\n    )\n    return {\"ok\": True, \"touched\": len(keys)}\n</code></pre>"},{"location":"reference/coordinator/#flowkit.coordinator.adapters.CoordinatorAdapters.vars_merge","title":"vars_merge  <code>async</code>","text":"<pre><code>vars_merge(task_id: str, **kv: Any) -&gt; dict[str, Any]\n</code></pre> <p>Deep-merge dict(s) into coordinator.vars. Non-dict leaves overwrite. Accepts {\"data\": {...}} or {\"kv\": {...}}.</p> Source code in <code>src/flowkit/coordinator/adapters.py</code> <pre><code>async def vars_merge(self, task_id: str, **kv: Any) -&gt; dict[str, Any]:\n    \"\"\"\n    Deep-merge dict(s) into coordinator.vars. Non-dict leaves overwrite.\n    Accepts {\"data\": {...}} or {\"kv\": {...}}.\n    \"\"\"\n    src = kv.get(\"data\", kv.get(\"kv\", kv))\n    if not isinstance(src, dict):\n        raise AdapterError(\"vars.merge expects a mapping under 'data' or 'kv'\")\n\n    set_doc = self._flatten_for_set(src, prefix=\"coordinator.vars\")\n    if not set_doc:\n        return {\"ok\": True, \"touched\": 0}\n\n    self._validate_paths_and_sizes(set_doc)\n    suspicious = [p for p, v in set_doc.items() if self._detector.is_sensitive(p, v)]\n    if kv.get(\"block_sensitive\") and suspicious:\n        raise AdapterError(f\"vars.merge blocked: {len(suspicious)} sensitive-looking values\")\n\n    existing = await self.db.tasks.find_one({\"id\": task_id}, {\"coordinator\": 1})\n    has_existing_vars = bool(((existing or {}).get(\"coordinator\") or {}).get(\"vars\"))\n    num_paths = len(set_doc)\n\n    await self.db.tasks.update_one(\n        {\"id\": task_id},\n        {\"$set\": set_doc, \"$currentDate\": {\"updated_at\": True}},\n    )\n\n    keys_sorted = sorted(set_doc.keys())\n    if not has_existing_vars and num_paths &gt; 1:\n        # Initial population with multiple paths \u2192 align logs with vars.set\n        self.log.info(\n            \"vars.set\",\n            event=\"coord.vars.set\",\n            task_id=task_id,\n            keys=keys_sorted,\n            n=num_paths,\n            sensitive_hits=len(suspicious),\n        )\n    else:\n        self.log.info(\n            \"vars.merge\",\n            event=\"coord.vars.merge\",\n            task_id=task_id,\n            keys=keys_sorted,\n            n=num_paths,\n            sensitive_hits=len(suspicious),\n        )\n    return {\"ok\": True, \"touched\": len(set_doc)}\n</code></pre>"},{"location":"reference/coordinator/#flowkit.coordinator.adapters.CoordinatorAdapters.vars_incr","title":"vars_incr  <code>async</code>","text":"<pre><code>vars_incr(task_id: str, *, key: str, by: int | float = 1) -&gt; dict[str, Any]\n</code></pre> <p>Atomically increment a numeric leaf under coordinator.vars.. Source code in <code>src/flowkit/coordinator/adapters.py</code> <pre><code>async def vars_incr(self, task_id: str, *, key: str, by: int | float = 1) -&gt; dict[str, Any]:\n    \"\"\"\n    Atomically increment a numeric leaf under coordinator.vars.&lt;key&gt;.\n    \"\"\"\n    if not isinstance(by, int | float):\n        raise AdapterError(\"'by' must be int or float\")\n    if not math.isfinite(float(by)):\n        raise AdapterError(\"'by' must be finite\")\n    if not isinstance(key, str) or not key:\n        raise AdapterError(\"'key' must be a non-empty string\")\n    for seg in key.split(\".\"):\n        self._validate_key_segment(seg)\n\n    path = f\"coordinator.vars.{key}\"\n\n    # Pre-check the existing type to surface a clean AdapterError instead of\n    # relying on storage-layer $inc behavior.\n    try:\n        existing = await self.db.tasks.find_one({\"id\": task_id}, {\"coordinator\": 1})\n    except Exception:\n        existing = None\n    if isinstance(existing, dict):\n        cur_val = self._get_path_value(existing, path)\n        if cur_val is not None and not isinstance(cur_val, int | float):\n            raise AdapterError(\"vars.incr expects a numeric leaf at target key\")\n\n    await self.db.tasks.update_one(\n        {\"id\": task_id},\n        {\"$inc\": {path: by}, \"$currentDate\": {\"updated_at\": True}},\n    )\n    self.log.info(\"vars.incr\", event=\"coord.vars.incr\", task_id=task_id, key=path, by=by)\n    return {\"ok\": True, \"key\": key, \"by\": by}\n</code></pre>"},{"location":"reference/coordinator/#flowkit.coordinator.adapters.CoordinatorAdapters.vars_unset","title":"vars_unset  <code>async</code>","text":"<pre><code>vars_unset(task_id: str, *paths: str, keys: Iterable[str] | None = None) -&gt; dict[str, Any]\n</code></pre> <p>Remove keys under <code>coordinator.vars</code> (dot-notation). No-op for missing keys.</p> Supports <p>await vars_unset(tid, \"a.b\", \"c.d\")</p> <p>and:     await vars_unset(tid, keys=[\"a.b\", \"c.d\"])</p> Source code in <code>src/flowkit/coordinator/adapters.py</code> <pre><code>async def vars_unset(\n    self,\n    task_id: str,\n    *paths: str,\n    keys: Iterable[str] | None = None,\n) -&gt; dict[str, Any]:\n    r\"\"\"\n    Remove keys under ``coordinator.vars`` (dot-notation). No-op for missing keys.\n\n    Supports:\n        await vars_unset(tid, \"a.b\", \"c.d\")\n    and:\n        await vars_unset(tid, keys=[\"a.b\", \"c.d\"])\n    \"\"\"\n    all_keys: list[str] = list(paths)\n    if keys is not None:\n        if isinstance(keys, str | bytes):\n            raise AdapterError(\"'keys' must be an iterable of strings\")\n        all_keys.extend([str(k) for k in keys])\n\n    if not all_keys:\n        return {\"ok\": True, \"touched\": 0}\n\n    unset_doc: dict[str, Any] = {}\n    for k in all_keys:\n        if not isinstance(k, str) or k == \"\":\n            raise AdapterError(\"unset key must be a non-empty string\")\n\n        for seg in k.split(\".\"):\n            self._validate_key_segment(seg)\n\n        path = f\"coordinator.vars.{k}\"\n        if len(path) &gt; self._limits[\"max_path_len\"]:\n            raise AdapterError(f\"path too long: {path!r}\")\n        if path.count(\".\") &gt;= self._limits[\"max_key_depth\"]:\n            raise AdapterError(f\"path too deep: {path!r}\")\n\n        unset_doc[path] = True  # value is ignored by Mongo\n\n    await self.db.tasks.update_one(\n        {\"id\": task_id},\n        {\"$unset\": unset_doc, \"$currentDate\": {\"updated_at\": True}},\n    )\n    self.log.info(\n        \"vars.unset\",\n        event=\"coord.vars.unset\",\n        task_id=task_id,\n        keys=sorted(unset_doc.keys()),\n        n=len(unset_doc),\n    )\n\n    # Second pass: prune empty parent objects that may remain after the unset.\n    try:\n        doc_after = await self.db.tasks.find_one({\"id\": task_id}, {\"coordinator\": 1})\n    except Exception:\n        doc_after = None\n\n    vars_tree = (((doc_after or {}).get(\"coordinator\") or {}).get(\"vars\")) or {}\n    if isinstance(vars_tree, dict):\n        prune_paths, _ = self._collect_empty_paths(vars_tree, prefix=\"coordinator.vars\")\n        if prune_paths:\n            # Issue a single $unset for all empty subtrees.\n            await self.db.tasks.update_one(\n                {\"id\": task_id},\n                {\"$unset\": {p: True for p in prune_paths}, \"$currentDate\": {\"updated_at\": True}},\n            )\n            self.log.info(\n                \"vars.unset.prune\",\n                event=\"coord.vars.unset.prune\",\n                task_id=task_id,\n                keys=sorted(prune_paths),\n                n=len(prune_paths),\n            )\n    return {\"ok\": True, \"touched\": len(unset_doc)}\n</code></pre>"},{"location":"reference/coordinator/#flowkit.coordinator.adapters.CoordinatorAdapters.merge_generic","title":"merge_generic  <code>async</code>","text":"<pre><code>merge_generic(task_id: str, from_nodes: list[str], target: dict[str, Any]) -&gt; dict[str, Any]\n</code></pre> <p>Merge metadata from multiple nodes and mark a target artifact as complete.</p> Source code in <code>src/flowkit/coordinator/adapters.py</code> <pre><code>async def merge_generic(self, task_id: str, from_nodes: list[str], target: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"\n    Merge metadata from multiple nodes and mark a target artifact as complete.\n    \"\"\"\n    if not target or not isinstance(target, dict):\n        raise AdapterError(\"merge_generic: 'target' must be a dict with at least node_id\")\n    target_node = target.get(\"node_id\") or \"coordinator\"\n\n    partial_batches = 0\n    complete_nodes = set()\n    batch_uids = set()\n\n    cur = self.db.artifacts.find({\"task_id\": task_id, \"node_id\": {\"$in\": from_nodes}})\n    async for a in cur:\n        st = a.get(\"status\")\n        if st == \"complete\":\n            complete_nodes.add(a.get(\"node_id\"))\n        elif st == \"partial\":\n            uid = a.get(\"batch_uid\")\n            if uid:\n                batch_uids.add(uid)\n            partial_batches += 1\n\n    meta = {\n        \"merged_from\": from_nodes,\n        \"complete_nodes\": sorted(list(complete_nodes)),\n        \"partial_batches\": partial_batches,\n        \"distinct_batch_uids\": len(batch_uids),\n        \"merged_at\": self.clock.now_dt().isoformat(),\n    }\n\n    await self.db.artifacts.update_one(\n        {\"task_id\": task_id, \"node_id\": target_node},\n        {\n            \"$set\": {\"status\": \"complete\", \"meta\": meta, \"updated_at\": self.clock.now_dt()},\n            \"$setOnInsert\": {\n                \"task_id\": task_id,\n                \"node_id\": target_node,\n                \"attempt_epoch\": 0,\n                \"created_at\": self.clock.now_dt(),\n            },\n        },\n        upsert=True,\n    )\n    return {\"ok\": True, \"meta\": meta}\n</code></pre>"},{"location":"reference/coordinator/#flowkit.coordinator.adapters.CoordinatorAdapters.metrics_aggregate","title":"metrics_aggregate  <code>async</code>","text":"<pre><code>metrics_aggregate(task_id: str, node_id: str, *, mode: str = 'sum') -&gt; dict[str, Any]\n</code></pre> <p>Aggregate metrics from raw entries and store the result on the node.</p> Source code in <code>src/flowkit/coordinator/adapters.py</code> <pre><code>async def metrics_aggregate(self, task_id: str, node_id: str, *, mode: str = \"sum\") -&gt; dict[str, Any]:\n    \"\"\"\n    Aggregate metrics from raw entries and store the result on the node.\n    \"\"\"\n    cur = self.db.metrics_raw.find({\"task_id\": task_id, \"node_id\": node_id, \"failed\": {\"$ne\": True}})\n    acc: dict[str, float] = {}\n    cnt: dict[str, int] = {}\n    async for m in cur:\n        for k, v in (m.get(\"metrics\") or {}).items():\n            try:\n                x = float(v)\n            except Exception:\n                continue\n            acc[k] = acc.get(k, 0.0) + x\n            cnt[k] = cnt.get(k, 0) + 1\n\n    out = {k: (acc[k] / max(1, cnt[k])) for k in acc} if mode == \"mean\" else {k: acc[k] for k in acc}\n\n    await self.db.tasks.update_one(\n        {\"id\": task_id, \"graph.nodes.node_id\": node_id},\n        {\"$set\": {\"graph.nodes.$.stats\": out, \"graph.nodes.$.stats_cached_at\": self.clock.now_dt()}},\n    )\n    return {\"ok\": True, \"mode\": mode, \"stats\": out}\n</code></pre>"},{"location":"reference/coordinator/#flowkit.coordinator.adapters-functions","title":"Functions","text":""},{"location":"reference/core/","title":"Core API Reference","text":""},{"location":"reference/core/#configuration-classes","title":"Configuration Classes","text":""},{"location":"reference/core/#flowkit.core.config.CoordinatorConfig","title":"flowkit.core.config.CoordinatorConfig  <code>dataclass</code>","text":"<pre><code>CoordinatorConfig(kafka_bootstrap: str = 'kafka:9092', worker_types: list[str] = (lambda: ['indexer', 'enricher', 'grouper', 'analyzer'])(), topic_cmd_fmt: str = 'cmd.{type}.v1', topic_status_fmt: str = 'status.{type}.v1', topic_worker_announce: str = 'workers.announce.v1', topic_query: str = 'query.tasks.v1', topic_reply: str = 'reply.tasks.v1', topic_signals: str = 'signals.v1', heartbeat_soft_sec: int = 300, heartbeat_hard_sec: int = 3600, lease_ttl_sec: int = 45, discovery_window_sec: int = 8, cancel_grace_sec: int = 30, scheduler_tick_sec: float = 1.0, finalizer_tick_sec: float = 5.0, hb_monitor_tick_sec: float = 10.0, outbox_dispatch_tick_sec: float = 0.25, outbox_max_retry: int = 12, outbox_backoff_min_ms: int = 250, outbox_backoff_max_ms: int = 60000, hb_soft_ms: int = 0, hb_hard_ms: int = 0, lease_ttl_ms: int = 0, discovery_window_ms: int = 0, cancel_grace_ms: int = 0, scheduler_tick_ms: int = 0, finalizer_tick_ms: int = 0, hb_monitor_tick_ms: int = 0, outbox_dispatch_tick_ms: int = 0)\n</code></pre>"},{"location":"reference/core/#flowkit.core.config.WorkerConfig","title":"flowkit.core.config.WorkerConfig  <code>dataclass</code>","text":"<pre><code>WorkerConfig(kafka_bootstrap: str = 'kafka:9092', topic_cmd_fmt: str = 'cmd.{type}.v1', topic_status_fmt: str = 'status.{type}.v1', topic_worker_announce: str = 'workers.announce.v1', topic_query: str = 'query.tasks.v1', topic_reply: str = 'reply.tasks.v1', topic_signals: str = 'signals.v1', roles: list[str] = (lambda: ['echo'])(), worker_id: str | None = None, worker_version: str = '2.0.0', lease_ttl_sec: int = 60, hb_interval_sec: int = 20, announce_interval_sec: int = 60, dedup_cache_size: int = 10000, dedup_ttl_ms: int = 3600000, pull_poll_ms_default: int = 300, pull_empty_backoff_ms_max: int = 4000, db_cancel_poll_ms: int = 500, strict_input_adapters: bool = False, lease_ttl_ms: int = 60000, hb_interval_ms: int = 20000, announce_interval_ms: int = 60000)\n</code></pre>"},{"location":"reference/core/#time-and-clock","title":"Time and Clock","text":""},{"location":"reference/core/#flowkit.core.time","title":"flowkit.core.time","text":""},{"location":"reference/core/#flowkit.core.time-classes","title":"Classes","text":""},{"location":"reference/core/#flowkit.core.time.SystemClock","title":"SystemClock","text":"<p>Default production/test clock.</p>"},{"location":"reference/core/#flowkit.core.time.ManualClock","title":"ManualClock","text":"<pre><code>ManualClock(start_ms: int = 0)\n</code></pre> <p>               Bases: <code>SystemClock</code></p> <p>Simple controllable clock for tests. - Wall time is advanced manually (affects persistence fields). - Monotonic time mirrors wall unless overridden.</p> Source code in <code>src/flowkit/core/time.py</code> <pre><code>def __init__(self, start_ms: int = 0) -&gt; None:\n    self._wall = start_ms\n    self._mono = start_ms\n</code></pre>"},{"location":"reference/core/#utilities","title":"Utilities","text":""},{"location":"reference/core/#flowkit.core.utils","title":"flowkit.core.utils","text":""},{"location":"reference/protocol/","title":"Protocol API Reference","text":""},{"location":"reference/protocol/#message-types-and-enums","title":"Message Types and Enums","text":""},{"location":"reference/protocol/#flowkit.protocol.messages","title":"flowkit.protocol.messages","text":""},{"location":"reference/testing/","title":"Test Suite Reference","text":"<p>This page is auto-generated from pytest docstrings at build time. Do not edit manually. See <code>docs/_scripts/gen_tests_doc.py</code>.</p> <p>Modules: 35 \u00a0\u00a0 Tests: 108</p>"},{"location":"reference/testing/#how-it-works","title":"How it works","text":"<p>The generator scans <code>tests/</code> for <code>test_*.py</code> modules, reads docstrings of modules, test classes, and test functions (including <code>async def</code>), and renders them here. The first sentence becomes a short summary in the index; the full docstring appears in Details.</p>"},{"location":"reference/testing/#writing-good-test-docstrings","title":"Writing good test docstrings","text":"<pre><code>async def test_outbox_retry_backoff():\n    \"\"\"\n    First two sends fail \u2192 item moves to *retry* with exponential backoff;\n    on the 3rd attempt the item becomes *sent*.\n    \"\"\"\n</code></pre>"},{"location":"reference/testing/#modules","title":"Modules","text":"<code>tests/e2e/vars/test_cancel_preserves_vars.py</code> 1 test (marks: asyncio) E2E: cancelling a running task does not erase previously written coordinator.vars. <ul> <li>test_cancel_midflow_preserves_coordinator_vars \u2014 (no docstring)</li> </ul> <code>tests/e2e/vars/test_conditional_routing_xfail.py</code> 1 test (marks: asyncio, xfail) XFails (placeholder): conditional routing by coordinator.vars is not implemented yet. <ul> <li>test_conditional_routing_by_vars \u2014 (no docstring)</li> </ul> <code>tests/e2e/vars/test_coord_fn_vars_echo_e2e.py</code> 1 test (marks: asyncio) E2E (slow): scheduler-driven coordinator_fn \u2192 coordinator_fn chain. <ul> <li>test_scheduler_runs_coordfn_chain_and_child_sees_vars \u2014 (no docstring)</li> </ul> <code>tests/integration/adapters/test_adapter_misc.py</code> 5 tests (marks: asyncio) <ul> <li>test_cmd_from_node_alias_supported \u2014 cmd.input_inline may provide 'from_node' (single) instead of 'from_nodes' (list).</li> <li>test_cmd_unknown_adapter_causes_permanent_fail \u2014 If cmd specifies an unknown adapter, the worker must fail the task permanently and must NOT call handler.iter_batches.</li> <li>test_fallback_to_iter_batches_when_no_adapter \u2014 If neither cmd nor handler specifies an adapter, worker must fallback to handler.iter_batches.</li> <li>test_handler_adapter_used_when_cmd_absent \u2014 When cmd does not specify an adapter, the worker may use the handler's adapter if it is known.</li> <li>test_handler_unknown_adapter_ignored_when_cmd_valid \u2014 If the handler proposes an unknown adapter but cmd provides a valid one, the worker must follow cmd and succeed.</li> </ul> <code>tests/integration/adapters/test_empty_upstream.py</code> 1 test (marks: asyncio) <ul> <li>test_empty_upstream_finishes_with_zero_count \u2014 (no docstring)</li> </ul> <code>tests/integration/adapters/test_input_adapter_contract.py</code> 3 tests (marks: asyncio) Integration tests for the orchestrator-first input adapter contract. <ul> <li>test_empty_upstream_is_not_error \u2014 Valid adapter + empty upstream \u2192 normal finish (count=0), not a failure.</li> <li>test_missing_required_args_yields_bad_input_args \u2014 cmd.input_inline with 'pull.from_artifacts.rechunk:size' must include 'size'.</li> <li>test_rechunk_without_meta_list_key_treats_each_artifact_as_single_item \u2014 No meta_list_key \u2192 each upstream artifact's meta is a single logical item (no heuristics).</li> </ul> <code>tests/integration/adapters/test_rechunk_args.py</code> 3 tests (marks: asyncio) <ul> <li>test_rechunk_invalid_meta_list_key_type_fails \u2014 (no docstring)</li> <li>test_rechunk_invalid_size_fails_early \u2014 (no docstring)</li> <li>test_rechunk_missing_size_fails_early \u2014 (no docstring)</li> </ul> <code>tests/integration/adapters/test_rechunk_determinism.py</code> 1 test (marks: asyncio) <ul> <li>test_rechunk_without_meta_key_treats_each_artifact_as_one_item \u2014 (no docstring)</li> </ul> <code>tests/integration/adapters/test_rechunk_strict_mode.py</code> 2 tests (marks: asyncio, cfg) <ul> <li>test_rechunk_requires_meta_key_in_strict_mode \u2014 In strict mode, rechunk without meta_list_key must fail early.</li> <li>test_rechunk_with_meta_key_passes_in_strict_mode \u2014 (no docstring)</li> </ul> <code>tests/integration/adapters/test_unknown_adapter.py</code> 1 test (marks: asyncio) <ul> <li>test_cmd_unknown_adapter_permanent_fail \u2014 (no docstring)</li> </ul> <code>tests/integration/adapters/test_worker_adapter_precedence.py</code> 1 test (marks: asyncio) <ul> <li>test_cmd_input_inline_overrides_handler_load_input \u2014 When configurations conflict, cmd.input_inline must take precedence over handler-provided load_input.</li> </ul> <code>tests/integration/streaming/test_isolation.py</code> 2 tests (marks: asyncio) Isolation of aggregation and cross-talk guard between tasks. <ul> <li>test_metrics_cross_talk_guard \u2014 Two concurrent tasks (same node_ids across different task_ids) do not interfere: final 'count' values are isolated per task.</li> <li>test_metrics_isolation_between_tasks \u2014 Two back-to-back tasks must keep metric aggregation isolated per task document.</li> </ul> <code>tests/integration/streaming/test_metrics_dedup.py</code> 3 tests (marks: asyncio) Idempotency &amp; event ordering for aggregated metrics. <ul> <li>test_metrics_dedup_persists_across_coord_restart \u2014 Metric deduplication survives a coordinator restart: duplicates of STATUS (BATCH_OK/TASK_DONE) sent during the restart must not double-count.</li> <li>test_metrics_idempotent_on_duplicate_status_events \u2014 Duplicate STATUS events (BATCH_OK / TASK_DONE) must not double-increment aggregated metrics.</li> <li>test_status_out_of_order_does_not_break_aggregation \u2014 BATCH_OK STATUS events arrive out of order \u2192 the aggregated metric remains correct.</li> </ul> <code>tests/integration/streaming/test_metrics_exactness.py</code> 3 tests (marks: asyncio) Exactness and accounting of aggregated metrics. <ul> <li>test_metrics_multistream_exact_sum \u2014 Three upstreams -&gt; one downstream: analyzer's aggregated 'count' must equal the sum of all totals.</li> <li>test_metrics_partial_batches_exact_count \u2014 With a remainder in the last upstream batch, analyzer's 'count' must still exactly equal total.</li> <li>test_metrics_single_stream_exact_count \u2014 Single upstream -&gt; single downstream: analyzer's aggregated 'count' must equal the total items.</li> </ul> <code>tests/integration/streaming/test_multistream_fanin.py</code> 1 test (marks: asyncio) Multistream fan-in to a single downstream consumer. <ul> <li>test_multistream_fanin_stream_to_one_downstream \u2014 Multi-stream fan-in: three upstream indexers stream into one analyzer.</li> </ul> <code>tests/integration/streaming/test_start_when.py</code> 2 tests (marks: asyncio) Tests for streaming/async fan-in behavior: early vs delayed downstream start. <ul> <li>test_after_upstream_complete_delays_start \u2014 Without start_when, the downstream must not start until the upstream is fully finished.</li> <li>test_start_when_first_batch_starts_early \u2014 Downstream should start while upstream is still running when `start_when=first_batch` is set.</li> </ul> <code>tests/integration/vars/test_coord_fn_chain.py</code> 1 test (marks: asyncio, parametrize) <ul> <li>test_coord_fn_chain_set_merge_incr_unset \u2014 Integration scenario:  n1: vars.set  \u2192 routing.sla=&lt;param&gt;, limits.max_batches=&lt;param&gt;  n2: vars.merge \u2192 flags=&lt;param&gt;  n3: vars.incr \u2192 counters.pages += &lt;param&gt;  n4: vars.unset \u2192 limits.max_batches Verifies:.</li> </ul> <code>tests/integration/vars/test_unset_forms.py</code> 1 test (marks: asyncio, parametrize) <ul> <li>test_vars_unset_kwargs_list_in_coord_fn \u2014 Coordinator function 'vars.unset' must accept kwargs form `keys=[...]`.</li> </ul> <code>tests/test_artifacts_data.py</code> 2 tests (marks: asyncio) <ul> <li>test_merge_generic_creates_complete_artifact \u2014 coordinator_fn: merge.generic combines results of nodes 'a' and 'b'.</li> <li>test_partial_shards_and_stream_counts \u2014 Source w1 (indexer) emits batches with batch_uid \u2192 worker creates partial artifacts.</li> </ul> <code>tests/test_cancel_and_restart.py</code> 5 tests (marks: asyncio) <ul> <li>test_cancel_before_any_start_keeps_all_nodes_idle \u2014 Cancel the task before any node can start: no node must enter 'running'.</li> <li>test_cancel_on_deferred_prevents_retry \u2014 Node 'flaky' fails on first attempt \u2192 becomes deferred with backoff.</li> <li>test_cascade_cancel_prevents_downstream \u2014 Cancel the task while upstream runs: downstream must not start.</li> <li>test_restart_higher_epoch_ignores_old_batch_ok \u2014 After a node restarts with a higher epoch, the coordinator must ignore a stale BATCH_OK from a lower epoch.</li> <li>test_restart_higher_epoch_ignores_old_events \u2014 After accepting epoch&gt;=1, re-inject an old event (epoch=0).</li> </ul> <code>tests/test_chaos_models.py</code> 4 tests (marks: asyncio) End-to-end streaming smoke tests under chaos. <ul> <li>test_chaos_coordinator_restart \u2014 Restart the Coordinator while the task is running.</li> <li>test_chaos_delays_and_duplications \u2014 Chaos mode: small broker/consumer jitter + message duplications (no drops).</li> <li>test_chaos_worker_restart_mid_stream \u2014 Restart the 'enricher' worker in the middle of the stream.</li> <li>test_e2e_streaming_with_kafka_chaos \u2014 With chaos enabled (broker/consumer jitter and message duplications, no drops), the full pipeline should still complete and produce artifacts at indexer/enricher/ocr stages.</li> </ul> <code>tests/test_concurrency_limits.py</code> 4 tests (marks: asyncio) <ul> <li>test_concurrent_tasks_respect_global_limit \u2014 (no docstring)</li> <li>test_max_global_running_limit \u2014 (no docstring)</li> <li>test_max_type_concurrency_limits \u2014 (no docstring)</li> <li>test_multi_workers_same_type_rr_distribution \u2014 (no docstring)</li> </ul> <code>tests/test_lease_heartbeat_resume.py</code> 11 tests (marks: asyncio, cfg) Heartbeat / grace-window / resume tests. <ul> <li>test_deferred_retry_ignores_grace_gate \u2014 A DEFERRED retry must not be throttled by discovery grace window.</li> <li>test_grace_gate_blocks_then_allows_after_window \u2014 Grace window should delay start initially and allow it after the window elapses.</li> <li>test_heartbeat_hard_marks_task_failed \u2014 If hard heartbeat window is exceeded, the task should be marked FAILED.</li> <li>test_heartbeat_soft_deferred_then_recovers \u2014 With a short soft heartbeat window the task becomes DEFERRED, then recovers and finishes.</li> <li>test_heartbeat_tolerates_clock_skew \u2014 Worker clock skew should not cause a hard timeout; lease deadlines must be non-decreasing.</li> <li>test_heartbeat_updates_lease_deadline_simple \u2014 Heartbeats should move the lease.deadline_ts_ms forward while the node runs.</li> <li>test_lease_expiry_cascades_cancel \u2014 Permanent fail upstream should cascade-cancel downstream nodes.</li> <li>test_no_task_resumed_on_worker_restart \u2014 On a cold worker restart there must be no TASK_RESUMED event emitted by the worker.</li> <li>test_resume_inflight_worker_restarts_with_local_state \u2014 Restart with the same worker_id: coordinator should adopt inflight work without new epoch.</li> <li>test_task_discover_complete_artifacts_skips_node_start \u2014 If artifacts are 'complete' during discovery, node should auto-finish without starting its handler.</li> <li>test_worker_restart_with_new_id_bumps_epoch \u2014 Restart with a new worker_id must bump attempt_epoch; stale heartbeats from the old epoch are ignored.</li> </ul> <code>tests/test_outbox_delivery.py</code> 6 tests (marks: asyncio, cfg, use_outbox) <ul> <li>test_outbox_backoff_caps_with_jitter \u2014 Exponential backoff is capped by max and jitter stays within a reasonable window.</li> <li>test_outbox_crash_between_send_and_mark_sent \u2014 Send succeeds, coordinator crashes before mark(sent) \u2192 after restart the outbox may resend, but there is only ONE effective delivery for (topic,key,dedup_id).</li> <li>test_outbox_dedup_survives_restart \u2014 Deduplication by (topic,key,dedup_id) persists across coordinator restart: re-enqueue of the same envelope after restart does not create a second outbox row or send.</li> <li>test_outbox_dlq_after_max_retries \u2014 After reaching max retries, the outbox row is moved to a terminal state with attempts &gt;= max.</li> <li>test_outbox_exactly_once_fp_uniqueness \u2014 Two enqueues with identical (topic,key,dedup_id) \u2192 single outbox row and exactly one real send.</li> <li>test_outbox_retry_backoff \u2014 First two _raw_send calls fail \u2192 outbox goes to 'retry' with exponential backoff (with jitter), then on the 3rd attempt becomes 'sent'.</li> </ul> <code>tests/test_pipeline_smoke.py</code> 1 test (marks: asyncio) End-to-end streaming smoke test. <ul> <li>test_e2e_streaming_with_kafka_sim \u2014 Full pipeline should complete and produce artifacts at indexer/enricher/ocr stages.</li> </ul> <code>tests/test_reliability_failures.py</code> 7 tests (marks: asyncio, cfg) Tests around source-like roles: idempotent metrics, retries, fencing, coordinator restart adoption, cascade cancel, and heartbeat/lease updates. <ul> <li>test_coordinator_restart_adopts_inflight_without_new_epoch \u2014 When the coordinator restarts, it should adopt in-flight work without incrementing the worker's attempt_epoch unnecessarily (i.e., source keeps epoch=1).</li> <li>test_explicit_cascade_cancel_moves_node_to_deferred \u2014 Explicit cascade cancel should move a running node to a cancelling/deferred/queued state within the configured cancel_grace window.</li> <li>test_heartbeat_updates_lease_deadline \u2014 Heartbeats from a worker must extend the lease deadline in the task document.</li> <li>test_idempotent_metrics_on_duplicate_events \u2014 Duplicated STATUS events (BATCH_OK/TASK_DONE) must not double-count metrics.</li> <li>test_permanent_fail_cascades_cancel_and_task_failed \u2014 Permanent failure in an upstream node should cause the task to fail, while dependents get cancelled/deferred/queued depending on race windows.</li> <li>test_status_fencing_ignores_stale_epoch \u2014 Status fencing must ignore events from a stale attempt_epoch.</li> <li>test_transient_failure_deferred_then_retry \u2014 A transient error should defer the node and succeed on retry according to retry_policy (max&gt;=2).</li> </ul> <code>tests/test_scheduler_fanin_routing.py</code> 7 tests (marks: asyncio, xfail) Fan-in behavior (ANY/ALL/COUNT:N) and coordinator_fn merge smoke tests. <ul> <li>test_coordinator_fn_merge_without_worker \u2014 coordinator_fn node should run without a worker and produce artifacts that a downstream analyzer can consume via pull.from_artifacts.</li> <li>test_edges_vs_routing_priority \u2014 If explicit graph edges are present and a node also has routing.on_success, edges should take precedence (routing target should not run).</li> <li>test_fanin_all_waits_all_parents \u2014 Fan-in ALL: without start_when hint, downstream should only start after both parents are finished.</li> <li>test_fanin_any_starts_early \u2014 Fan-in ANY: downstream should start as soon as at least one parent streams (start_when='first_batch'), even if other parents are not yet finished.</li> <li>test_fanin_count_n \u2014 Fan-in COUNT:N placeholder: downstream should start when at least N parents are ready.</li> <li>test_fanout_one_upstream_two_downstreams_mixed_start_when \u2014 One upstream \u2192 two downstreams: A has start_when=first_batch (starts early), B has no start_when (waits for completion).</li> <li>test_routing_on_failure_triggers_remediator_only \u2014 On upstream TASK_FAILED(permanent=True), only the 'on_failure' remediator should run; 'on_success' must not.</li> </ul> <code>tests/unit/vars/test_detectors.py</code> 3 tests (marks: asyncio) <ul> <li>test_detector_function_with_merge \u2014 DI: callable detector.</li> <li>test_detector_object_block_and_soft \u2014 DI: object detector.</li> <li>test_detector_string_import \u2014 DI: string import \"module:factory\".</li> </ul> <code>tests/unit/vars/test_incr.py</code> 5 tests (marks: asyncio, parametrize) <ul> <li>test_vars_incr_concurrency_sum \u2014 Concurrency: 10x1 + 5x2 \u2192 exact sum.</li> <li>test_vars_incr_creates_missing_leaf \u2014 Creating a missing leaf should create it and increment.</li> <li>test_vars_incr_float_and_negative_and_type_conflict \u2014 Mixed numeric types are allowed; negative increments also work.</li> <li>test_vars_incr_rejects_nan_inf \u2014 NaN / +/-Inf must be rejected.</li> <li>test_vars_incr_validates_key \u2014 Key validation (segments).</li> </ul> <code>tests/unit/vars/test_merge.py</code> 4 tests (marks: asyncio) <ul> <li>test_vars_merge_block_sensitive_with_external_detector \u2014 External detector: block when block_sensitive=True, otherwise write but count sensitive_hits in logs.</li> <li>test_vars_merge_deep_overwrite_mixed_types \u2014 Deep-merge with leaf overwrite; mixed types (dict \u2192 leaf).</li> <li>test_vars_merge_limits_and_value_size_str_and_bytes \u2014 Limits: number of paths, depth, full path length, and value size (str/bytes).</li> <li>test_vars_merge_noop_empty \u2014 Empty input \u2192 no-op (touched=0), 'vars' field is not created.</li> </ul> <code>tests/unit/vars/test_set.py</code> 7 tests (marks: asyncio, parametrize) <ul> <li>test_vars_set_block_sensitive_with_external_detector \u2014 With block_sensitive=True the detector blocks.</li> <li>test_vars_set_deterministic_keys_order_and_sensitive_hits \u2014 Deterministic sort order in logs + sensitive_hits counter.</li> <li>test_vars_set_dotted_and_nested_combo_and_logging \u2014 Dotted + nested forms in one call; verify values and structured logs.</li> <li>test_vars_set_limits_depth_and_path_len \u2014 Limits: key depth and full path length.</li> <li>test_vars_set_limits_paths_count \u2014 Limit on number of paths in a single $set.</li> <li>test_vars_set_limits_value_size_str_and_bytes \u2014 Limit on value size (string/bytes).</li> <li>test_vars_set_validates_key_segments \u2014 Validate segments: $, '.', NUL, segment length.</li> </ul> <code>tests/unit/vars/test_unset.py</code> 4 tests (marks: asyncio, parametrize) <ul> <li>test_vars_unset_accepts_args_and_kwargs \u2014 The adapter should accept both *args (varargs keys) and kwargs form 'keys=[...]'.</li> <li>test_vars_unset_path_length_limit \u2014 Validate full path length limit (including prefix 'coordinator.vars.').</li> <li>test_vars_unset_simple_nested_and_noop \u2014 Remove a simple leaf and a nested branch; missing key is a no-op (operation remains valid and accounted in 'touched').</li> <li>test_vars_unset_validates_paths \u2014 Validate key/path formats: $, '.', NUL, segment length.</li> </ul> <code>tests/unit/worker/test_error_classification.py</code> 1 test Unit test: default RoleHandler.classify_error marks config/programming errors as permanent. <ul> <li>test_programming_errors_are_permanent \u2014 (no docstring)</li> </ul> <code>tests/unit/worker/test_error_policy.py</code> 2 tests <ul> <li>test_classify_error_non_permanent_for_other_exceptions \u2014 (no docstring)</li> <li>test_classify_error_permanent_for_common_programming_faults \u2014 (no docstring)</li> </ul> <code>tests/unit/worker/test_pull_adapters_rechunk.py</code> 2 tests (marks: asyncio) Unit tests for PullAdapters.iter_from_artifacts_rechunk() selection rules. <ul> <li>test_rechunk_with_meta_list_key_splits_that_list \u2014 (no docstring)</li> <li>test_rechunk_without_meta_list_key_wraps_meta_as_single_item \u2014 (no docstring)</li> </ul>"},{"location":"reference/testing/#testse2evarstest_cancel_preserves_varspy","title":"tests/e2e/vars/test_cancel_preserves_vars.py","text":"<p>E2E: cancelling a running task does not erase previously written coordinator.vars.</p> <p>Scenario:   - n1: coordinator_fn (vars.set) writes task variables.   - s : long-running cancellable 'source' node (worker-based).   - Cancel the task while 's' is running. Assertions:   - Task (or node) transitions away from 'running' due to cancel (implementation-dependent).   - coordinator.vars written by n1 remain intact after cancellation.</p>"},{"location":"reference/testing/#tests","title":"Tests","text":"Test Summary Marks Location test_cancel_midflow_preserves_coordinator_vars (no docstring) asyncio <code>tests/e2e/vars/test_cancel_preserves_vars.py:27</code>"},{"location":"reference/testing/#details","title":"Details","text":""},{"location":"reference/testing/#test_cancel_midflow_preserves_coordinator_vars","title":"test_cancel_midflow_preserves_coordinator_vars","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/e2e/vars/test_cancel_preserves_vars.py:27</code></p> <p>No docstring.</p>"},{"location":"reference/testing/#testse2evarstest_conditional_routing_xfailpy","title":"tests/e2e/vars/test_conditional_routing_xfail.py","text":"<p>XFails (placeholder): conditional routing by coordinator.vars is not implemented yet.</p> <p>Intended behavior (when implemented):   - n1 sets coordinator.vars.routing.sla = \"gold\".   - Downstream 'gold_only' should run when sla == \"gold\".   - Downstream 'silver_only' should NOT run in this case.</p> <p>Current status:   - Marked xfail until coordinator supports variable-driven conditional routing.</p>"},{"location":"reference/testing/#tests_1","title":"Tests","text":"Test Summary Marks Location test_conditional_routing_by_vars (no docstring) asyncio, xfail <code>tests/e2e/vars/test_conditional_routing_xfail.py:25</code>"},{"location":"reference/testing/#details_1","title":"Details","text":""},{"location":"reference/testing/#test_conditional_routing_by_vars","title":"test_conditional_routing_by_vars","text":"<p>marks: <code>asyncio, xfail</code> \u2022 location: <code>tests/e2e/vars/test_conditional_routing_xfail.py:25</code></p> <p>No docstring.</p>"},{"location":"reference/testing/#testse2evarstest_coord_fn_vars_echo_e2epy","title":"tests/e2e/vars/test_coord_fn_vars_echo_e2e.py","text":"<p>E2E (slow): scheduler-driven coordinator_fn \u2192 coordinator_fn chain.</p> <p>Graph:   n1: coordinator_fn (vars.set)     \u2192 routing.sla=\"gold\", limits.max_batches=5   n2: coordinator_fn (vars.echo)    \u2192 reads coordinator.vars and writes an artifact echo</p> <p>Checks:   - scheduler starts nodes automatically (no manual _run_coordinator_fn)   - both nodes finish   - echo artifact exists for n2 and contains an exact copy of coordinator.vars   - updated_at present (and sane)</p>"},{"location":"reference/testing/#tests_2","title":"Tests","text":"Test Summary Marks Location test_scheduler_runs_coordfn_chain_and_child_sees_vars (no docstring) asyncio <code>tests/e2e/vars/test_coord_fn_vars_echo_e2e.py:26</code>"},{"location":"reference/testing/#details_2","title":"Details","text":""},{"location":"reference/testing/#test_scheduler_runs_coordfn_chain_and_child_sees_vars","title":"test_scheduler_runs_coordfn_chain_and_child_sees_vars","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/e2e/vars/test_coord_fn_vars_echo_e2e.py:26</code></p> <p>No docstring.</p>"},{"location":"reference/testing/#testsintegrationadapterstest_adapter_miscpy","title":"tests/integration/adapters/test_adapter_misc.py","text":""},{"location":"reference/testing/#tests_3","title":"Tests","text":"Test Summary Marks Location test_cmd_from_node_alias_supported cmd.input_inline may provide 'from_node' (single) instead of 'from_nodes' (list). asyncio <code>tests/integration/adapters/test_adapter_misc.py:213</code> test_cmd_unknown_adapter_causes_permanent_fail If cmd specifies an unknown adapter, the worker must fail the task permanently and must NOT call handler.iter_batches. asyncio <code>tests/integration/adapters/test_adapter_misc.py:179</code> test_fallback_to_iter_batches_when_no_adapter If neither cmd nor handler specifies an adapter, worker must fallback to handler.iter_batches. asyncio <code>tests/integration/adapters/test_adapter_misc.py:147</code> test_handler_adapter_used_when_cmd_absent When cmd does not specify an adapter, the worker may use the handler's adapter if it is known. asyncio <code>tests/integration/adapters/test_adapter_misc.py:102</code> test_handler_unknown_adapter_ignored_when_cmd_valid If the handler proposes an unknown adapter but cmd provides a valid one, the worker must follow cmd and succeed. asyncio <code>tests/integration/adapters/test_adapter_misc.py:262</code>"},{"location":"reference/testing/#details_3","title":"Details","text":""},{"location":"reference/testing/#test_cmd_from_node_alias_supported","title":"test_cmd_from_node_alias_supported","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/integration/adapters/test_adapter_misc.py:213</code></p> <p>cmd.input_inline may provide 'from_node' (single) instead of 'from_nodes' (list).</p>"},{"location":"reference/testing/#test_cmd_unknown_adapter_causes_permanent_fail","title":"test_cmd_unknown_adapter_causes_permanent_fail","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/integration/adapters/test_adapter_misc.py:179</code></p> <p>If cmd specifies an unknown adapter, the worker must fail the task permanently and must NOT call handler.iter_batches.</p>"},{"location":"reference/testing/#test_fallback_to_iter_batches_when_no_adapter","title":"test_fallback_to_iter_batches_when_no_adapter","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/integration/adapters/test_adapter_misc.py:147</code></p> <p>If neither cmd nor handler specifies an adapter, worker must fallback to handler.iter_batches.</p>"},{"location":"reference/testing/#test_handler_adapter_used_when_cmd_absent","title":"test_handler_adapter_used_when_cmd_absent","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/integration/adapters/test_adapter_misc.py:102</code></p> <p>When cmd does not specify an adapter, the worker may use the handler's adapter if it is known.</p>"},{"location":"reference/testing/#test_handler_unknown_adapter_ignored_when_cmd_valid","title":"test_handler_unknown_adapter_ignored_when_cmd_valid","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/integration/adapters/test_adapter_misc.py:262</code></p> <p>If the handler proposes an unknown adapter but cmd provides a valid one, the worker must follow cmd and succeed.</p>"},{"location":"reference/testing/#testsintegrationadapterstest_empty_upstreampy","title":"tests/integration/adapters/test_empty_upstream.py","text":""},{"location":"reference/testing/#tests_4","title":"Tests","text":"Test Summary Marks Location test_empty_upstream_finishes_with_zero_count (no docstring) asyncio <code>tests/integration/adapters/test_empty_upstream.py:21</code>"},{"location":"reference/testing/#details_4","title":"Details","text":""},{"location":"reference/testing/#test_empty_upstream_finishes_with_zero_count","title":"test_empty_upstream_finishes_with_zero_count","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/integration/adapters/test_empty_upstream.py:21</code></p> <p>No docstring.</p>"},{"location":"reference/testing/#testsintegrationadapterstest_input_adapter_contractpy","title":"tests/integration/adapters/test_input_adapter_contract.py","text":"<p>Integration tests for the orchestrator-first input adapter contract.</p> <p>Covers:   - Unknown adapter in cmd \u2192 permanent TASK_FAILED (bad_input_adapter).   - Missing required adapter args in cmd \u2192 permanent TASK_FAILED (bad_input_args).   - Empty upstream is not an error (finished with count=0).   - Rechunk without meta_list_key treats each artifact as a single logical item (no heuristics).</p>"},{"location":"reference/testing/#tests_5","title":"Tests","text":"Test Summary Marks Location test_empty_upstream_is_not_error Valid adapter + empty upstream \u2192 normal finish (count=0), not a failure. asyncio <code>tests/integration/adapters/test_input_adapter_contract.py:80</code> test_missing_required_args_yields_bad_input_args cmd.input_inline with 'pull.from_artifacts.rechunk:size' must include 'size'. asyncio <code>tests/integration/adapters/test_input_adapter_contract.py:49</code> test_rechunk_without_meta_list_key_treats_each_artifact_as_single_item No meta_list_key \u2192 each upstream artifact's meta is a single logical item (no heuristics). asyncio <code>tests/integration/adapters/test_input_adapter_contract.py:127</code>"},{"location":"reference/testing/#details_5","title":"Details","text":""},{"location":"reference/testing/#test_empty_upstream_is_not_error","title":"test_empty_upstream_is_not_error","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/integration/adapters/test_input_adapter_contract.py:80</code></p> <p>Valid adapter + empty upstream \u2192 normal finish (count=0), not a failure.</p>"},{"location":"reference/testing/#test_missing_required_args_yields_bad_input_args","title":"test_missing_required_args_yields_bad_input_args","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/integration/adapters/test_input_adapter_contract.py:49</code></p> <p>cmd.input_inline with 'pull.from_artifacts.rechunk:size' must include 'size'. Omission \u2192 early permanent failure (bad_input_args). Handler.iter_batches() must NOT be called.</p>"},{"location":"reference/testing/#test_rechunk_without_meta_list_key_treats_each_artifact_as_single_item","title":"test_rechunk_without_meta_list_key_treats_each_artifact_as_single_item","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/integration/adapters/test_input_adapter_contract.py:127</code></p> <p>No meta_list_key \u2192 each upstream artifact's meta is a single logical item (no heuristics). If upstream emits 2 partial artifacts, probe should aggregate 'count' == 2 (not N of inner lists).</p>"},{"location":"reference/testing/#testsintegrationadapterstest_rechunk_argspy","title":"tests/integration/adapters/test_rechunk_args.py","text":""},{"location":"reference/testing/#tests_6","title":"Tests","text":"Test Summary Marks Location test_rechunk_invalid_meta_list_key_type_fails (no docstring) asyncio <code>tests/integration/adapters/test_rechunk_args.py:93</code> test_rechunk_invalid_size_fails_early (no docstring) asyncio <code>tests/integration/adapters/test_rechunk_args.py:60</code> test_rechunk_missing_size_fails_early (no docstring) asyncio <code>tests/integration/adapters/test_rechunk_args.py:50</code>"},{"location":"reference/testing/#details_6","title":"Details","text":""},{"location":"reference/testing/#test_rechunk_invalid_meta_list_key_type_fails","title":"test_rechunk_invalid_meta_list_key_type_fails","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/integration/adapters/test_rechunk_args.py:93</code></p> <p>No docstring.</p>"},{"location":"reference/testing/#test_rechunk_invalid_size_fails_early","title":"test_rechunk_invalid_size_fails_early","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/integration/adapters/test_rechunk_args.py:60</code></p> <p>No docstring.</p>"},{"location":"reference/testing/#test_rechunk_missing_size_fails_early","title":"test_rechunk_missing_size_fails_early","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/integration/adapters/test_rechunk_args.py:50</code></p> <p>No docstring.</p>"},{"location":"reference/testing/#testsintegrationadapterstest_rechunk_determinismpy","title":"tests/integration/adapters/test_rechunk_determinism.py","text":""},{"location":"reference/testing/#tests_7","title":"Tests","text":"Test Summary Marks Location test_rechunk_without_meta_key_treats_each_artifact_as_one_item (no docstring) asyncio <code>tests/integration/adapters/test_rechunk_determinism.py:24</code>"},{"location":"reference/testing/#details_7","title":"Details","text":""},{"location":"reference/testing/#test_rechunk_without_meta_key_treats_each_artifact_as_one_item","title":"test_rechunk_without_meta_key_treats_each_artifact_as_one_item","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/integration/adapters/test_rechunk_determinism.py:24</code></p> <p>No docstring.</p>"},{"location":"reference/testing/#testsintegrationadapterstest_rechunk_strict_modepy","title":"tests/integration/adapters/test_rechunk_strict_mode.py","text":""},{"location":"reference/testing/#tests_8","title":"Tests","text":"Test Summary Marks Location test_rechunk_requires_meta_key_in_strict_mode In strict mode, rechunk without meta_list_key must fail early. asyncio, cfg <code>tests/integration/adapters/test_rechunk_strict_mode.py:15</code> test_rechunk_with_meta_key_passes_in_strict_mode (no docstring) asyncio, cfg <code>tests/integration/adapters/test_rechunk_strict_mode.py:60</code>"},{"location":"reference/testing/#details_8","title":"Details","text":""},{"location":"reference/testing/#test_rechunk_requires_meta_key_in_strict_mode","title":"test_rechunk_requires_meta_key_in_strict_mode","text":"<p>marks: <code>asyncio, cfg</code> \u2022 location: <code>tests/integration/adapters/test_rechunk_strict_mode.py:15</code></p> <p>In strict mode, rechunk without meta_list_key must fail early.</p>"},{"location":"reference/testing/#test_rechunk_with_meta_key_passes_in_strict_mode","title":"test_rechunk_with_meta_key_passes_in_strict_mode","text":"<p>marks: <code>asyncio, cfg</code> \u2022 location: <code>tests/integration/adapters/test_rechunk_strict_mode.py:60</code></p> <p>No docstring.</p>"},{"location":"reference/testing/#testsintegrationadapterstest_unknown_adapterpy","title":"tests/integration/adapters/test_unknown_adapter.py","text":""},{"location":"reference/testing/#tests_9","title":"Tests","text":"Test Summary Marks Location test_cmd_unknown_adapter_permanent_fail (no docstring) asyncio <code>tests/integration/adapters/test_unknown_adapter.py:24</code>"},{"location":"reference/testing/#details_9","title":"Details","text":""},{"location":"reference/testing/#test_cmd_unknown_adapter_permanent_fail","title":"test_cmd_unknown_adapter_permanent_fail","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/integration/adapters/test_unknown_adapter.py:24</code></p> <p>No docstring.</p>"},{"location":"reference/testing/#testsintegrationadapterstest_worker_adapter_precedencepy","title":"tests/integration/adapters/test_worker_adapter_precedence.py","text":""},{"location":"reference/testing/#tests_10","title":"Tests","text":"Test Summary Marks Location test_cmd_input_inline_overrides_handler_load_input When configurations conflict, cmd.input_inline must take precedence over handler-provided load_input. asyncio <code>tests/integration/adapters/test_worker_adapter_precedence.py:49</code>"},{"location":"reference/testing/#details_10","title":"Details","text":""},{"location":"reference/testing/#test_cmd_input_inline_overrides_handler_load_input","title":"test_cmd_input_inline_overrides_handler_load_input","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/integration/adapters/test_worker_adapter_precedence.py:49</code></p> <p>When configurations conflict, cmd.input_inline must take precedence over handler-provided load_input.</p>"},{"location":"reference/testing/#testsintegrationstreamingtest_isolationpy","title":"tests/integration/streaming/test_isolation.py","text":"<p>Isolation of aggregation and cross-talk guard between tasks.</p>"},{"location":"reference/testing/#tests_11","title":"Tests","text":"Test Summary Marks Location test_metrics_cross_talk_guard Two concurrent tasks (same node_ids across different task_ids) do not interfere: final 'count' values are isolated per task. asyncio <code>tests/integration/streaming/test_isolation.py:62</code> test_metrics_isolation_between_tasks Two back-to-back tasks must keep metric aggregation isolated per task document. asyncio <code>tests/integration/streaming/test_isolation.py:17</code>"},{"location":"reference/testing/#details_11","title":"Details","text":""},{"location":"reference/testing/#test_metrics_cross_talk_guard","title":"test_metrics_cross_talk_guard","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/integration/streaming/test_isolation.py:62</code></p> <p>Two concurrent tasks (same node_ids across different task_ids) do not interfere: final 'count' values are isolated per task.</p>"},{"location":"reference/testing/#test_metrics_isolation_between_tasks","title":"test_metrics_isolation_between_tasks","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/integration/streaming/test_isolation.py:17</code></p> <p>Two back-to-back tasks must keep metric aggregation isolated per task document.</p>"},{"location":"reference/testing/#testsintegrationstreamingtest_metrics_deduppy","title":"tests/integration/streaming/test_metrics_dedup.py","text":"<p>Idempotency &amp; event ordering for aggregated metrics.</p>"},{"location":"reference/testing/#tests_12","title":"Tests","text":"Test Summary Marks Location test_metrics_dedup_persists_across_coord_restart Metric deduplication survives a coordinator restart: duplicates of STATUS (BATCH_OK/TASK_DONE) sent during the restart must not double-count. asyncio <code>tests/integration/streaming/test_metrics_dedup.py:155</code> test_metrics_idempotent_on_duplicate_status_events Duplicate STATUS events (BATCH_OK / TASK_DONE) must not double-increment aggregated metrics. asyncio <code>tests/integration/streaming/test_metrics_dedup.py:21</code> test_status_out_of_order_does_not_break_aggregation BATCH_OK STATUS events arrive out of order \u2192 the aggregated metric remains correct. asyncio <code>tests/integration/streaming/test_metrics_dedup.py:82</code>"},{"location":"reference/testing/#details_12","title":"Details","text":""},{"location":"reference/testing/#test_metrics_dedup_persists_across_coord_restart","title":"test_metrics_dedup_persists_across_coord_restart","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/integration/streaming/test_metrics_dedup.py:155</code></p> <p>Metric deduplication survives a coordinator restart: duplicates of STATUS (BATCH_OK/TASK_DONE) sent during the restart must not double-count. Skips if the fixture <code>coord</code> has no <code>restart</code> method`.</p>"},{"location":"reference/testing/#test_metrics_idempotent_on_duplicate_status_events","title":"test_metrics_idempotent_on_duplicate_status_events","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/integration/streaming/test_metrics_dedup.py:21</code></p> <p>Duplicate STATUS events (BATCH_OK / TASK_DONE) must not double-increment aggregated metrics.</p> <p>We monkeypatch AIOKafkaProducerMock.send_and_wait to produce the same STATUS event twice for status topics. The Coordinator should deduplicate by envelope key and keep metrics stable.</p>"},{"location":"reference/testing/#test_status_out_of_order_does_not_break_aggregation","title":"test_status_out_of_order_does_not_break_aggregation","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/integration/streaming/test_metrics_dedup.py:82</code></p> <p>BATCH_OK STATUS events arrive out of order \u2192 the aggregated metric remains correct. We hold the first BATCH_OK so later ones arrive first.</p>"},{"location":"reference/testing/#testsintegrationstreamingtest_metrics_exactnesspy","title":"tests/integration/streaming/test_metrics_exactness.py","text":"<p>Exactness and accounting of aggregated metrics.</p>"},{"location":"reference/testing/#tests_13","title":"Tests","text":"Test Summary Marks Location test_metrics_multistream_exact_sum Three upstreams -&gt; one downstream: analyzer's aggregated 'count' must equal the sum of all totals. asyncio <code>tests/integration/streaming/test_metrics_exactness.py:56</code> test_metrics_partial_batches_exact_count With a remainder in the last upstream batch, analyzer's 'count' must still exactly equal total. asyncio <code>tests/integration/streaming/test_metrics_exactness.py:105</code> test_metrics_single_stream_exact_count Single upstream -&gt; single downstream: analyzer's aggregated 'count' must equal the total items. asyncio <code>tests/integration/streaming/test_metrics_exactness.py:15</code>"},{"location":"reference/testing/#details_13","title":"Details","text":""},{"location":"reference/testing/#test_metrics_multistream_exact_sum","title":"test_metrics_multistream_exact_sum","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/integration/streaming/test_metrics_exactness.py:56</code></p> <p>Three upstreams -&gt; one downstream: analyzer's aggregated 'count' must equal the sum of all totals.</p>"},{"location":"reference/testing/#test_metrics_partial_batches_exact_count","title":"test_metrics_partial_batches_exact_count","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/integration/streaming/test_metrics_exactness.py:105</code></p> <p>With a remainder in the last upstream batch, analyzer's 'count' must still exactly equal total.</p>"},{"location":"reference/testing/#test_metrics_single_stream_exact_count","title":"test_metrics_single_stream_exact_count","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/integration/streaming/test_metrics_exactness.py:15</code></p> <p>Single upstream -&gt; single downstream: analyzer's aggregated 'count' must equal the total items.</p>"},{"location":"reference/testing/#testsintegrationstreamingtest_multistream_faninpy","title":"tests/integration/streaming/test_multistream_fanin.py","text":"<p>Multistream fan-in to a single downstream consumer.</p>"},{"location":"reference/testing/#tests_14","title":"Tests","text":"Test Summary Marks Location test_multistream_fanin_stream_to_one_downstream Multi-stream fan-in: three upstream indexers stream into one analyzer. asyncio <code>tests/integration/streaming/test_multistream_fanin.py:17</code>"},{"location":"reference/testing/#details_14","title":"Details","text":""},{"location":"reference/testing/#test_multistream_fanin_stream_to_one_downstream","title":"test_multistream_fanin_stream_to_one_downstream","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/integration/streaming/test_multistream_fanin.py:17</code></p> <p>Multi-stream fan-in: three upstream indexers stream into one analyzer. Analyzer should start early on first batch and eventually see the full flow.</p>"},{"location":"reference/testing/#testsintegrationstreamingtest_start_whenpy","title":"tests/integration/streaming/test_start_when.py","text":"<p>Tests for streaming/async fan-in behavior: early vs delayed downstream start.</p>"},{"location":"reference/testing/#tests_15","title":"Tests","text":"Test Summary Marks Location test_after_upstream_complete_delays_start Without start_when, the downstream must not start until the upstream is fully finished. asyncio <code>tests/integration/streaming/test_start_when.py:85</code> test_start_when_first_batch_starts_early Downstream should start while upstream is still running when <code>start_when=first_batch</code> is set. asyncio <code>tests/integration/streaming/test_start_when.py:27</code>"},{"location":"reference/testing/#details_15","title":"Details","text":""},{"location":"reference/testing/#test_after_upstream_complete_delays_start","title":"test_after_upstream_complete_delays_start","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/integration/streaming/test_start_when.py:85</code></p> <p>Without start_when, the downstream must not start until the upstream is fully finished. We first assert a negative hold window while the upstream is running, then assert the downstream starts and finishes after the upstream completes.</p>"},{"location":"reference/testing/#test_start_when_first_batch_starts_early","title":"test_start_when_first_batch_starts_early","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/integration/streaming/test_start_when.py:27</code></p> <p>Downstream should start while upstream is still running when <code>start_when=first_batch</code> is set.</p>"},{"location":"reference/testing/#testsintegrationvarstest_coord_fn_chainpy","title":"tests/integration/vars/test_coord_fn_chain.py","text":""},{"location":"reference/testing/#tests_16","title":"Tests","text":"Test Summary Marks Location test_coord_fn_chain_set_merge_incr_unset Integration scenario:  n1: vars.set  \u2192 routing.sla=, limits.max_batches=  n2: vars.merge \u2192 flags=  n3: vars.incr \u2192 counters.pages +=   n4: vars.unset \u2192 limits.max_batches Verifies:. asyncio, parametrize <code>tests/integration/vars/test_coord_fn_chain.py:27</code>"},{"location":"reference/testing/#details_16","title":"Details","text":""},{"location":"reference/testing/#test_coord_fn_chain_set_merge_incr_unset","title":"test_coord_fn_chain_set_merge_incr_unset","text":"<p>marks: <code>asyncio, parametrize</code> \u2022 location: <code>tests/integration/vars/test_coord_fn_chain.py:27</code></p> <p>Integration scenario:   n1: vars.set   \u2192 routing.sla=, limits.max_batches=   n2: vars.merge \u2192 flags=   n3: vars.incr  \u2192 counters.pages +=    n4: vars.unset \u2192 limits.max_batches</p> <p>Verifies:   - each node finishes (status=finished)   - final coordinator.vars matches expected   - updated_at increases across steps (if tracked by inmemory_db)</p>"},{"location":"reference/testing/#testsintegrationvarstest_unset_formspy","title":"tests/integration/vars/test_unset_forms.py","text":""},{"location":"reference/testing/#tests_17","title":"Tests","text":"Test Summary Marks Location test_vars_unset_kwargs_list_in_coord_fn Coordinator function 'vars.unset' must accept kwargs form <code>keys=[...]</code>. asyncio, parametrize <code>tests/integration/vars/test_unset_forms.py:20</code>"},{"location":"reference/testing/#details_17","title":"Details","text":""},{"location":"reference/testing/#test_vars_unset_kwargs_list_in_coord_fn","title":"test_vars_unset_kwargs_list_in_coord_fn","text":"<p>marks: <code>asyncio, parametrize</code> \u2022 location: <code>tests/integration/vars/test_unset_forms.py:20</code></p> <p>Coordinator function 'vars.unset' must accept kwargs form <code>keys=[...]</code>.</p>"},{"location":"reference/testing/#teststest_artifacts_datapy","title":"tests/test_artifacts_data.py","text":""},{"location":"reference/testing/#tests_18","title":"Tests","text":"Test Summary Marks Location test_merge_generic_creates_complete_artifact coordinator_fn: merge.generic combines results of nodes 'a' and 'b'. asyncio <code>tests/test_artifacts_data.py:158</code> test_partial_shards_and_stream_counts Source w1 (indexer) emits batches with batch_uid \u2192 worker creates partial artifacts. asyncio <code>tests/test_artifacts_data.py:90</code>"},{"location":"reference/testing/#details_18","title":"Details","text":""},{"location":"reference/testing/#test_merge_generic_creates_complete_artifact","title":"test_merge_generic_creates_complete_artifact","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_artifacts_data.py:158</code></p> <p>coordinator_fn: merge.generic combines results of nodes 'a' and 'b'.</p> <p>We verify:   * the task finishes   * merge node 'm' has a 'complete' artifact   * nodes 'a' and 'b' have artifacts (partial/complete \u2014 does not matter)</p>"},{"location":"reference/testing/#test_partial_shards_and_stream_counts","title":"test_partial_shards_and_stream_counts","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_artifacts_data.py:90</code></p> <p>Source w1 (indexer) emits batches with batch_uid \u2192 worker creates partial artifacts. Completion marks a 'complete' artifact. Analyzer reads via rechunk and accumulates a counter.</p> <p>We verify:   * number of partial shards at w1 == ceil(total / batch_size)   * a 'complete' artifact exists at w1   * w2 has node.stats.count == total</p>"},{"location":"reference/testing/#teststest_cancel_and_restartpy","title":"tests/test_cancel_and_restart.py","text":""},{"location":"reference/testing/#tests_19","title":"Tests","text":"Test Summary Marks Location test_cancel_before_any_start_keeps_all_nodes_idle Cancel the task before any node can start: no node must enter 'running'. asyncio <code>tests/test_cancel_and_restart.py:231</code> test_cancel_on_deferred_prevents_retry Node 'flaky' fails on first attempt \u2192 becomes deferred with backoff. asyncio <code>tests/test_cancel_and_restart.py:295</code> test_cascade_cancel_prevents_downstream Cancel the task while upstream runs: downstream must not start. asyncio <code>tests/test_cancel_and_restart.py:97</code> test_restart_higher_epoch_ignores_old_batch_ok After a node restarts with a higher epoch, the coordinator must ignore a stale BATCH_OK from a lower epoch. asyncio <code>tests/test_cancel_and_restart.py:366</code> test_restart_higher_epoch_ignores_old_events After accepting epoch&gt;=1, re-inject an old event (epoch=0). asyncio <code>tests/test_cancel_and_restart.py:169</code>"},{"location":"reference/testing/#details_19","title":"Details","text":""},{"location":"reference/testing/#test_cancel_before_any_start_keeps_all_nodes_idle","title":"test_cancel_before_any_start_keeps_all_nodes_idle","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_cancel_and_restart.py:231</code></p> <p>Cancel the task before any node can start: no node must enter 'running'.</p>"},{"location":"reference/testing/#test_cancel_on_deferred_prevents_retry","title":"test_cancel_on_deferred_prevents_retry","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_cancel_and_restart.py:295</code></p> <p>Node 'flaky' fails on first attempt \u2192 becomes deferred with backoff. Cancel the task right after TASK_FAILED(epoch=1) and ensure no higher epoch is accepted.</p>"},{"location":"reference/testing/#test_cascade_cancel_prevents_downstream","title":"test_cascade_cancel_prevents_downstream","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_cancel_and_restart.py:97</code></p> <p>Cancel the task while upstream runs: downstream must not start.</p>"},{"location":"reference/testing/#test_restart_higher_epoch_ignores_old_batch_ok","title":"test_restart_higher_epoch_ignores_old_batch_ok","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_cancel_and_restart.py:366</code></p> <p>After a node restarts with a higher epoch, the coordinator must ignore a stale BATCH_OK from a lower epoch. Also ensure injected stale event doesn't create duplicate metrics.</p>"},{"location":"reference/testing/#test_restart_higher_epoch_ignores_old_events","title":"test_restart_higher_epoch_ignores_old_events","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_cancel_and_restart.py:169</code></p> <p>After accepting epoch&gt;=1, re-inject an old event (epoch=0). Coordinator must ignore it by fencing.</p>"},{"location":"reference/testing/#teststest_chaos_modelspy","title":"tests/test_chaos_models.py","text":"<p>End-to-end streaming smoke tests under chaos.</p> <p>Pipelines covered:   - w1(indexer) -&gt; w2(enricher) -&gt; w5(ocr) -&gt; w4(analyzer)                     ___________/           /                          _____ w3(merge) _/</p> <p>Checks:   - all nodes finish successfully (including the merge node in the extended graph)   - artifacts are produced for indexer/enricher/ocr (streaming stages)   - resiliency under worker and coordinator restarts</p>"},{"location":"reference/testing/#tests_20","title":"Tests","text":"Test Summary Marks Location test_chaos_coordinator_restart Restart the Coordinator while the task is running. asyncio <code>tests/test_chaos_models.py:344</code> test_chaos_delays_and_duplications Chaos mode: small broker/consumer jitter + message duplications (no drops). asyncio <code>tests/test_chaos_models.py:277</code> test_chaos_worker_restart_mid_stream Restart the 'enricher' worker in the middle of the stream. asyncio <code>tests/test_chaos_models.py:304</code> test_e2e_streaming_with_kafka_chaos With chaos enabled (broker/consumer jitter and message duplications, no drops), the full pipeline should still complete and produce artifacts at indexer/enricher/ocr stages. asyncio <code>tests/test_chaos_models.py:245</code>"},{"location":"reference/testing/#details_20","title":"Details","text":""},{"location":"reference/testing/#test_chaos_coordinator_restart","title":"test_chaos_coordinator_restart","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_chaos_models.py:344</code></p> <p>Restart the Coordinator while the task is running. Expect the pipeline to recover and finish.</p>"},{"location":"reference/testing/#test_chaos_delays_and_duplications","title":"test_chaos_delays_and_duplications","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_chaos_models.py:277</code></p> <p>Chaos mode: small broker/consumer jitter + message duplications (no drops). Expect the pipeline to finish and produce artifacts for w1/w2/w5.</p>"},{"location":"reference/testing/#test_chaos_worker_restart_mid_stream","title":"test_chaos_worker_restart_mid_stream","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_chaos_models.py:304</code></p> <p>Restart the 'enricher' worker in the middle of the stream. Expect the coordinator to fence and the pipeline to still finish.</p>"},{"location":"reference/testing/#test_e2e_streaming_with_kafka_chaos","title":"test_e2e_streaming_with_kafka_chaos","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_chaos_models.py:245</code></p> <p>With chaos enabled (broker/consumer jitter and message duplications, no drops), the full pipeline should still complete and produce artifacts at indexer/enricher/ocr stages.</p>"},{"location":"reference/testing/#teststest_concurrency_limitspy","title":"tests/test_concurrency_limits.py","text":""},{"location":"reference/testing/#tests_21","title":"Tests","text":"Test Summary Marks Location test_concurrent_tasks_respect_global_limit (no docstring) asyncio <code>tests/test_concurrency_limits.py:302</code> test_max_global_running_limit (no docstring) asyncio <code>tests/test_concurrency_limits.py:153</code> test_max_type_concurrency_limits (no docstring) asyncio <code>tests/test_concurrency_limits.py:196</code> test_multi_workers_same_type_rr_distribution (no docstring) asyncio <code>tests/test_concurrency_limits.py:252</code>"},{"location":"reference/testing/#details_21","title":"Details","text":""},{"location":"reference/testing/#test_concurrent_tasks_respect_global_limit","title":"test_concurrent_tasks_respect_global_limit","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_concurrency_limits.py:302</code></p> <p>No docstring.</p>"},{"location":"reference/testing/#test_max_global_running_limit","title":"test_max_global_running_limit","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_concurrency_limits.py:153</code></p> <p>No docstring.</p>"},{"location":"reference/testing/#test_max_type_concurrency_limits","title":"test_max_type_concurrency_limits","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_concurrency_limits.py:196</code></p> <p>No docstring.</p>"},{"location":"reference/testing/#test_multi_workers_same_type_rr_distribution","title":"test_multi_workers_same_type_rr_distribution","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_concurrency_limits.py:252</code></p> <p>No docstring.</p>"},{"location":"reference/testing/#teststest_lease_heartbeat_resumepy","title":"tests/test_lease_heartbeat_resume.py","text":"<p>Heartbeat / grace-window / resume tests.</p> <p>Covers: - Soft heartbeat -&gt; deferred -&gt; recovery - Hard heartbeat -&gt; task failed - Resume with local worker state (no new epoch) - Discover w/ complete artifacts -&gt; skip start - Grace-gate delay, and that deferred retry ignores the gate - Heartbeat extends lease deadline - Restart with a new worker_id bumps attempt_epoch and fences stale heartbeats</p>"},{"location":"reference/testing/#tests_22","title":"Tests","text":"Test Summary Marks Location test_deferred_retry_ignores_grace_gate A DEFERRED retry must not be throttled by discovery grace window. cfg, asyncio <code>tests/test_lease_heartbeat_resume.py:258</code> test_grace_gate_blocks_then_allows_after_window Grace window should delay start initially and allow it after the window elapses. cfg, asyncio <code>tests/test_lease_heartbeat_resume.py:209</code> test_heartbeat_hard_marks_task_failed If hard heartbeat window is exceeded, the task should be marked FAILED. cfg, asyncio <code>tests/test_lease_heartbeat_resume.py:77</code> test_heartbeat_soft_deferred_then_recovers With a short soft heartbeat window the task becomes DEFERRED, then recovers and finishes. cfg, asyncio <code>tests/test_lease_heartbeat_resume.py:39</code> test_heartbeat_tolerates_clock_skew Worker clock skew should not cause a hard timeout; lease deadlines must be non-decreasing. asyncio, cfg <code>tests/test_lease_heartbeat_resume.py:556</code> test_heartbeat_updates_lease_deadline_simple Heartbeats should move the lease.deadline_ts_ms forward while the node runs. cfg, asyncio <code>tests/test_lease_heartbeat_resume.py:348</code> test_lease_expiry_cascades_cancel Permanent fail upstream should cascade-cancel downstream nodes. asyncio, cfg <code>tests/test_lease_heartbeat_resume.py:625</code> test_no_task_resumed_on_worker_restart On a cold worker restart there must be no TASK_RESUMED event emitted by the worker. cfg, asyncio <code>tests/test_lease_heartbeat_resume.py:292</code> test_resume_inflight_worker_restarts_with_local_state Restart with the same worker_id: coordinator should adopt inflight work without new epoch. cfg, asyncio <code>tests/test_lease_heartbeat_resume.py:113</code> test_task_discover_complete_artifacts_skips_node_start If artifacts are 'complete' during discovery, node should auto-finish without starting its handler. asyncio <code>tests/test_lease_heartbeat_resume.py:173</code> test_worker_restart_with_new_id_bumps_epoch Restart with a new worker_id must bump attempt_epoch; stale heartbeats from the old epoch are ignored. asyncio, cfg <code>tests/test_lease_heartbeat_resume.py:394</code>"},{"location":"reference/testing/#details_22","title":"Details","text":""},{"location":"reference/testing/#test_deferred_retry_ignores_grace_gate","title":"test_deferred_retry_ignores_grace_gate","text":"<p>marks: <code>cfg, asyncio</code> \u2022 location: <code>tests/test_lease_heartbeat_resume.py:258</code></p> <p>A DEFERRED retry must not be throttled by discovery grace window.</p>"},{"location":"reference/testing/#test_grace_gate_blocks_then_allows_after_window","title":"test_grace_gate_blocks_then_allows_after_window","text":"<p>marks: <code>cfg, asyncio</code> \u2022 location: <code>tests/test_lease_heartbeat_resume.py:209</code></p> <p>Grace window should delay start initially and allow it after the window elapses.</p>"},{"location":"reference/testing/#test_heartbeat_hard_marks_task_failed","title":"test_heartbeat_hard_marks_task_failed","text":"<p>marks: <code>cfg, asyncio</code> \u2022 location: <code>tests/test_lease_heartbeat_resume.py:77</code></p> <p>If hard heartbeat window is exceeded, the task should be marked FAILED.</p>"},{"location":"reference/testing/#test_heartbeat_soft_deferred_then_recovers","title":"test_heartbeat_soft_deferred_then_recovers","text":"<p>marks: <code>cfg, asyncio</code> \u2022 location: <code>tests/test_lease_heartbeat_resume.py:39</code></p> <p>With a short soft heartbeat window the task becomes DEFERRED, then recovers and finishes.</p>"},{"location":"reference/testing/#test_heartbeat_tolerates_clock_skew","title":"test_heartbeat_tolerates_clock_skew","text":"<p>marks: <code>asyncio, cfg</code> \u2022 location: <code>tests/test_lease_heartbeat_resume.py:556</code></p> <p>Worker clock skew should not cause a hard timeout; lease deadlines must be non-decreasing.</p>"},{"location":"reference/testing/#test_heartbeat_updates_lease_deadline_simple","title":"test_heartbeat_updates_lease_deadline_simple","text":"<p>marks: <code>cfg, asyncio</code> \u2022 location: <code>tests/test_lease_heartbeat_resume.py:348</code></p> <p>Heartbeats should move the lease.deadline_ts_ms forward while the node runs.</p>"},{"location":"reference/testing/#test_lease_expiry_cascades_cancel","title":"test_lease_expiry_cascades_cancel","text":"<p>marks: <code>asyncio, cfg</code> \u2022 location: <code>tests/test_lease_heartbeat_resume.py:625</code></p> <p>Permanent fail upstream should cascade-cancel downstream nodes.</p>"},{"location":"reference/testing/#test_no_task_resumed_on_worker_restart","title":"test_no_task_resumed_on_worker_restart","text":"<p>marks: <code>cfg, asyncio</code> \u2022 location: <code>tests/test_lease_heartbeat_resume.py:292</code></p> <p>On a cold worker restart there must be no TASK_RESUMED event emitted by the worker.</p>"},{"location":"reference/testing/#test_resume_inflight_worker_restarts_with_local_state","title":"test_resume_inflight_worker_restarts_with_local_state","text":"<p>marks: <code>cfg, asyncio</code> \u2022 location: <code>tests/test_lease_heartbeat_resume.py:113</code></p> <p>Restart with the same worker_id: coordinator should adopt inflight work without new epoch.</p>"},{"location":"reference/testing/#test_task_discover_complete_artifacts_skips_node_start","title":"test_task_discover_complete_artifacts_skips_node_start","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_lease_heartbeat_resume.py:173</code></p> <p>If artifacts are 'complete' during discovery, node should auto-finish without starting its handler.</p>"},{"location":"reference/testing/#test_worker_restart_with_new_id_bumps_epoch","title":"test_worker_restart_with_new_id_bumps_epoch","text":"<p>marks: <code>asyncio, cfg</code> \u2022 location: <code>tests/test_lease_heartbeat_resume.py:394</code></p> <p>Restart with a new worker_id must bump attempt_epoch; stale heartbeats from the old epoch are ignored.</p>"},{"location":"reference/testing/#teststest_outbox_deliverypy","title":"tests/test_outbox_delivery.py","text":""},{"location":"reference/testing/#tests_23","title":"Tests","text":"Test Summary Marks Location test_outbox_backoff_caps_with_jitter Exponential backoff is capped by max and jitter stays within a reasonable window. asyncio, use_outbox, cfg <code>tests/test_outbox_delivery.py:366</code> test_outbox_crash_between_send_and_mark_sent Send succeeds, coordinator crashes before mark(sent) \u2192 after restart the outbox may resend, but there is only ONE effective delivery for (topic,key,dedup_id). asyncio, use_outbox <code>tests/test_outbox_delivery.py:214</code> test_outbox_dedup_survives_restart Deduplication by (topic,key,dedup_id) persists across coordinator restart: re-enqueue of the same envelope after restart does not create a second outbox row or send. asyncio, use_outbox <code>tests/test_outbox_delivery.py:305</code> test_outbox_dlq_after_max_retries After reaching max retries, the outbox row is moved to a terminal state with attempts &gt;= max. asyncio, use_outbox, cfg <code>tests/test_outbox_delivery.py:419</code> test_outbox_exactly_once_fp_uniqueness Two enqueues with identical (topic,key,dedup_id) \u2192 single outbox row and exactly one real send. asyncio <code>tests/test_outbox_delivery.py:138</code> test_outbox_retry_backoff First two _raw_send calls fail \u2192 outbox goes to 'retry' with exponential backoff (with jitter), then on the 3<sup>rd</sup> attempt becomes 'sent'. asyncio <code>tests/test_outbox_delivery.py:46</code>"},{"location":"reference/testing/#details_23","title":"Details","text":""},{"location":"reference/testing/#test_outbox_backoff_caps_with_jitter","title":"test_outbox_backoff_caps_with_jitter","text":"<p>marks: <code>asyncio, use_outbox, cfg</code> \u2022 location: <code>tests/test_outbox_delivery.py:366</code></p> <p>Exponential backoff is capped by max and jitter stays within a reasonable window.</p>"},{"location":"reference/testing/#test_outbox_crash_between_send_and_mark_sent","title":"test_outbox_crash_between_send_and_mark_sent","text":"<p>marks: <code>asyncio, use_outbox</code> \u2022 location: <code>tests/test_outbox_delivery.py:214</code></p> <p>Send succeeds, coordinator crashes before mark(sent) \u2192 after restart the outbox may resend, but there is only ONE effective delivery for (topic,key,dedup_id).</p>"},{"location":"reference/testing/#test_outbox_dedup_survives_restart","title":"test_outbox_dedup_survives_restart","text":"<p>marks: <code>asyncio, use_outbox</code> \u2022 location: <code>tests/test_outbox_delivery.py:305</code></p> <p>Deduplication by (topic,key,dedup_id) persists across coordinator restart: re-enqueue of the same envelope after restart does not create a second outbox row or send.</p>"},{"location":"reference/testing/#test_outbox_dlq_after_max_retries","title":"test_outbox_dlq_after_max_retries","text":"<p>marks: <code>asyncio, use_outbox, cfg</code> \u2022 location: <code>tests/test_outbox_delivery.py:419</code></p> <p>After reaching max retries, the outbox row is moved to a terminal state with attempts &gt;= max.</p>"},{"location":"reference/testing/#test_outbox_exactly_once_fp_uniqueness","title":"test_outbox_exactly_once_fp_uniqueness","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_outbox_delivery.py:138</code></p> <p>Two enqueues with identical (topic,key,dedup_id) \u2192 single outbox row and exactly one real send.</p>"},{"location":"reference/testing/#test_outbox_retry_backoff","title":"test_outbox_retry_backoff","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_outbox_delivery.py:46</code></p> <p>First two _raw_send calls fail \u2192 outbox goes to 'retry' with exponential backoff (with jitter), then on the 3<sup>rd</sup> attempt becomes 'sent'.</p>"},{"location":"reference/testing/#teststest_pipeline_smokepy","title":"tests/test_pipeline_smoke.py","text":"<p>End-to-end streaming smoke test.</p> <p>Pipeline:   w1(indexer) -&gt; w2(enricher) -&gt; w5(ocr) -&gt; w4(analyzer)                  ___________/           /                       _____ w3(merge) _/</p> <p>Checks:   - all nodes finish successfully   - artifacts are produced for indexer/enricher/ocr</p>"},{"location":"reference/testing/#tests_24","title":"Tests","text":"Test Summary Marks Location test_e2e_streaming_with_kafka_sim Full pipeline should complete and produce artifacts at indexer/enricher/ocr stages. asyncio <code>tests/test_pipeline_smoke.py:135</code>"},{"location":"reference/testing/#details_24","title":"Details","text":""},{"location":"reference/testing/#test_e2e_streaming_with_kafka_sim","title":"test_e2e_streaming_with_kafka_sim","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_pipeline_smoke.py:135</code></p> <p>Full pipeline should complete and produce artifacts at indexer/enricher/ocr stages.</p>"},{"location":"reference/testing/#teststest_reliability_failurespy","title":"tests/test_reliability_failures.py","text":"<p>Tests around source-like roles: idempotent metrics, retries, fencing, coordinator restart adoption, cascade cancel, and heartbeat/lease updates.</p> <p>Design: - All runtime knobs come via coord_cfg/worker_cfg fixtures (see conftest.py). - For one-off tweaks, use @pytest.mark.cfg(coord={...}, worker={...}). - Workers are started with in-memory DB and handlers built from helper builders.</p>"},{"location":"reference/testing/#tests_25","title":"Tests","text":"Test Summary Marks Location test_coordinator_restart_adopts_inflight_without_new_epoch When the coordinator restarts, it should adopt in-flight work without incrementing the worker's attempt_epoch unnecessarily (i.e., source keeps epoch=1). asyncio <code>tests/test_reliability_failures.py:218</code> test_explicit_cascade_cancel_moves_node_to_deferred Explicit cascade cancel should move a running node to a cancelling/deferred/queued state within the configured cancel_grace window. cfg, asyncio <code>tests/test_reliability_failures.py:273</code> test_heartbeat_updates_lease_deadline Heartbeats from a worker must extend the lease deadline in the task document. cfg, asyncio <code>tests/test_reliability_failures.py:322</code> test_idempotent_metrics_on_duplicate_events Duplicated STATUS events (BATCH_OK/TASK_DONE) must not double-count metrics. asyncio <code>tests/test_reliability_failures.py:36</code> test_permanent_fail_cascades_cancel_and_task_failed Permanent failure in an upstream node should cause the task to fail, while dependents get cancelled/deferred/queued depending on race windows. cfg, asyncio <code>tests/test_reliability_failures.py:125</code> test_status_fencing_ignores_stale_epoch Status fencing must ignore events from a stale attempt_epoch. asyncio <code>tests/test_reliability_failures.py:169</code> test_transient_failure_deferred_then_retry A transient error should defer the node and succeed on retry according to retry_policy (max&gt;=2). asyncio <code>tests/test_reliability_failures.py:83</code>"},{"location":"reference/testing/#details_25","title":"Details","text":""},{"location":"reference/testing/#test_coordinator_restart_adopts_inflight_without_new_epoch","title":"test_coordinator_restart_adopts_inflight_without_new_epoch","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_reliability_failures.py:218</code></p> <p>When the coordinator restarts, it should adopt in-flight work without incrementing the worker's attempt_epoch unnecessarily (i.e., source keeps epoch=1).</p>"},{"location":"reference/testing/#test_explicit_cascade_cancel_moves_node_to_deferred","title":"test_explicit_cascade_cancel_moves_node_to_deferred","text":"<p>marks: <code>cfg, asyncio</code> \u2022 location: <code>tests/test_reliability_failures.py:273</code></p> <p>Explicit cascade cancel should move a running node to a cancelling/deferred/queued state within the configured cancel_grace window.</p>"},{"location":"reference/testing/#test_heartbeat_updates_lease_deadline","title":"test_heartbeat_updates_lease_deadline","text":"<p>marks: <code>cfg, asyncio</code> \u2022 location: <code>tests/test_reliability_failures.py:322</code></p> <p>Heartbeats from a worker must extend the lease deadline in the task document.</p>"},{"location":"reference/testing/#test_idempotent_metrics_on_duplicate_events","title":"test_idempotent_metrics_on_duplicate_events","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_reliability_failures.py:36</code></p> <p>Duplicated STATUS events (BATCH_OK/TASK_DONE) must not double-count metrics. We duplicate STATUS envelopes at the Kafka level; aggregator must remain stable.</p>"},{"location":"reference/testing/#test_permanent_fail_cascades_cancel_and_task_failed","title":"test_permanent_fail_cascades_cancel_and_task_failed","text":"<p>marks: <code>cfg, asyncio</code> \u2022 location: <code>tests/test_reliability_failures.py:125</code></p> <p>Permanent failure in an upstream node should cause the task to fail, while dependents get cancelled/deferred/queued depending on race windows.</p>"},{"location":"reference/testing/#test_status_fencing_ignores_stale_epoch","title":"test_status_fencing_ignores_stale_epoch","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_reliability_failures.py:169</code></p> <p>Status fencing must ignore events from a stale attempt_epoch. We finish the task, then send a forged event with attempt_epoch=0; stats must remain unchanged.</p>"},{"location":"reference/testing/#test_transient_failure_deferred_then_retry","title":"test_transient_failure_deferred_then_retry","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_reliability_failures.py:83</code></p> <p>A transient error should defer the node and succeed on retry according to retry_policy (max&gt;=2).</p>"},{"location":"reference/testing/#teststest_scheduler_fanin_routingpy","title":"tests/test_scheduler_fanin_routing.py","text":"<p>Fan-in behavior (ANY/ALL/COUNT:N) and coordinator_fn merge smoke tests.</p> <p>Covers:   - ANY fan-in: downstream starts as soon as any parent streams a first batch.   - ALL fan-in: downstream starts only after all parents are done (no start_when).   - COUNT:N fan-in: xfail placeholder until the coordinator supports it.   - Edges vs routing priority: xfail placeholder for precedence logic.   - coordinator_fn merge: runs without a worker and feeds downstream via artifacts.</p>"},{"location":"reference/testing/#tests_26","title":"Tests","text":"Test Summary Marks Location test_coordinator_fn_merge_without_worker coordinator_fn node should run without a worker and produce artifacts that a downstream analyzer can consume via pull.from_artifacts. asyncio <code>tests/test_scheduler_fanin_routing.py:294</code> test_edges_vs_routing_priority If explicit graph edges are present and a node also has routing.on_success, edges should take precedence (routing target should not run). asyncio, xfail <code>tests/test_scheduler_fanin_routing.py:230</code> test_fanin_all_waits_all_parents Fan-in ALL: without start_when hint, downstream should only start after both parents are finished. asyncio <code>tests/test_scheduler_fanin_routing.py:127</code> test_fanin_any_starts_early Fan-in ANY: downstream should start as soon as at least one parent streams (start_when='first_batch'), even if other parents are not yet finished. asyncio <code>tests/test_scheduler_fanin_routing.py:69</code> test_fanin_count_n Fan-in COUNT:N placeholder: downstream should start when at least N parents are ready. asyncio, xfail <code>tests/test_scheduler_fanin_routing.py:176</code> test_fanout_one_upstream_two_downstreams_mixed_start_when One upstream \u2192 two downstreams: A has start_when=first_batch (starts early), B has no start_when (waits for completion). asyncio <code>tests/test_scheduler_fanin_routing.py:431</code> test_routing_on_failure_triggers_remediator_only On upstream TASK_FAILED(permanent=True), only the 'on_failure' remediator should run; 'on_success' must not. asyncio, xfail <code>tests/test_scheduler_fanin_routing.py:358</code>"},{"location":"reference/testing/#details_26","title":"Details","text":""},{"location":"reference/testing/#test_coordinator_fn_merge_without_worker","title":"test_coordinator_fn_merge_without_worker","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_scheduler_fanin_routing.py:294</code></p> <p>coordinator_fn node should run without a worker and produce artifacts that a downstream analyzer can consume via pull.from_artifacts.</p>"},{"location":"reference/testing/#test_edges_vs_routing_priority","title":"test_edges_vs_routing_priority","text":"<p>marks: <code>asyncio, xfail</code> \u2022 location: <code>tests/test_scheduler_fanin_routing.py:230</code></p> <p>If explicit graph edges are present and a node also has routing.on_success, edges should take precedence (routing target should not run).</p>"},{"location":"reference/testing/#test_fanin_all_waits_all_parents","title":"test_fanin_all_waits_all_parents","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_scheduler_fanin_routing.py:127</code></p> <p>Fan-in ALL: without start_when hint, downstream should only start after both parents are finished.</p>"},{"location":"reference/testing/#test_fanin_any_starts_early","title":"test_fanin_any_starts_early","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_scheduler_fanin_routing.py:69</code></p> <p>Fan-in ANY: downstream should start as soon as at least one parent streams (start_when='first_batch'), even if other parents are not yet finished. We run a single 'indexer' worker so upstream parents execute sequentially.</p>"},{"location":"reference/testing/#test_fanin_count_n","title":"test_fanin_count_n","text":"<p>marks: <code>asyncio, xfail</code> \u2022 location: <code>tests/test_scheduler_fanin_routing.py:176</code></p> <p>Fan-in COUNT:N placeholder: downstream should start when at least N parents are ready. Marked xfail until coordinator supports 'count:n'.</p>"},{"location":"reference/testing/#test_fanout_one_upstream_two_downstreams_mixed_start_when","title":"test_fanout_one_upstream_two_downstreams_mixed_start_when","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_scheduler_fanin_routing.py:431</code></p> <p>One upstream \u2192 two downstreams: A has start_when=first_batch (starts early), B has no start_when (waits for completion).</p>"},{"location":"reference/testing/#test_routing_on_failure_triggers_remediator_only","title":"test_routing_on_failure_triggers_remediator_only","text":"<p>marks: <code>asyncio, xfail</code> \u2022 location: <code>tests/test_scheduler_fanin_routing.py:358</code></p> <p>On upstream TASK_FAILED(permanent=True), only the 'on_failure' remediator should run; 'on_success' must not.</p>"},{"location":"reference/testing/#testsunitvarstest_detectorspy","title":"tests/unit/vars/test_detectors.py","text":""},{"location":"reference/testing/#tests_27","title":"Tests","text":"Test Summary Marks Location test_detector_function_with_merge DI: callable detector. asyncio <code>tests/unit/vars/test_detectors.py:38</code> test_detector_object_block_and_soft DI: object detector. asyncio <code>tests/unit/vars/test_detectors.py:18</code> test_detector_string_import DI: string import \"module:factory\". asyncio <code>tests/unit/vars/test_detectors.py:63</code>"},{"location":"reference/testing/#details_27","title":"Details","text":""},{"location":"reference/testing/#test_detector_function_with_merge","title":"test_detector_function_with_merge","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/unit/vars/test_detectors.py:38</code></p> <p>DI: callable detector. Block big binary payload when block_sensitive=True, otherwise log hit.</p>"},{"location":"reference/testing/#test_detector_object_block_and_soft","title":"test_detector_object_block_and_soft","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/unit/vars/test_detectors.py:18</code></p> <p>DI: object detector. With block_sensitive=True \u2014 raise; without \u2014 write and count hit.</p>"},{"location":"reference/testing/#test_detector_string_import","title":"test_detector_string_import","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/unit/vars/test_detectors.py:63</code></p> <p>DI: string import \"module:factory\". Factory returns an object with is_sensitive().</p>"},{"location":"reference/testing/#testsunitvarstest_incrpy","title":"tests/unit/vars/test_incr.py","text":""},{"location":"reference/testing/#tests_28","title":"Tests","text":"Test Summary Marks Location test_vars_incr_concurrency_sum Concurrency: 10x1 + 5x2 \u2192 exact sum. asyncio <code>tests/unit/vars/test_incr.py:27</code> test_vars_incr_creates_missing_leaf Creating a missing leaf should create it and increment. asyncio <code>tests/unit/vars/test_incr.py:17</code> test_vars_incr_float_and_negative_and_type_conflict Mixed numeric types are allowed; negative increments also work. asyncio <code>tests/unit/vars/test_incr.py:61</code> test_vars_incr_rejects_nan_inf NaN / \u00b1Inf must be rejected. asyncio, parametrize <code>tests/unit/vars/test_incr.py:42</code> test_vars_incr_validates_key Key validation (segments). asyncio, parametrize <code>tests/unit/vars/test_incr.py:52</code>"},{"location":"reference/testing/#details_28","title":"Details","text":""},{"location":"reference/testing/#test_vars_incr_concurrency_sum","title":"test_vars_incr_concurrency_sum","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/unit/vars/test_incr.py:27</code></p> <p>Concurrency: 10x1 + 5x2 \u2192 exact sum.</p>"},{"location":"reference/testing/#test_vars_incr_creates_missing_leaf","title":"test_vars_incr_creates_missing_leaf","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/unit/vars/test_incr.py:17</code></p> <p>Creating a missing leaf should create it and increment.</p>"},{"location":"reference/testing/#test_vars_incr_float_and_negative_and_type_conflict","title":"test_vars_incr_float_and_negative_and_type_conflict","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/unit/vars/test_incr.py:61</code></p> <p>Mixed numeric types are allowed; negative increments also work. If a non-numeric value already exists at the leaf, the adapter should raise.</p>"},{"location":"reference/testing/#test_vars_incr_rejects_nan_inf","title":"test_vars_incr_rejects_nan_inf","text":"<p>marks: <code>asyncio, parametrize</code> \u2022 location: <code>tests/unit/vars/test_incr.py:42</code></p> <p>NaN / \u00b1Inf must be rejected.</p>"},{"location":"reference/testing/#test_vars_incr_validates_key","title":"test_vars_incr_validates_key","text":"<p>marks: <code>asyncio, parametrize</code> \u2022 location: <code>tests/unit/vars/test_incr.py:52</code></p> <p>Key validation (segments).</p>"},{"location":"reference/testing/#testsunitvarstest_mergepy","title":"tests/unit/vars/test_merge.py","text":""},{"location":"reference/testing/#tests_29","title":"Tests","text":"Test Summary Marks Location test_vars_merge_block_sensitive_with_external_detector External detector: block when block_sensitive=True, otherwise write but count sensitive_hits in logs. asyncio <code>tests/unit/vars/test_merge.py:101</code> test_vars_merge_deep_overwrite_mixed_types Deep-merge with leaf overwrite; mixed types (dict \u2192 leaf). asyncio <code>tests/unit/vars/test_merge.py:15</code> test_vars_merge_limits_and_value_size_str_and_bytes Limits: number of paths, depth, full path length, and value size (str/bytes). asyncio <code>tests/unit/vars/test_merge.py:64</code> test_vars_merge_noop_empty Empty input \u2192 no-op (touched=0), 'vars' field is not created. asyncio <code>tests/unit/vars/test_merge.py:52</code>"},{"location":"reference/testing/#details_29","title":"Details","text":""},{"location":"reference/testing/#test_vars_merge_block_sensitive_with_external_detector","title":"test_vars_merge_block_sensitive_with_external_detector","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/unit/vars/test_merge.py:101</code></p> <p>External detector: block when block_sensitive=True, otherwise write but count sensitive_hits in logs.</p>"},{"location":"reference/testing/#test_vars_merge_deep_overwrite_mixed_types","title":"test_vars_merge_deep_overwrite_mixed_types","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/unit/vars/test_merge.py:15</code></p> <p>Deep-merge with leaf overwrite; mixed types (dict \u2192 leaf).</p>"},{"location":"reference/testing/#test_vars_merge_limits_and_value_size_str_and_bytes","title":"test_vars_merge_limits_and_value_size_str_and_bytes","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/unit/vars/test_merge.py:64</code></p> <p>Limits: number of paths, depth, full path length, and value size (str/bytes).</p>"},{"location":"reference/testing/#test_vars_merge_noop_empty","title":"test_vars_merge_noop_empty","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/unit/vars/test_merge.py:52</code></p> <p>Empty input \u2192 no-op (touched=0), 'vars' field is not created.</p>"},{"location":"reference/testing/#testsunitvarstest_setpy","title":"tests/unit/vars/test_set.py","text":""},{"location":"reference/testing/#tests_30","title":"Tests","text":"Test Summary Marks Location test_vars_set_block_sensitive_with_external_detector With block_sensitive=True the detector blocks. asyncio <code>tests/unit/vars/test_set.py:138</code> test_vars_set_deterministic_keys_order_and_sensitive_hits Deterministic sort order in logs + sensitive_hits counter. asyncio <code>tests/unit/vars/test_set.py:118</code> test_vars_set_dotted_and_nested_combo_and_logging Dotted + nested forms in one call; verify values and structured logs. asyncio <code>tests/unit/vars/test_set.py:15</code> test_vars_set_limits_depth_and_path_len Limits: key depth and full path length. asyncio <code>tests/unit/vars/test_set.py:84</code> test_vars_set_limits_paths_count Limit on number of paths in a single $set. asyncio <code>tests/unit/vars/test_set.py:72</code> test_vars_set_limits_value_size_str_and_bytes Limit on value size (string/bytes). asyncio <code>tests/unit/vars/test_set.py:102</code> test_vars_set_validates_key_segments Validate segments: $, '.', NUL, segment length. asyncio, parametrize <code>tests/unit/vars/test_set.py:63</code>"},{"location":"reference/testing/#details_30","title":"Details","text":""},{"location":"reference/testing/#test_vars_set_block_sensitive_with_external_detector","title":"test_vars_set_block_sensitive_with_external_detector","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/unit/vars/test_set.py:138</code></p> <p>With block_sensitive=True the detector blocks. Without it, write succeeds and logs record sensitive_hits=1.</p>"},{"location":"reference/testing/#test_vars_set_deterministic_keys_order_and_sensitive_hits","title":"test_vars_set_deterministic_keys_order_and_sensitive_hits","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/unit/vars/test_set.py:118</code></p> <p>Deterministic sort order in logs + sensitive_hits counter.</p>"},{"location":"reference/testing/#test_vars_set_dotted_and_nested_combo_and_logging","title":"test_vars_set_dotted_and_nested_combo_and_logging","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/unit/vars/test_set.py:15</code></p> <p>Dotted + nested forms in one call; verify values and structured logs.</p>"},{"location":"reference/testing/#test_vars_set_limits_depth_and_path_len","title":"test_vars_set_limits_depth_and_path_len","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/unit/vars/test_set.py:84</code></p> <p>Limits: key depth and full path length.</p>"},{"location":"reference/testing/#test_vars_set_limits_paths_count","title":"test_vars_set_limits_paths_count","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/unit/vars/test_set.py:72</code></p> <p>Limit on number of paths in a single $set.</p>"},{"location":"reference/testing/#test_vars_set_limits_value_size_str_and_bytes","title":"test_vars_set_limits_value_size_str_and_bytes","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/unit/vars/test_set.py:102</code></p> <p>Limit on value size (string/bytes).</p>"},{"location":"reference/testing/#test_vars_set_validates_key_segments","title":"test_vars_set_validates_key_segments","text":"<p>marks: <code>asyncio, parametrize</code> \u2022 location: <code>tests/unit/vars/test_set.py:63</code></p> <p>Validate segments: $, '.', NUL, segment length.</p>"},{"location":"reference/testing/#testsunitvarstest_unsetpy","title":"tests/unit/vars/test_unset.py","text":""},{"location":"reference/testing/#tests_31","title":"Tests","text":"Test Summary Marks Location test_vars_unset_accepts_args_and_kwargs The adapter should accept both *args (varargs keys) and kwargs form 'keys=[...]'. asyncio <code>tests/unit/vars/test_unset.py:76</code> test_vars_unset_path_length_limit Validate full path length limit (including prefix 'coordinator.vars.'). asyncio <code>tests/unit/vars/test_unset.py:64</code> test_vars_unset_simple_nested_and_noop Remove a simple leaf and a nested branch; missing key is a no-op (operation remains valid and accounted in 'touched'). asyncio <code>tests/unit/vars/test_unset.py:14</code> test_vars_unset_validates_paths Validate key/path formats: $, '.', NUL, segment length. asyncio, parametrize <code>tests/unit/vars/test_unset.py:55</code>"},{"location":"reference/testing/#details_31","title":"Details","text":""},{"location":"reference/testing/#test_vars_unset_accepts_args_and_kwargs","title":"test_vars_unset_accepts_args_and_kwargs","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/unit/vars/test_unset.py:76</code></p> <p>The adapter should accept both *args (varargs keys) and kwargs form 'keys=[...]'.</p>"},{"location":"reference/testing/#test_vars_unset_path_length_limit","title":"test_vars_unset_path_length_limit","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/unit/vars/test_unset.py:64</code></p> <p>Validate full path length limit (including prefix 'coordinator.vars.').</p>"},{"location":"reference/testing/#test_vars_unset_simple_nested_and_noop","title":"test_vars_unset_simple_nested_and_noop","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/unit/vars/test_unset.py:14</code></p> <p>Remove a simple leaf and a nested branch; missing key is a no-op (operation remains valid and accounted in 'touched').</p>"},{"location":"reference/testing/#test_vars_unset_validates_paths","title":"test_vars_unset_validates_paths","text":"<p>marks: <code>asyncio, parametrize</code> \u2022 location: <code>tests/unit/vars/test_unset.py:55</code></p> <p>Validate key/path formats: $, '.', NUL, segment length.</p>"},{"location":"reference/testing/#testsunitworkertest_error_classificationpy","title":"tests/unit/worker/test_error_classification.py","text":"<p>Unit test: default RoleHandler.classify_error marks config/programming errors as permanent. We don't pin the exact reason string, only the permanence contract.</p>"},{"location":"reference/testing/#tests_32","title":"Tests","text":"Test Summary Marks Location test_programming_errors_are_permanent (no docstring) \u2014 <code>tests/unit/worker/test_error_classification.py:15</code>"},{"location":"reference/testing/#details_32","title":"Details","text":""},{"location":"reference/testing/#test_programming_errors_are_permanent","title":"test_programming_errors_are_permanent","text":"<p>location: <code>tests/unit/worker/test_error_classification.py:15</code></p> <p>No docstring.</p>"},{"location":"reference/testing/#testsunitworkertest_error_policypy","title":"tests/unit/worker/test_error_policy.py","text":""},{"location":"reference/testing/#tests_33","title":"Tests","text":"Test Summary Marks Location test_classify_error_non_permanent_for_other_exceptions (no docstring) \u2014 <code>tests/unit/worker/test_error_policy.py:18</code> test_classify_error_permanent_for_common_programming_faults (no docstring) \u2014 <code>tests/unit/worker/test_error_policy.py:10</code>"},{"location":"reference/testing/#details_33","title":"Details","text":""},{"location":"reference/testing/#test_classify_error_non_permanent_for_other_exceptions","title":"test_classify_error_non_permanent_for_other_exceptions","text":"<p>location: <code>tests/unit/worker/test_error_policy.py:18</code></p> <p>No docstring.</p>"},{"location":"reference/testing/#test_classify_error_permanent_for_common_programming_faults","title":"test_classify_error_permanent_for_common_programming_faults","text":"<p>location: <code>tests/unit/worker/test_error_policy.py:10</code></p> <p>No docstring.</p>"},{"location":"reference/testing/#testsunitworkertest_pull_adapters_rechunkpy","title":"tests/unit/worker/test_pull_adapters_rechunk.py","text":"<p>Unit tests for PullAdapters.iter_from_artifacts_rechunk() selection rules.</p> <p>Contract:   - If meta_list_key is provided and points to a list \u2192 chunk that list.   - Otherwise \u2192 treat the entire meta as a single item (wrap into a one-element list), no heuristics.</p> <p>Notes:   * We keep these tests unit-level (no Kafka/Coordinator). To let the adapter     terminate its polling loop, we set <code>eof_on_task_done=True</code> and append a     synthetic <code>{status: \"complete\"}</code> marker for the source node. Our stub     <code>count_documents</code> recognizes such queries.</p>"},{"location":"reference/testing/#tests_34","title":"Tests","text":"Test Summary Marks Location test_rechunk_with_meta_list_key_splits_that_list (no docstring) asyncio <code>tests/unit/worker/test_pull_adapters_rechunk.py:70</code> test_rechunk_without_meta_list_key_wraps_meta_as_single_item (no docstring) asyncio <code>tests/unit/worker/test_pull_adapters_rechunk.py:113</code>"},{"location":"reference/testing/#details_34","title":"Details","text":""},{"location":"reference/testing/#test_rechunk_with_meta_list_key_splits_that_list","title":"test_rechunk_with_meta_list_key_splits_that_list","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/unit/worker/test_pull_adapters_rechunk.py:70</code></p> <p>No docstring.</p>"},{"location":"reference/testing/#test_rechunk_without_meta_list_key_wraps_meta_as_single_item","title":"test_rechunk_without_meta_list_key_wraps_meta_as_single_item","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/unit/worker/test_pull_adapters_rechunk.py:113</code></p> <p>No docstring.</p>"},{"location":"reference/worker/","title":"Worker API Reference","text":""},{"location":"reference/worker/#worker-class","title":"Worker Class","text":""},{"location":"reference/worker/#flowkit.worker.runner.Worker","title":"flowkit.worker.runner.Worker","text":"<pre><code>Worker(*, db, cfg: WorkerConfig | None = None, clock: Clock | None = None, roles: list[str] | None = None, handlers: dict[str, RoleHandler] | None = None)\n</code></pre> <p>Stream-aware worker with cooperative cancellation and resilient batching. - Kafka I/O (producers/consumers per role) - Discovery (TASK_DISCOVER \u2192 TASK_SNAPSHOT) - Control-plane CANCEL via signals topic - DB-backed state for resume/takeover</p> Source code in <code>src/flowkit/worker/runner.py</code> <pre><code>def __init__(\n    self,\n    *,\n    db,\n    cfg: WorkerConfig | None = None,\n    clock: Clock | None = None,\n    roles: list[str] | None = None,\n    handlers: dict[str, RoleHandler] | None = None,\n) -&gt; None:\n    self.db = db\n    self.cfg = copy.deepcopy(cfg) if cfg is not None else WorkerConfig.load()\n    if roles:\n        self.cfg.roles = list(roles)\n    self.clock: Clock = clock or SystemClock()\n\n    # identity\n    self.worker_id = self.cfg.worker_id or f\"w-{uuid.uuid4().hex[:8]}\"\n    self.worker_version = self.cfg.worker_version\n\n    # adapters registry\n    self.input_adapters = build_input_adapters(db=db, clock=self.clock, cfg=self.cfg)\n\n    # handlers registry (user injects custom handlers; Echo kept as example)\n    self.handlers: dict[str, RoleHandler] = handlers or {}\n    if \"echo\" in (self.cfg.roles or []):\n        self.handlers.setdefault(\"echo\", EchoHandler())\n\n    # Kafka\n    self._producer: AIOKafkaProducer | None = None\n    self._cmd_consumers: dict[str, AIOKafkaConsumer] = {}\n    self._query_consumer: AIOKafkaConsumer | None = None\n    self._signals_consumer: AIOKafkaConsumer | None = None\n\n    # run-state\n    self._busy = False\n    self._busy_lock = asyncio.Lock()\n    self._cancel_flag = asyncio.Event()\n    self._cancel_meta: dict[str, Any] = {\"reason\": None, \"deadline_ts_ms\": None}\n    self._stopping = False\n\n    self.state = LocalStateManager(db=self.db, clock=self.clock, worker_id=self.worker_id)\n    self.active: ActiveRun | None = None\n\n    # dedup of command envelopes\n    self._dedup: OrderedDict[str, int] = OrderedDict()\n    self._dedup_lock = asyncio.Lock()\n\n    self._main_tasks: set[asyncio.Task] = set()\n\n    # logging\n    self.log = get_logger(\"worker\")\n    bind_context(role=\"worker\", worker_id=self.worker_id, version=self.worker_version)\n    self.log.debug(\"worker.init\", event=\"worker.init\", roles=self.cfg.roles, version=self.worker_version)\n</code></pre>"},{"location":"reference/worker/#flowkit.worker.runner.Worker-functions","title":"Functions","text":""},{"location":"reference/worker/#flowkit.worker.runner.Worker.start","title":"start  <code>async</code>","text":"<pre><code>start() -&gt; None\n</code></pre> Source code in <code>src/flowkit/worker/runner.py</code> <pre><code>async def start(self) -&gt; None:\n    # Helpful config print (silent by default unless tests enable stdout)\n    cfg_dump: Any\n    if hasattr(self.cfg, \"model_dump\"):  # pydantic v2\n        cfg_dump = self.cfg.model_dump()\n    elif hasattr(self.cfg, \"dict\"):  # pydantic v1\n        cfg_dump = self.cfg.dict()\n    else:\n        cfg_dump = getattr(self.cfg, \"__dict__\", str(self.cfg))\n    self.log.debug(\"worker.start\", event=\"worker.start\", cfg=cfg_dump)\n\n    await self._ensure_indexes()\n    self._producer = AIOKafkaProducer(\n        bootstrap_servers=self.cfg.kafka_bootstrap, value_serializer=dumps, enable_idempotence=True\n    )\n    await self._producer.start()\n\n    await self.state.refresh()\n    self.active = self.state.read_active()\n    if self.active and self.active.step_type not in self.cfg.roles:\n        self.active = None\n        await self.state.write_active(None)\n\n    await self._send_announce(\n        EventKind.WORKER_ONLINE,\n        extra={\n            \"worker_id\": self.worker_id,\n            \"type\": \",\".join(self.cfg.roles),\n            \"capabilities\": {\"roles\": self.cfg.roles},\n            \"version\": self.worker_version,\n            \"capacity\": {\"tasks\": 1},\n            \"resume\": self.active.__dict__ if self.active else None,\n        },\n    )\n\n    # command consumers per role\n    for role in self.cfg.roles:\n        topic = self.cfg.topic_cmd(role)\n        c = AIOKafkaConsumer(\n            topic,\n            bootstrap_servers=self.cfg.kafka_bootstrap,\n            value_deserializer=loads,\n            enable_auto_commit=False,\n            auto_offset_reset=\"latest\",\n            group_id=f\"workers.{role}.v1\",\n        )\n        await c.start()\n        self._cmd_consumers[role] = c\n        self._spawn(self._cmd_loop(role, c))\n\n    # query consumer (discovery)\n    self._query_consumer = AIOKafkaConsumer(\n        self.cfg.topic_query,\n        bootstrap_servers=self.cfg.kafka_bootstrap,\n        value_deserializer=loads,\n        enable_auto_commit=False,\n        auto_offset_reset=\"latest\",\n        group_id=\"workers.query.v1\",\n    )\n    await self._query_consumer.start()\n    self._spawn(self._query_loop(self._query_consumer))\n\n    # signals consumer (control plane; unique group per worker)\n    self._signals_consumer = AIOKafkaConsumer(\n        self.cfg.topic_signals,\n        bootstrap_servers=self.cfg.kafka_bootstrap,\n        value_deserializer=loads,\n        enable_auto_commit=False,\n        auto_offset_reset=\"latest\",\n        group_id=f\"workers.signals.{self.worker_id}\",\n    )\n    await self._signals_consumer.start()\n    self._spawn(self._signals_loop(self._signals_consumer))\n\n    # periodic announce\n    self._spawn(self._periodic_announce())\n\n    if self.active:\n        self.log.debug(\n            \"worker.recovery_present\",\n            event=\"worker.recovery_present\",\n            task_id=self.active.task_id,\n            node_id=self.active.node_id,\n        )\n\n    self.log.debug(\"worker.started\", event=\"worker.started\", worker_id=self.worker_id)\n</code></pre>"},{"location":"reference/worker/#flowkit.worker.runner.Worker.stop","title":"stop  <code>async</code>","text":"<pre><code>stop() -&gt; None\n</code></pre> Source code in <code>src/flowkit/worker/runner.py</code> <pre><code>async def stop(self) -&gt; None:\n    self._stopping = True\n    for t in list(self._main_tasks):\n        t.cancel()\n    self._main_tasks.clear()\n\n    if self._query_consumer:\n        with swallow(\n            logger=self.log,\n            code=\"worker.query_consumer.stop\",\n            msg=\"query consumer stop failed\",\n            level=logging.WARNING,\n        ):\n            await self._query_consumer.stop()\n    if self._signals_consumer:\n        with swallow(\n            logger=self.log,\n            code=\"worker.signals_consumer.stop\",\n            msg=\"signals consumer stop failed\",\n            level=logging.WARNING,\n        ):\n            await self._signals_consumer.stop()\n    for c in self._cmd_consumers.values():\n        with swallow(\n            logger=self.log, code=\"worker.cmd_consumer.stop\", msg=\"cmd consumer stop failed\", level=logging.WARNING\n        ):\n            await c.stop()\n    self._cmd_consumers.clear()\n\n    if self._producer:\n        with swallow(\n            logger=self.log, code=\"worker.announce.offline\", msg=\"announce offline failed\", level=logging.WARNING\n        ):\n            await self._send_announce(EventKind.WORKER_OFFLINE, extra={\"worker_id\": self.worker_id})\n        with swallow(\n            logger=self.log, code=\"worker.producer.stop\", msg=\"producer stop failed\", level=logging.WARNING\n        ):\n            await self._producer.stop()\n    self._producer = None\n</code></pre>"},{"location":"reference/worker/#handlers","title":"Handlers","text":""},{"location":"reference/worker/#flowkit.worker.handlers.base.RoleHandler","title":"flowkit.worker.handlers.base.RoleHandler","text":""},{"location":"reference/worker/#flowkit.worker.handlers.base.RoleHandler-functions","title":"Functions","text":""},{"location":"reference/worker/#flowkit.worker.handlers.base.RoleHandler.load_input","title":"load_input  <code>async</code>","text":"<pre><code>load_input(input_ref: dict[str, Any] | None, input_inline: dict[str, Any] | None) -&gt; Any\n</code></pre> <p>Return any structure needed later by <code>iter_batches</code>.</p> <p>Important: if the Coordinator specifies <code>input_inline.input_adapter</code>, the worker selects and runs that adapter. Data returned from <code>load_input</code> must not override the explicitly requested adapter or its arguments.</p> Source code in <code>src/flowkit/worker/handlers/base.py</code> <pre><code>async def load_input(self, input_ref: dict[str, Any] | None, input_inline: dict[str, Any] | None) -&gt; Any:\n    \"\"\"\n    Return any structure needed later by `iter_batches`.\n\n    Important: if the Coordinator specifies `input_inline.input_adapter`,\n    the worker selects and runs that adapter. Data returned from\n    `load_input` must not override the explicitly requested adapter or\n    its arguments.\n    \"\"\"\n    return {\"input_ref\": input_ref or {}, \"input_inline\": input_inline or {}}\n</code></pre>"},{"location":"reference/worker/#flowkit.worker.handlers.base.RoleHandler.iter_batches","title":"iter_batches  <code>async</code>","text":"<pre><code>iter_batches(loaded: Any) -&gt; AsyncIterator[Batch]\n</code></pre> <p>Yield batches from the input when no adapter is selected.</p> <p>If an adapter is specified by the Coordinator, the worker will not call <code>iter_batches</code> and will stream via the selected pull adapter instead.</p> Source code in <code>src/flowkit/worker/handlers/base.py</code> <pre><code>async def iter_batches(self, loaded: Any) -&gt; AsyncIterator[Batch]:\n    \"\"\"\n    Yield batches from the input when no adapter is selected.\n\n    If an adapter is specified by the Coordinator, the worker will not call\n    `iter_batches` and will stream via the selected pull adapter instead.\n    \"\"\"\n    yield Batch(batch_uid=None, payload=loaded or {})\n</code></pre>"},{"location":"reference/worker/#flowkit.worker.handlers.base.Batch","title":"flowkit.worker.handlers.base.Batch","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"reference/worker/#flowkit.worker.handlers.base.BatchResult","title":"flowkit.worker.handlers.base.BatchResult","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"reference/worker/#flowkit.worker.handlers.base.FinalizeResult","title":"flowkit.worker.handlers.base.FinalizeResult","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"reference/worker/#worker-context","title":"Worker Context","text":""},{"location":"reference/worker/#flowkit.worker.context","title":"flowkit.worker.context","text":""},{"location":"reference/worker/#flowkit.worker.context-classes","title":"Classes","text":""},{"location":"reference/worker/#flowkit.worker.context.RunContext","title":"RunContext","text":"<pre><code>RunContext(*, cancel_flag: Event, cancel_meta: dict[str, Any], artifacts_writer, clock: Clock, task_id: str, node_id: str, attempt_epoch: int, worker_id: str)\n</code></pre> <p>Cancellation-aware runtime utilities for handlers: - shared cancel flag + meta (reason, deadline ts) - cancellable awaits - subprocess group termination with escalation - resource cleanup registry</p> Source code in <code>src/flowkit/worker/context.py</code> <pre><code>def __init__(\n    self,\n    *,\n    cancel_flag: asyncio.Event,\n    cancel_meta: dict[str, Any],\n    artifacts_writer,\n    clock: Clock,\n    task_id: str,\n    node_id: str,\n    attempt_epoch: int,\n    worker_id: str,\n):\n    self._cancel_flag = cancel_flag\n    self._cancel_meta = cancel_meta\n    self.artifacts = artifacts_writer\n    self.clock = clock\n\n    self.task_id = task_id\n    self.node_id = node_id\n    self.attempt_epoch = attempt_epoch\n    self.worker_id = worker_id\n\n    self.kv: dict[str, Any] = {}\n    self._cleanup_callbacks: list[Callable[[], Any]] = []\n    self._subprocesses: list[Any] = []\n    self._temp_paths: list[str] = []\n</code></pre>"},{"location":"reference/worker/#worker-state","title":"Worker State","text":""},{"location":"reference/worker/#flowkit.worker.state","title":"flowkit.worker.state","text":""},{"location":"reference/worker/#flowkit.worker.state-classes","title":"Classes","text":""},{"location":"reference/worker/#flowkit.worker.state.LocalStateManager","title":"LocalStateManager","text":"<pre><code>LocalStateManager(*, db, clock: Clock, worker_id: str)\n</code></pre> <p>DB-backed state manager for crash-resume and cross-host takeover.</p> Source code in <code>src/flowkit/worker/state.py</code> <pre><code>def __init__(self, *, db, clock: Clock, worker_id: str) -&gt; None:\n    self.db = db\n    self.clock = clock\n    self.worker_id = worker_id\n    self._lock = asyncio.Lock()\n    self._cache: dict[str, Any] = {}\n</code></pre>"},{"location":"reference/worker/#flowkit.worker.state.LocalStateManager-functions","title":"Functions","text":""},{"location":"reference/worker/#flowkit.worker.state.LocalStateManager.refresh","title":"refresh  <code>async</code>","text":"<pre><code>refresh() -&gt; None\n</code></pre> <p>Pull the latest state from DB into in-memory cache.</p> Source code in <code>src/flowkit/worker/state.py</code> <pre><code>async def refresh(self) -&gt; None:\n    \"\"\"Pull the latest state from DB into in-memory cache.\"\"\"\n    doc = await self.db.worker_state.find_one({\"_id\": self.worker_id})\n    self._cache = doc or {}\n</code></pre>"},{"location":"reference/worker/#flowkit.worker.state.LocalStateManager.read_active","title":"read_active","text":"<pre><code>read_active() -&gt; ActiveRun | None\n</code></pre> <p>Read active run from cache.</p> Source code in <code>src/flowkit/worker/state.py</code> <pre><code>def read_active(self) -&gt; ActiveRun | None:\n    \"\"\"Read active run from cache.\"\"\"\n    d = (self._cache or {}).get(\"active_run\")\n    return ActiveRun(**d) if d else None\n</code></pre>"},{"location":"reference/worker/#flowkit.worker.state.LocalStateManager.write_active","title":"write_active  <code>async</code>","text":"<pre><code>write_active(ar: ActiveRun | None) -&gt; None\n</code></pre> <p>Atomically persist active run to DB and update cache.</p> Source code in <code>src/flowkit/worker/state.py</code> <pre><code>async def write_active(self, ar: ActiveRun | None) -&gt; None:\n    \"\"\"Atomically persist active run to DB and update cache.\"\"\"\n    async with self._lock:\n        payload = {\n            \"active_run\": asdict(ar) if ar else None,\n            \"updated_at\": self.clock.now_dt(),  # must be datetime for TTL index\n        }\n        await self.db.worker_state.update_one(\n            {\"_id\": self.worker_id},\n            {\"$set\": payload},\n            upsert=True,\n        )\n        self._cache.update(payload)\n</code></pre>"},{"location":"reference/worker/#artifacts","title":"Artifacts","text":""},{"location":"reference/worker/#flowkit.worker.artifacts","title":"flowkit.worker.artifacts","text":""},{"location":"reference/worker/#behavior-contract-summary","title":"Behavior contract (summary)","text":"<ul> <li>Adapter precedence: <code>cmd.input_inline.input_adapter</code> (Coordinator) \u2192 handler suggestion \u2192 <code>iter_batches</code> fallback.</li> <li>Validation: unknown adapter \u2192 <code>bad_input_adapter</code> (permanent); bad args \u2192 <code>bad_input_args</code> (permanent).</li> <li>Aliases: <code>from_node</code> is accepted and normalized to <code>from_nodes=[...]</code>.</li> <li>Rechunk: with <code>meta_list_key</code> (list) chunk that list; otherwise each artifact meta is a single logical item.</li> </ul>"}]}