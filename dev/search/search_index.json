{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"FlowKit","text":"<p>FlowKit is a powerful flow orchestration toolkit that enables distributed task execution using a coordinator/worker architecture with Kafka as the message backplane.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Distributed Task Execution: Coordinate complex workflows across multiple workers</li> <li>Kafka-based Messaging: Reliable, scalable communication between components</li> <li>MongoDB Persistence: Durable task state and artifact storage</li> <li>Flexible DAG Support: Define complex task dependencies and fan-in/fan-out patterns</li> <li>Fault Tolerance: Built-in retry mechanisms, worker failure detection, and task recovery</li> <li>Streaming Processing: Support for streaming data through pipeline stages</li> <li>Testing Infrastructure: Comprehensive test helpers for building reliable workflows</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>import asyncio\nfrom flowkit import Coordinator, CoordinatorConfig, WorkerConfig\nfrom flowkit.worker import Worker\nfrom your_handlers import MyHandler\n\n# Start a coordinator\nasync def main():\n    # Configure and start coordinator\n    coord_config = CoordinatorConfig()\n    coordinator = Coordinator(db=your_mongo_db, cfg=coord_config)\n    await coordinator.start()\n\n    # Configure and start worker\n    worker_config = WorkerConfig(roles=[\"processor\"])\n    worker = Worker(\n        db=your_mongo_db,\n        cfg=worker_config,\n        handlers={\"processor\": MyHandler()}\n    )\n    await worker.start()\n\n    # Create a task with a simple graph\n    graph = {\n        \"nodes\": [\n            {\n                \"node_id\": \"process_data\",\n                \"type\": \"processor\",\n                \"io\": {\"input_inline\": {\"data\": \"hello world\"}}\n            }\n        ],\n        \"edges\": []\n    }\n\n    task_id = await coordinator.create_task(params={}, graph=graph)\n    print(f\"Created task: {task_id}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"#architecture-overview","title":"Architecture Overview","text":"<p>FlowKit consists of three main components:</p> <ol> <li>Coordinator: Orchestrates task execution, manages DAG scheduling, and monitors worker health</li> <li>Worker: Executes individual tasks and reports progress back to coordinators</li> <li>Message Bus: Kafka-based communication layer for reliable message delivery</li> </ol> <pre><code>graph TB\n    C[Coordinator] --&gt; K[Kafka]\n    W1[Worker 1] --&gt; K\n    W2[Worker 2] --&gt; K\n    W3[Worker N] --&gt; K\n    C --&gt; M[(MongoDB)]\n    W1 --&gt; M\n    W2 --&gt; M\n    W3 --&gt; M</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":"<ol> <li>Install FlowKit</li> <li>Follow the Quick Start guide</li> <li>Learn the basic concepts</li> <li>Explore examples</li> </ol>"},{"location":"#use-cases","title":"Use Cases","text":"<ul> <li>ETL Pipelines: Extract, transform, and load data across multiple stages</li> <li>ML Workflows: Orchestrate model training, validation, and deployment</li> <li>Data Processing: Handle large-scale data processing with fault tolerance</li> <li>Microservice Orchestration: Coordinate complex business processes</li> <li>Event-Driven Architecture: Build reactive systems with reliable message handling</li> </ul>"},{"location":"development/architecture/","title":"Architecture (WIP)","text":"<p>Coordinator/Worker/Outbox/Protocol components and flows. Placeholder.</p>"},{"location":"development/contributing/","title":"Contributing (WIP)","text":"<p>Docstring style (Google), typing policy, CI rules. Placeholder.</p>"},{"location":"development/github-pages/","title":"GitHub Pages Deployment Guide","text":"<p>This guide explains how to deploy FlowKit documentation to GitHub Pages.</p>"},{"location":"development/github-pages/#automatic-deployment-recommended","title":"Automatic Deployment (Recommended)","text":"<p>The repository includes a GitHub Actions workflow that automatically builds and deploys documentation when you push to the main/master branch.</p>"},{"location":"development/github-pages/#setup-steps","title":"Setup Steps","text":"<ol> <li>Enable GitHub Pages in your repository:</li> <li>Go to repository Settings \u2192 Pages</li> <li>Source: \"GitHub Actions\"</li> <li> <p>Save the settings</p> </li> <li> <p>The workflow will automatically:</p> </li> <li>Trigger on push to main/master branch</li> <li>Build the documentation using MkDocs</li> <li>Deploy to GitHub Pages</li> <li>Make docs available at <code>https://your-username.github.io/flowkit</code></li> </ol>"},{"location":"development/github-pages/#workflow-configuration","title":"Workflow Configuration","text":"<p>The workflow is defined in <code>.github/workflows/docs.yml</code>:</p> <pre><code>name: Deploy Documentation\non:\n  push:\n    branches: [main, master]\npermissions:\n  contents: read\n  pages: write\n  id-token: write\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Setup Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.11'\n      - name: Install dependencies\n        run: |\n          pip install -e .\n          pip install mkdocs mkdocs-material mkdocstrings[python]\n      - name: Build documentation\n        run: mkdocs build --clean --strict\n      - name: Upload artifact\n        uses: actions/upload-pages-artifact@v3\n        with:\n          path: ./site\n  deploy:\n    if: github.ref == 'refs/heads/main'\n    environment:\n      name: github-pages\n      url: ${{ steps.deployment.outputs.page_url }}\n    runs-on: ubuntu-latest\n    needs: build\n    steps:\n      - name: Deploy to GitHub Pages\n        uses: actions/deploy-pages@v4\n</code></pre>"},{"location":"development/github-pages/#manual-deployment","title":"Manual Deployment","text":"<p>You can also deploy manually using the MkDocs CLI:</p>"},{"location":"development/github-pages/#prerequisites","title":"Prerequisites","text":"<pre><code>pip install mkdocs mkdocs-material mkdocstrings[python]\n</code></pre>"},{"location":"development/github-pages/#deploy-command","title":"Deploy Command","text":"<pre><code># Build and deploy to gh-pages branch\nmkdocs gh-deploy\n\n# With custom commit message\nmkdocs gh-deploy -m \"Update documentation\"\n\n# Force push (use with caution)\nmkdocs gh-deploy --force\n</code></pre>"},{"location":"development/github-pages/#manual-setup-for-github-pages","title":"Manual Setup for GitHub Pages","text":"<p>If using manual deployment:</p> <ol> <li>Repository Settings:</li> <li>Go to Settings \u2192 Pages</li> <li>Source: \"Deploy from a branch\"</li> <li> <p>Branch: <code>gh-pages</code> / <code>/ (root)</code></p> </li> <li> <p>Deploy:    <pre><code>mkdocs gh-deploy\n</code></pre></p> </li> </ol>"},{"location":"development/github-pages/#local-development","title":"Local Development","text":""},{"location":"development/github-pages/#serve-documentation-locally","title":"Serve Documentation Locally","text":"<pre><code># Start development server\nmkdocs serve\n\n# Custom host/port\nmkdocs serve --dev-addr=0.0.0.0:8080\n\n# Auto-reload on changes\nmkdocs serve --watch=src --watch=docs\n</code></pre> <p>The docs will be available at <code>http://localhost:8000</code> with live reloading.</p>"},{"location":"development/github-pages/#build-documentation","title":"Build Documentation","text":"<pre><code># Build static site\nmkdocs build\n\n# Build with strict mode (fail on warnings)\nmkdocs build --strict\n\n# Clean build\nmkdocs build --clean\n</code></pre>"},{"location":"development/github-pages/#customization","title":"Customization","text":""},{"location":"development/github-pages/#site-configuration","title":"Site Configuration","text":"<p>Edit <code>mkdocs.yml</code> to customize:</p> <pre><code>site_name: FlowKit\nsite_url: https://your-org.github.io/flowkit\nrepo_url: https://github.com/your-org/flowkit\n\ntheme:\n  name: material\n  palette:\n    primary: blue\n    accent: blue\n</code></pre>"},{"location":"development/github-pages/#custom-domain","title":"Custom Domain","text":"<p>To use a custom domain:</p> <ol> <li> <p>Add CNAME file:    <pre><code>echo \"docs.yourdomain.com\" &gt; docs/CNAME\n</code></pre></p> </li> <li> <p>Configure DNS:</p> </li> <li> <p>Create a CNAME record pointing to <code>your-username.github.io</code></p> </li> <li> <p>Update mkdocs.yml:    <pre><code>site_url: https://docs.yourdomain.com\n</code></pre></p> </li> </ol>"},{"location":"development/github-pages/#troubleshooting","title":"Troubleshooting","text":""},{"location":"development/github-pages/#common-issues","title":"Common Issues","text":"<p>Build Failures: <pre><code># Check for syntax errors\nmkdocs build --strict\n\n# Validate configuration\nmkdocs config\n</code></pre></p> <p>Missing Dependencies: <pre><code># Install all documentation dependencies\npip install mkdocs mkdocs-material mkdocstrings[python]\n</code></pre></p> <p>Import Errors: <pre><code># Install the package in development mode\npip install -e .\n</code></pre></p> <p>GitHub Actions Permissions: - Ensure Pages permissions are enabled in repository settings - Check that the workflow has <code>pages: write</code> permission</p>"},{"location":"development/github-pages/#debug-locally","title":"Debug Locally","text":"<pre><code># Verbose build output\nmkdocs build --verbose\n\n# Check configuration\nmkdocs config\n\n# Validate all internal links\nmkdocs build --strict\n</code></pre>"},{"location":"development/github-pages/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Always test locally before pushing:    <pre><code>mkdocs serve\n# Check all pages and links work\n</code></pre></p> </li> <li> <p>Use strict mode to catch issues:    <pre><code>mkdocs build --strict\n</code></pre></p> </li> <li> <p>Keep documentation in sync with code changes</p> </li> <li> <p>Use descriptive commit messages for documentation changes</p> </li> <li> <p>Review documentation in pull requests</p> </li> </ol>"},{"location":"development/github-pages/#url-structure","title":"URL Structure","text":"<p>After deployment, your documentation will be available at:</p> <ul> <li>Main site: <code>https://your-username.github.io/flowkit/</code></li> <li>Getting Started: <code>https://your-username.github.io/flowkit/getting-started/quickstart/</code></li> <li>API Reference: <code>https://your-username.github.io/flowkit/reference/core/</code></li> <li>Examples: <code>https://your-username.github.io/flowkit/examples/simple-pipeline/</code></li> </ul>"},{"location":"development/github-pages/#security-considerations","title":"Security Considerations","text":"<ul> <li>Documentation is public by default</li> <li>Don't include sensitive information in docs</li> <li>Use environment variables for secrets in examples</li> <li>Review all code examples for security issues</li> </ul>"},{"location":"development/github-pages/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>Use <code>mkdocs build --clean</code> for clean builds</li> <li>Optimize images in <code>docs/images/</code></li> <li>Minimize external dependencies</li> <li>Use MkDocs Material's optimization features</li> </ul>"},{"location":"development/testing/","title":"Development: Testing (WIP)","text":"<p>How to run tests locally, coverage, flakes. Placeholder.</p>"},{"location":"examples/complex-workflows/","title":"Complex Workflows (WIP)","text":"<p>Multi-branch pipelines, fan-in variants, streaming patterns. Placeholder.</p>"},{"location":"examples/simple-pipeline/","title":"Simple Pipeline Example","text":"<p>This example demonstrates a basic three-stage ETL pipeline: Extract \u2192 Transform \u2192 Load.</p>"},{"location":"examples/simple-pipeline/#complete-example","title":"Complete Example","text":"<pre><code>import asyncio\nfrom typing import AsyncIterator\nfrom motor.motor_asyncio import AsyncIOMotorClient\n\nfrom flowkit import Coordinator, CoordinatorConfig, WorkerConfig\nfrom flowkit.worker import Worker\nfrom flowkit.worker.handlers.base import Handler, Batch, BatchResult, RunContext\n\n# Mock data source\nSAMPLE_DATA = [\n    {\"id\": 1, \"name\": \"Alice\", \"age\": 30, \"city\": \"New York\"},\n    {\"id\": 2, \"name\": \"Bob\", \"age\": 25, \"city\": \"San Francisco\"},\n    {\"id\": 3, \"name\": \"Charlie\", \"age\": 35, \"city\": \"Chicago\"},\n    {\"id\": 4, \"name\": \"Diana\", \"age\": 28, \"city\": \"Seattle\"},\n    {\"id\": 5, \"name\": \"Eve\", \"age\": 32, \"city\": \"Boston\"},\n]\n\nclass ExtractHandler(Handler):\n    \"\"\"Extract data from source.\"\"\"\n\n    async def iter_batches(self, input_data) -&gt; AsyncIterator[Batch]:\n        batch_size = input_data.get(\"input_inline\", {}).get(\"batch_size\", 2)\n\n        for i in range(0, len(SAMPLE_DATA), batch_size):\n            batch_data = SAMPLE_DATA[i:i + batch_size]\n            yield Batch(\n                batch_uid=f\"extract_batch_{i // batch_size}\",\n                payload={\"records\": batch_data}\n            )\n\n    async def process_batch(self, batch: Batch, ctx: RunContext) -&gt; BatchResult:\n        records = batch.payload[\"records\"]\n        print(f\"\ud83d\udce5 Extracted {len(records)} records\")\n\n        # Simulate extraction work\n        await asyncio.sleep(0.1)\n\n        return BatchResult(\n            success=True,\n            metrics={\"records_extracted\": len(records)},\n            artifacts_ref={\"data\": records, \"stage\": \"extract\"}\n        )\n\nclass TransformHandler(Handler):\n    \"\"\"Transform extracted data.\"\"\"\n\n    async def iter_batches(self, input_data) -&gt; AsyncIterator[Batch]:\n        # Use input adapter to pull from extract stage\n        input_args = input_data.get(\"input_inline\", {}).get(\"input_args\", {})\n        from_nodes = input_args.get(\"from_nodes\", [])\n\n        # In real implementation, this would pull from artifacts DB\n        # For demo, we'll simulate pulling transformed data\n        if \"extract\" in from_nodes:\n            # Simulate pulling from artifacts\n            for i in range(3):  # Mock 3 batches from extract\n                yield Batch(\n                    batch_uid=f\"transform_batch_{i}\",\n                    payload={\"batch_id\": i, \"from_extract\": True}\n                )\n\n    async def process_batch(self, batch: Batch, ctx: RunContext) -&gt; BatchResult:\n        print(f\"\ud83d\udd04 Transforming batch {batch.batch_uid}\")\n\n        # Simulate transformation: add computed fields\n        transformed_data = {\n            \"batch_id\": batch.payload.get(\"batch_id\"),\n            \"transformation\": \"added_full_name_and_category\",\n            \"processed_at\": ctx.clock.now_dt().isoformat()\n        }\n\n        await asyncio.sleep(0.15)  # Simulate work\n\n        return BatchResult(\n            success=True,\n            metrics={\"records_transformed\": 2},  # Simulated\n            artifacts_ref={\"transformed_data\": transformed_data, \"stage\": \"transform\"}\n        )\n\nclass LoadHandler(Handler):\n    \"\"\"Load transformed data to destination.\"\"\"\n\n    async def iter_batches(self, input_data) -&gt; AsyncIterator[Batch]:\n        # Pull from transform stage\n        input_args = input_data.get(\"input_inline\", {}).get(\"input_args\", {})\n        from_nodes = input_args.get(\"from_nodes\", [])\n\n        if \"transform\" in from_nodes:\n            # Simulate final loading batch\n            yield Batch(\n                batch_uid=\"load_final\",\n                payload={\"load_all\": True, \"from_transform\": True}\n            )\n\n    async def process_batch(self, batch: Batch, ctx: RunContext) -&gt; BatchResult:\n        print(f\"\ud83d\udcbe Loading data to destination\")\n\n        # Simulate loading to database/warehouse\n        await asyncio.sleep(0.2)\n\n        return BatchResult(\n            success=True,\n            metrics={\"records_loaded\": 5, \"load_time_ms\": 200},\n            artifacts_ref={\"load_status\": \"completed\", \"stage\": \"load\"}\n        )\n\nasync def run_etl_pipeline():\n    \"\"\"Run the complete ETL pipeline.\"\"\"\n\n    # Database setup\n    client = AsyncIOMotorClient(\"mongodb://localhost:27017\")\n    db = client.flowkit_examples\n\n    # Configuration\n    coord_config = CoordinatorConfig(\n        kafka_bootstrap=\"localhost:9092\",\n        worker_types=[\"extractor\", \"transformer\", \"loader\"],\n        scheduler_tick_sec=0.1  # Fast scheduling for demo\n    )\n\n    worker_config = WorkerConfig(\n        kafka_bootstrap=\"localhost:9092\",\n        roles=[\"extractor\", \"transformer\", \"loader\"]\n    )\n\n    # Start coordinator\n    coordinator = Coordinator(db=db, cfg=coord_config)\n    await coordinator.start()\n    print(\"\ud83d\ude80 Coordinator started\")\n\n    # Start worker with all handlers\n    worker = Worker(\n        db=db,\n        cfg=worker_config,\n        handlers={\n            \"extractor\": ExtractHandler(),\n            \"transformer\": TransformHandler(),\n            \"loader\": LoadHandler()\n        }\n    )\n    await worker.start()\n    print(\"\ud83d\ude80 Worker started\")\n\n    try:\n        # Define ETL graph\n        graph = {\n            \"nodes\": [\n                {\n                    \"node_id\": \"extract\",\n                    \"type\": \"extractor\",\n                    \"depends_on\": [],\n                    \"io\": {\n                        \"input_inline\": {\"batch_size\": 2}\n                    }\n                },\n                {\n                    \"node_id\": \"transform\",\n                    \"type\": \"transformer\",\n                    \"depends_on\": [\"extract\"],\n                    \"io\": {\n                        \"start_when\": \"first_batch\",  # Start as soon as extract produces data\n                        \"input_inline\": {\n                            \"input_adapter\": \"pull.from_artifacts\",\n                            \"input_args\": {\"from_nodes\": [\"extract\"]}\n                        }\n                    }\n                },\n                {\n                    \"node_id\": \"load\",\n                    \"type\": \"loader\",\n                    \"depends_on\": [\"transform\"],\n                    \"fan_in\": \"all\",  # Wait for transform to complete\n                    \"io\": {\n                        \"input_inline\": {\n                            \"input_adapter\": \"pull.from_artifacts\",\n                            \"input_args\": {\"from_nodes\": [\"transform\"]}\n                        }\n                    }\n                }\n            ],\n            \"edges\": [\n                [\"extract\", \"transform\"],\n                [\"transform\", \"load\"]\n            ]\n        }\n\n        # Create and run task\n        task_id = await coordinator.create_task(\n            params={\"pipeline_type\": \"etl_demo\"},\n            graph=graph\n        )\n        print(f\"\ud83d\udccb Created ETL task: {task_id}\")\n\n        # Monitor progress\n        print(\"\\\n\u23f3 Waiting for pipeline completion...\")\n        for i in range(30):  # Wait up to 30 seconds\n            task_doc = await db.tasks.find_one({\"id\": task_id})\n            status = task_doc.get(\"status\")\n\n            if status == \"finished\":\n                print(f\"\u2705 Pipeline completed successfully!\")\n                break\n            elif status == \"failed\":\n                print(f\"\u274c Pipeline failed\")\n                break\n\n            await asyncio.sleep(1)\n\n        # Show final results\n        task_doc = await db.tasks.find_one({\"id\": task_id})\n        nodes = {n[\"node_id\"]: n[\"status\"] for n in task_doc[\"graph\"][\"nodes\"]}\n        print(f\"\\\n\ud83d\udcca Final node statuses: {nodes}\")\n\n        # Show artifacts\n        artifacts = await db.artifacts.find({\"task_id\": task_id}).to_list(10)\n        print(f\"\ud83d\udce6 Total artifacts created: {len(artifacts)}\")\n\n        # Show metrics\n        metrics = await db.metrics_raw.find({\"task_id\": task_id}).to_list(20)\n        total_extracted = sum(m.get(\"metrics\", {}).get(\"records_extracted\", 0) for m in metrics)\n        total_transformed = sum(m.get(\"metrics\", {}).get(\"records_transformed\", 0) for m in metrics)\n        total_loaded = sum(m.get(\"metrics\", {}).get(\"records_loaded\", 0) for m in metrics)\n\n        print(f\"\\\n\ud83d\udcc8 Pipeline Metrics:\")\n        print(f\"  Records extracted: {total_extracted}\")\n        print(f\"  Records transformed: {total_transformed}\")\n        print(f\"  Records loaded: {total_loaded}\")\n\n    finally:\n        # Cleanup\n        await worker.stop()\n        await coordinator.stop()\n        client.close()\n        print(\"\\\n\u2705 Pipeline shutdown complete\")\n\nif __name__ == \"__main__\":\n    asyncio.run(run_etl_pipeline())\n</code></pre>"},{"location":"examples/simple-pipeline/#key-concepts-demonstrated","title":"Key Concepts Demonstrated","text":""},{"location":"examples/simple-pipeline/#1-handler-implementation","title":"1. Handler Implementation","text":"<p>Each stage implements the <code>Handler</code> interface with: - <code>iter_batches()</code>: Generates batches for processing - <code>process_batch()</code>: Processes individual batches - Returns <code>BatchResult</code> with success status and metrics</p>"},{"location":"examples/simple-pipeline/#2-input-adapters","title":"2. Input Adapters","text":"<p>The transform and load stages use <code>pull.from_artifacts</code> to get data from upstream stages:</p> <pre><code>\"input_inline\": {\n    \"input_adapter\": \"pull.from_artifacts\",\n    \"input_args\": {\"from_nodes\": [\"extract\"]}\n}\n</code></pre>"},{"location":"examples/simple-pipeline/#3-streaming-processing","title":"3. Streaming Processing","text":"<p>The transform stage starts as soon as extract produces its first batch:</p> <pre><code>\"io\": {\n    \"start_when\": \"first_batch\"\n}\n</code></pre>"},{"location":"examples/simple-pipeline/#4-dag-dependencies","title":"4. DAG Dependencies","text":"<p>The graph clearly defines the flow: extract \u2192 transform \u2192 load</p>"},{"location":"examples/simple-pipeline/#5-metrics-collection","title":"5. Metrics Collection","text":"<p>Each handler reports metrics that are aggregated for monitoring</p>"},{"location":"examples/simple-pipeline/#running-the-example","title":"Running the Example","text":"<ol> <li> <p>Start Kafka and MongoDB: <pre><code># Terminal 1: Kafka\ndocker run -d --name kafka -p 9092:9092 confluentinc/cp-kafka:latest\n\n# Terminal 2: MongoDB\ndocker run -d --name mongodb -p 27017:27017 mongo:6.0\n</code></pre></p> </li> <li> <p>Run the pipeline: <pre><code>python simple_pipeline.py\n</code></pre></p> </li> </ol> <p>Expected output: <pre><code>\ud83d\ude80 Coordinator started\n\ud83d\ude80 Worker started\n\ud83d\udccb Created ETL task: abc123...\n\u23f3 Waiting for pipeline completion...\n\ud83d\udce5 Extracted 2 records\n\ud83d\udce5 Extracted 2 records\n\ud83d\udce5 Extracted 1 records\n\ud83d\udd04 Transforming batch transform_batch_0\n\ud83d\udd04 Transforming batch transform_batch_1\n\ud83d\udd04 Transforming batch transform_batch_2\n\ud83d\udcbe Loading data to destination\n\u2705 Pipeline completed successfully!\n\ud83d\udcca Final node statuses: {'extract': 'finished', 'transform': 'finished', 'load': 'finished'}\n\ud83d\udce6 Total artifacts created: 7\n\ud83d\udcc8 Pipeline Metrics:\n  Records extracted: 5\n  Records transformed: 6\n  Records loaded: 5\n\u2705 Pipeline shutdown complete\n</code></pre></p>"},{"location":"examples/simple-pipeline/#next-steps","title":"Next Steps","text":"<ul> <li>Complex Workflows - Multi-path DAGs with fan-in/fan-out</li> <li>Testing Scenarios - How to test your pipelines</li> <li>Error Handling - Robust error handling patterns</li> </ul>"},{"location":"examples/testing/","title":"Testing Scenarios (WIP)","text":"<p>Chaos tests, resilience cases, concurrency &amp; lease checks. Placeholder.</p>"},{"location":"getting-started/concepts/","title":"Basic Concepts","text":"<p>Understanding FlowKit's core concepts will help you build robust, scalable workflows.</p>"},{"location":"getting-started/concepts/#core-components","title":"Core Components","text":""},{"location":"getting-started/concepts/#coordinator","title":"Coordinator","text":"<p>The Coordinator is the brain of FlowKit. It:</p> <ul> <li>Schedules tasks based on DAG dependencies</li> <li>Monitors worker health and task progress</li> <li>Handles task retries and failure recovery</li> <li>Manages task state transitions</li> <li>Coordinates multiple workers across different types</li> </ul> <pre><code>from flowkit import Coordinator, CoordinatorConfig\n\ncoordinator = Coordinator(\n    db=mongodb_instance,\n    cfg=CoordinatorConfig(worker_types=[\"indexer\", \"processor\"])\n)\n</code></pre>"},{"location":"getting-started/concepts/#worker","title":"Worker","text":"<p>A Worker executes individual tasks. Workers:</p> <ul> <li>Pull tasks from Kafka topics</li> <li>Execute custom handler logic</li> <li>Report progress and results back to coordinators</li> <li>Handle task cancellation and cleanup</li> <li>Support multiple roles/task types</li> </ul> <pre><code>from flowkit.worker import Worker, WorkerConfig\n\nworker = Worker(\n    db=mongodb_instance,\n    cfg=WorkerConfig(roles=[\"processor\"]),\n    handlers={\"processor\": MyProcessorHandler()}\n)\n</code></pre>"},{"location":"getting-started/concepts/#task-graph-dag","title":"Task Graph (DAG)","text":"<p>Tasks are organized into Directed Acyclic Graphs (DAGs) that define:</p> <ul> <li>Nodes: Individual processing steps</li> <li>Edges: Dependencies between nodes</li> <li>Fan-in/Fan-out: How data flows between stages</li> </ul> <pre><code>graph = {\n    \"nodes\": [\n        {\"node_id\": \"extract\", \"type\": \"extractor\", \"depends_on\": []},\n        {\"node_id\": \"transform\", \"type\": \"transformer\", \"depends_on\": [\"extract\"]},\n        {\"node_id\": \"load\", \"type\": \"loader\", \"depends_on\": [\"transform\"]}\n    ],\n    \"edges\": [[\"extract\", \"transform\"], [\"transform\", \"load\"]]\n}\n</code></pre>"},{"location":"getting-started/concepts/#key-concepts","title":"Key Concepts","text":""},{"location":"getting-started/concepts/#handlers","title":"Handlers","text":"<p>Handlers define the actual processing logic for each task type:</p> <pre><code>from flowkit.worker.handlers.base import Handler, Batch, BatchResult\n\nclass MyHandler(Handler):\n    async def iter_batches(self, input_data) -&gt; AsyncIterator[Batch]:\n        # Generate batches from input\n        for item in input_data:\n            yield Batch(batch_uid=item.id, payload=item.data)\n\n    async def process_batch(self, batch: Batch, ctx: RunContext) -&gt; BatchResult:\n        # Process single batch\n        result = process_data(batch.payload)\n        return BatchResult(success=True, artifacts_ref=result)\n\n    async def finalize(self, ctx: RunContext) -&gt; FinalizationResult:\n        # Clean up after all batches processed\n        return FinalizationResult(metrics={\"total_processed\": ctx.processed_count})\n</code></pre>"},{"location":"getting-started/concepts/#artifacts","title":"Artifacts","text":"<p>Artifacts are the data outputs from each processing stage:</p> <ul> <li>Stored in MongoDB with metadata</li> <li>Can be partial (streaming) or complete</li> <li>Referenced by downstream stages</li> <li>Include processing metrics and status</li> </ul>"},{"location":"getting-started/concepts/#input-adapters","title":"Input Adapters","text":"<p>Input Adapters define how data flows between stages:</p> <pre><code># Pull from upstream artifacts\n\"input_inline\": {\n    \"input_adapter\": \"pull.from_artifacts\",\n    \"input_args\": {\"from_nodes\": [\"upstream_node\"]}\n}\n\n# Rechunk data with different batch sizes\n\"input_inline\": {\n    \"input_adapter\": \"pull.from_artifacts.rechunk:size\",\n    \"input_args\": {\"from_nodes\": [\"upstream\"], \"size\": 10}\n}\n</code></pre>"},{"location":"getting-started/concepts/#task-states","title":"Task States","text":"<p>Tasks progress through several states:</p> <ul> <li>queued: Waiting to be scheduled</li> <li>running: Currently being processed</li> <li>deferred: Temporarily paused (e.g., for retry)</li> <li>finished: Completed successfully</li> <li>failed: Terminated with error</li> <li>cancelling: Being cancelled</li> </ul>"},{"location":"getting-started/concepts/#flow-control-patterns","title":"Flow Control Patterns","text":""},{"location":"getting-started/concepts/#fan-in-strategies","title":"Fan-in Strategies","text":"<p>Control when a node starts based on dependencies:</p> <pre><code>{\n    \"node_id\": \"combiner\",\n    \"depends_on\": [\"node1\", \"node2\", \"node3\"],\n    \"fan_in\": \"all\"        # Wait for all dependencies (default)\n    # \"fan_in\": \"any\"      # Start when any dependency completes\n    # \"fan_in\": \"count:2\"  # Start when 2 dependencies complete\n}\n</code></pre>"},{"location":"getting-started/concepts/#streaming-vs-batch-processing","title":"Streaming vs Batch Processing","text":"<pre><code># Start processing as soon as first batch is available\n{\n    \"node_id\": \"processor\",\n    \"io\": {\"start_when\": \"first_batch\"}\n}\n\n# Wait for upstream to completely finish\n{\n    \"node_id\": \"processor\",\n    \"io\": {\"start_when\": \"ready\"}  # default\n}\n</code></pre>"},{"location":"getting-started/concepts/#coordinator-functions","title":"Coordinator Functions","text":"<p>Execute logic directly in the coordinator (no worker needed):</p> <pre><code>{\n    \"node_id\": \"merger\",\n    \"type\": \"coordinator_fn\",\n    \"io\": {\n        \"fn\": \"merge.generic\",\n        \"fn_args\": {\n            \"from_nodes\": [\"node1\", \"node2\"],\n            \"target\": {\"key\": \"merged_result\"}\n        }\n    }\n}\n</code></pre>"},{"location":"getting-started/concepts/#error-handling","title":"Error Handling","text":""},{"location":"getting-started/concepts/#retry-policies","title":"Retry Policies","text":"<pre><code>{\n    \"node_id\": \"flaky_task\",\n    \"retry_policy\": {\n        \"max\": 3,\n        \"backoff_sec\": 300,\n        \"permanent_on\": [\"bad_input\", \"schema_mismatch\"]\n    }\n}\n</code></pre>"},{"location":"getting-started/concepts/#error-classification","title":"Error Classification","text":"<p>Handlers can classify errors as permanent or transient:</p> <pre><code>def classify_error(self, error: Exception) -&gt; tuple[str, bool]:\n    if isinstance(error, ValidationError):\n        return \"validation_failed\", True  # Permanent - don't retry\n    else:\n        return \"temporary_failure\", False  # Transient - retry\n</code></pre>"},{"location":"getting-started/concepts/#message-flow","title":"Message Flow","text":"<ol> <li>Coordinator publishes task commands to Kafka topics</li> <li>Workers consume from role-specific topics (e.g., <code>cmd.processor.v1</code>)</li> <li>Workers publish status updates to status topics</li> <li>Coordinator consumes status updates and updates task state</li> <li>Artifacts are stored in MongoDB for inter-stage communication</li> </ol>"},{"location":"getting-started/concepts/#scalability-patterns","title":"Scalability Patterns","text":""},{"location":"getting-started/concepts/#horizontal-scaling","title":"Horizontal Scaling","text":"<ul> <li>Run multiple coordinators (they coordinate through MongoDB)</li> <li>Run multiple workers of the same type for parallel processing</li> <li>Workers auto-discover tasks and distribute load</li> </ul>"},{"location":"getting-started/concepts/#partitioning","title":"Partitioning","text":"<ul> <li>Use Kafka partitioning for parallel processing</li> <li>Workers process partitions independently</li> <li>Results are merged by downstream stages</li> </ul>"},{"location":"getting-started/concepts/#resource-management","title":"Resource Management","text":"<ul> <li>Configure worker capacity limits</li> <li>Set concurrency limits per worker type</li> <li>Use backpressure mechanisms for flow control</li> </ul>"},{"location":"getting-started/concepts/#next-steps","title":"Next Steps","text":"<ul> <li>Set up Coordinators</li> <li>Create Custom Workers</li> <li>Design Task Graphs</li> <li>Handle Errors Gracefully</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.11+</li> <li>Apache Kafka</li> <li>MongoDB</li> <li>Docker (optional, for development)</li> </ul>"},{"location":"getting-started/installation/#install-flowkit","title":"Install FlowKit","text":""},{"location":"getting-started/installation/#from-pypi","title":"From PyPI","text":"<pre><code>pip install flowkit\n</code></pre>"},{"location":"getting-started/installation/#from-source","title":"From Source","text":"<pre><code>git clone https://github.com/your-org/flowkit.git\ncd flowkit\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>For development with all dependencies:</p> <pre><code>git clone https://github.com/your-org/flowkit.git\ncd flowkit\npip install -e \".[dev,test]\"\n</code></pre>"},{"location":"getting-started/installation/#infrastructure-setup","title":"Infrastructure Setup","text":""},{"location":"getting-started/installation/#apache-kafka","title":"Apache Kafka","text":"<p>FlowKit requires Kafka for message coordination. You can run Kafka locally using Docker:</p> <pre><code># Start Zookeeper and Kafka\ndocker run -d --name zookeeper -p 2181:2181 zookeeper:3.8\ndocker run -d --name kafka -p 9092:9092 \\\n    --link zookeeper:zookeeper \\\n    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \\\n    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092 \\\n    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \\\n    confluentinc/cp-kafka:latest\n</code></pre> <p>Or use the provided Docker Compose file:</p> <pre><code>cd flowkit\ndocker-compose up -d kafka\n</code></pre>"},{"location":"getting-started/installation/#mongodb","title":"MongoDB","text":"<p>FlowKit uses MongoDB for task state and artifact persistence:</p> <pre><code># Run MongoDB with Docker\ndocker run -d --name mongodb -p 27017:27017 mongo:6.0\n</code></pre> <p>Or with Docker Compose:</p> <pre><code>cd flowkit\ndocker-compose up -d mongodb\n</code></pre>"},{"location":"getting-started/installation/#configuration","title":"Configuration","text":"<p>Create configuration files for your coordinator and workers:</p>"},{"location":"getting-started/installation/#coordinator-configuration-coordinatorjson","title":"Coordinator Configuration (<code>coordinator.json</code>)","text":"<pre><code>{\n    \"kafka_bootstrap\": \"localhost:9092\",\n    \"worker_types\": [\"indexer\", \"processor\", \"analyzer\"],\n    \"heartbeat_soft_sec\": 300,\n    \"heartbeat_hard_sec\": 3600,\n    \"lease_ttl_sec\": 60\n}\n</code></pre>"},{"location":"getting-started/installation/#worker-configuration-workerjson","title":"Worker Configuration (<code>worker.json</code>)","text":"<pre><code>{\n    \"kafka_bootstrap\": \"localhost:9092\",\n    \"roles\": [\"processor\"],\n    \"worker_id\": null,\n    \"lease_ttl_sec\": 60,\n    \"hb_interval_sec\": 20\n}\n</code></pre>"},{"location":"getting-started/installation/#verification","title":"Verification","text":"<p>Test your installation:</p> <pre><code>import flowkit\nprint(f\"FlowKit version: {flowkit.__version__}\")\n\n# Test coordinator and worker imports\nfrom flowkit import Coordinator, CoordinatorConfig, WorkerConfig\nfrom flowkit.worker import Worker\nprint(\"FlowKit installed successfully!\")\n</code></pre>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>This guide will walk you through creating your first FlowKit pipeline in just a few minutes.</p>"},{"location":"getting-started/quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>FlowKit installed (Installation Guide)</li> <li>Kafka running on <code>localhost:9092</code></li> <li>MongoDB running on <code>localhost:27017</code></li> </ul>"},{"location":"getting-started/quickstart/#step-1-create-a-simple-handler","title":"Step 1: Create a Simple Handler","text":"<p>First, create a custom handler that will process your data:</p> <pre><code># handlers.py\nimport asyncio\nfrom typing import AsyncIterator\nfrom flowkit.worker.handlers.base import Handler, Batch, BatchResult, RunContext\n\nclass EchoHandler(Handler):\n    \"\"\"A simple handler that echoes input data.\"\"\"\n\n    async def iter_batches(self, input_data) -&gt; AsyncIterator[Batch]:\n        \"\"\"Generate batches from input data.\"\"\"\n        data = input_data.get(\"input_inline\", {}).get(\"message\", \"Hello World\")\n        yield Batch(\n            batch_uid=\"batch_1\",\n            payload={\"message\": data, \"processed_at\": \"now\"}\n        )\n\n    async def process_batch(self, batch: Batch, ctx: RunContext) -&gt; BatchResult:\n        \"\"\"Process a single batch.\"\"\"\n        message = batch.payload.get(\"message\", \"\")\n        processed_message = f\"Echo: {message.upper()}\"\n\n        print(f\"Processing: {processed_message}\")\n\n        # Simulate some work\n        await asyncio.sleep(0.1)\n\n        return BatchResult(\n            success=True,\n            metrics={\"messages_processed\": 1},\n            artifacts_ref={\"output\": processed_message}\n        )\n</code></pre>"},{"location":"getting-started/quickstart/#step-2-set-up-database-connection","title":"Step 2: Set Up Database Connection","text":"<pre><code># database.py\nfrom motor.motor_asyncio import AsyncIOMotorClient\n\nasync def get_database():\n    \"\"\"Get MongoDB database connection.\"\"\"\n    client = AsyncIOMotorClient(\"mongodb://localhost:27017\")\n    return client.flowkit_db\n</code></pre>"},{"location":"getting-started/quickstart/#step-3-create-your-first-pipeline","title":"Step 3: Create Your First Pipeline","text":"<pre><code># pipeline.py\nimport asyncio\nfrom flowkit import Coordinator, CoordinatorConfig, WorkerConfig\nfrom flowkit.worker import Worker\nfrom handlers import EchoHandler\nfrom database import get_database\n\nasync def run_pipeline():\n    # Get database connection\n    db = await get_database()\n\n    # Configure coordinator\n    coord_config = CoordinatorConfig(\n        kafka_bootstrap=\"localhost:9092\",\n        worker_types=[\"echo\"]\n    )\n\n    # Configure worker\n    worker_config = WorkerConfig(\n        kafka_bootstrap=\"localhost:9092\",\n        roles=[\"echo\"]\n    )\n\n    # Start coordinator\n    coordinator = Coordinator(db=db, cfg=coord_config)\n    await coordinator.start()\n    print(\"\u2705 Coordinator started\")\n\n    # Start worker\n    worker = Worker(\n        db=db,\n        cfg=worker_config,\n        handlers={\"echo\": EchoHandler()}\n    )\n    await worker.start()\n    print(\"\u2705 Worker started\")\n\n    try:\n        # Create a simple task graph\n        graph = {\n            \"nodes\": [\n                {\n                    \"node_id\": \"echo_task\",\n                    \"type\": \"echo\",\n                    \"depends_on\": [],\n                    \"io\": {\n                        \"input_inline\": {\n                            \"message\": \"Hello FlowKit!\"\n                        }\n                    }\n                }\n            ],\n            \"edges\": []\n        }\n\n        # Create and execute task\n        task_id = await coordinator.create_task(\n            params={\"pipeline_name\": \"quickstart\"},\n            graph=graph\n        )\n        print(f\"\u2705 Created task: {task_id}\")\n\n        # Wait for completion (in real apps, you'd monitor differently)\n        await asyncio.sleep(5)\n\n        # Check task status\n        task_doc = await db.tasks.find_one({\"id\": task_id})\n        print(f\"\u2705 Task status: {task_doc['status']}\")\n\n        # Check artifacts\n        artifacts = await db.artifacts.find({\"task_id\": task_id}).to_list(10)\n        print(f\"\u2705 Artifacts created: {len(artifacts)}\")\n\n    finally:\n        # Clean shutdown\n        await worker.stop()\n        await coordinator.stop()\n        print(\"\u2705 Pipeline shutdown complete\")\n\nif __name__ == \"__main__\":\n    asyncio.run(run_pipeline())\n</code></pre>"},{"location":"getting-started/quickstart/#step-4-run-your-pipeline","title":"Step 4: Run Your Pipeline","text":"<pre><code>python pipeline.py\n</code></pre> <p>Expected output: <pre><code>\u2705 Coordinator started\n\u2705 Worker started\n\u2705 Created task: 12345678-1234-1234-1234-123456789abc\nProcessing: Echo: HELLO FLOWKIT!\n\u2705 Task status: finished\n\u2705 Artifacts created: 1\n\u2705 Pipeline shutdown complete\n</code></pre></p>"},{"location":"getting-started/quickstart/#step-5-create-a-multi-stage-pipeline","title":"Step 5: Create a Multi-Stage Pipeline","text":"<p>Now let's create a more complex pipeline with multiple stages:</p> <pre><code># complex_pipeline.py\nimport asyncio\nfrom flowkit import Coordinator, CoordinatorConfig, WorkerConfig\nfrom flowkit.worker import Worker\nfrom handlers import EchoHandler\nfrom database import get_database\n\nclass ProcessorHandler(Handler):\n    \"\"\"Processes data from previous stage.\"\"\"\n\n    async def iter_batches(self, input_data) -&gt; AsyncIterator[Batch]:\n        # Use input adapter to pull from artifacts\n        input_adapter = input_data.get(\"input_inline\", {}).get(\"input_adapter\")\n        if input_adapter == \"pull.from_artifacts\":\n            # This would pull from upstream artifacts\n            # For simplicity, we'll simulate\n            yield Batch(\n                batch_uid=\"process_batch_1\",\n                payload={\"data\": \"processed_data\", \"stage\": \"processor\"}\n            )\n\n    async def process_batch(self, batch: Batch, ctx: RunContext) -&gt; BatchResult:\n        data = batch.payload.get(\"data\", \"\")\n        result = f\"Processed: {data}\"\n\n        print(f\"Stage 2 - {result}\")\n        await asyncio.sleep(0.1)\n\n        return BatchResult(\n            success=True,\n            metrics={\"items_processed\": 1},\n            artifacts_ref={\"result\": result}\n        )\n\nasync def run_complex_pipeline():\n    db = await get_database()\n\n    coord_config = CoordinatorConfig(\n        kafka_bootstrap=\"localhost:9092\",\n        worker_types=[\"echo\", \"processor\"]\n    )\n\n    worker_config = WorkerConfig(\n        kafka_bootstrap=\"localhost:9092\",\n        roles=[\"echo\", \"processor\"]\n    )\n\n    coordinator = Coordinator(db=db, cfg=coord_config)\n    await coordinator.start()\n\n    worker = Worker(\n        db=db,\n        cfg=worker_config,\n        handlers={\n            \"echo\": EchoHandler(),\n            \"processor\": ProcessorHandler()\n        }\n    )\n    await worker.start()\n\n    try:\n        # Multi-stage graph\n        graph = {\n            \"nodes\": [\n                {\n                    \"node_id\": \"stage1\",\n                    \"type\": \"echo\",\n                    \"depends_on\": [],\n                    \"io\": {\"input_inline\": {\"message\": \"Input data\"}}\n                },\n                {\n                    \"node_id\": \"stage2\",\n                    \"type\": \"processor\",\n                    \"depends_on\": [\"stage1\"],\n                    \"io\": {\n                        \"input_inline\": {\n                            \"input_adapter\": \"pull.from_artifacts\",\n                            \"input_args\": {\"from_nodes\": [\"stage1\"]}\n                        }\n                    }\n                }\n            ],\n            \"edges\": [[\"stage1\", \"stage2\"]]  # stage1 -&gt; stage2\n        }\n\n        task_id = await coordinator.create_task(params={}, graph=graph)\n        print(f\"\u2705 Multi-stage task created: {task_id}\")\n\n        await asyncio.sleep(10)  # Wait for completion\n\n        task_doc = await db.tasks.find_one({\"id\": task_id})\n        print(f\"\u2705 Final status: {task_doc['status']}\")\n\n    finally:\n        await worker.stop()\n        await coordinator.stop()\n\nif __name__ == \"__main__\":\n    asyncio.run(run_complex_pipeline())\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Basic Concepts</li> <li>Explore Task Graphs in detail</li> <li>Check out more Examples</li> <li>Read about Error Handling</li> </ul>"},{"location":"guide/configuration/","title":"Configuration (WIP)","text":"<p>Coordinator/Worker settings, environment overrides, and defaults. Placeholder.</p>"},{"location":"guide/coordinators/","title":"Coordinators","text":"<p>Coordinators are the orchestration layer of FlowKit, responsible for managing task execution, scheduling DAG nodes, and monitoring worker health.</p>"},{"location":"guide/coordinators/#overview","title":"Overview","text":"<p>A Coordinator:</p> <ul> <li>Schedules Tasks: Determines when nodes in a DAG are ready to execute</li> <li>Manages Workers: Tracks worker health and capacity</li> <li>Handles Failures: Implements retry policies and failure recovery</li> <li>Coordinates State: Maintains task state in MongoDB</li> <li>Routes Messages: Uses Kafka for reliable communication with workers</li> </ul>"},{"location":"guide/coordinators/#basic-setup","title":"Basic Setup","text":"<pre><code>from flowkit import Coordinator, CoordinatorConfig\nfrom motor.motor_asyncio import AsyncIOMotorClient\n\nasync def setup_coordinator():\n    # Database connection\n    client = AsyncIOMotorClient(\"mongodb://localhost:27017\")\n    db = client.flowkit\n\n    # Configuration\n    config = CoordinatorConfig(\n        kafka_bootstrap=\"localhost:9092\",\n        worker_types=[\"processor\", \"analyzer\", \"loader\"]\n    )\n\n    # Create and start coordinator\n    coordinator = Coordinator(db=db, cfg=config)\n    await coordinator.start()\n\n    return coordinator\n</code></pre>"},{"location":"guide/coordinators/#configuration-options","title":"Configuration Options","text":""},{"location":"guide/coordinators/#core-settings","title":"Core Settings","text":"<pre><code>config = CoordinatorConfig(\n    # Kafka configuration\n    kafka_bootstrap=\"localhost:9092\",\n    worker_types=[\"indexer\", \"processor\", \"analyzer\"],\n\n    # Topic naming patterns\n    topic_cmd_fmt=\"cmd.{type}.v1\",\n    topic_status_fmt=\"status.{type}.v1\",\n\n    # Timing settings (seconds)\n    heartbeat_soft_sec=300,      # Soft heartbeat timeout\n    heartbeat_hard_sec=3600,     # Hard heartbeat timeout\n    lease_ttl_sec=45,            # Worker lease duration\n    scheduler_tick_sec=1.0,      # Scheduling frequency\n\n    # Retry and backoff\n    cancel_grace_sec=30,         # Grace period for cancellation\n    outbox_max_retry=12,         # Max retry attempts\n    outbox_backoff_min_ms=250,   # Min backoff time\n    outbox_backoff_max_ms=60000, # Max backoff time\n)\n</code></pre>"},{"location":"guide/coordinators/#loading-from-file","title":"Loading from File","text":"<pre><code># Load from JSON file\nconfig = CoordinatorConfig.load(\"configs/coordinator.json\")\n\n# With overrides\nconfig = CoordinatorConfig.load(\n    \"configs/coordinator.json\",\n    overrides={\"scheduler_tick_sec\": 0.5}\n)\n\n# Environment variables\n# KAFKA_BOOTSTRAP_SERVERS, WORKER_TYPES are automatically loaded\n</code></pre>"},{"location":"guide/coordinators/#task-management","title":"Task Management","text":""},{"location":"guide/coordinators/#creating-tasks","title":"Creating Tasks","text":"<pre><code>async def create_pipeline_task(coordinator):\n    graph = {\n        \"nodes\": [\n            {\n                \"node_id\": \"extract\",\n                \"type\": \"extractor\",\n                \"depends_on\": [],\n                \"io\": {\"input_inline\": {\"source\": \"database\"}}\n            },\n            {\n                \"node_id\": \"transform\",\n                \"type\": \"processor\",\n                \"depends_on\": [\"extract\"],\n                \"io\": {\n                    \"input_inline\": {\n                        \"input_adapter\": \"pull.from_artifacts\",\n                        \"input_args\": {\"from_nodes\": [\"extract\"]}\n                    }\n                }\n            }\n        ],\n        \"edges\": [[\"extract\", \"transform\"]]\n    }\n\n    task_id = await coordinator.create_task(\n        params={\"pipeline_name\": \"etl_demo\"},\n        graph=graph\n    )\n\n    return task_id\n</code></pre>"},{"location":"guide/coordinators/#task-lifecycle","title":"Task Lifecycle","text":"<p>Tasks progress through these states:</p> <ol> <li>queued \u2192 Task created, waiting to be scheduled</li> <li>running \u2192 At least one node is executing</li> <li>finished \u2192 All nodes completed successfully</li> <li>failed \u2192 Task failed permanently</li> <li>deferred \u2192 Temporarily paused for retry</li> </ol>"},{"location":"guide/coordinators/#monitoring-tasks","title":"Monitoring Tasks","text":"<pre><code>async def monitor_task(coordinator, task_id):\n    # Get task document from database\n    task_doc = await coordinator.db.tasks.find_one({\"id\": task_id})\n\n    # Check overall status\n    print(f\"Task status: {task_doc['status']}\")\n\n    # Check individual node statuses\n    for node in task_doc[\"graph\"][\"nodes\"]:\n        print(f\"Node {node['node_id']}: {node['status']}\")\n\n    # Check artifacts\n    artifacts = await coordinator.db.artifacts.find(\n        {\"task_id\": task_id}\n    ).to_list(100)\n    print(f\"Artifacts created: {len(artifacts)}\")\n</code></pre>"},{"location":"guide/coordinators/#scheduling-behavior","title":"Scheduling Behavior","text":""},{"location":"guide/coordinators/#node-readiness","title":"Node Readiness","text":"<p>The coordinator schedules nodes when:</p> <ol> <li>Dependencies satisfied: All <code>depends_on</code> nodes are finished</li> <li>Fan-in condition met: Based on <code>fan_in</code> strategy</li> <li>Not already running: Node isn't currently being processed</li> <li>Retry conditions met: If deferred, retry time has passed</li> </ol>"},{"location":"guide/coordinators/#fan-in-strategies","title":"Fan-in Strategies","text":"<pre><code># Wait for all dependencies (default)\n{\n    \"node_id\": \"combiner\",\n    \"depends_on\": [\"node1\", \"node2\", \"node3\"],\n    \"fan_in\": \"all\"\n}\n\n# Start when any dependency completes\n{\n    \"node_id\": \"processor\",\n    \"depends_on\": [\"node1\", \"node2\"],\n    \"fan_in\": \"any\"\n}\n\n# Start when N dependencies complete\n{\n    \"node_id\": \"analyzer\",\n    \"depends_on\": [\"node1\", \"node2\", \"node3\"],\n    \"fan_in\": \"count:2\"\n}\n</code></pre>"},{"location":"guide/coordinators/#streaming-execution","title":"Streaming Execution","text":"<pre><code># Start as soon as upstream produces first batch\n{\n    \"node_id\": \"processor\",\n    \"depends_on\": [\"extractor\"],\n    \"io\": {\"start_when\": \"first_batch\"}\n}\n\n# Wait for upstream to completely finish (default)\n{\n    \"node_id\": \"loader\",\n    \"depends_on\": [\"processor\"],\n    \"io\": {\"start_when\": \"ready\"}\n}\n</code></pre>"},{"location":"guide/coordinators/#coordinator-functions","title":"Coordinator Functions","text":"<p>Coordinators can execute logic directly without workers:</p> <pre><code>{\n    \"node_id\": \"merger\",\n    \"type\": \"coordinator_fn\",\n    \"depends_on\": [\"extract1\", \"extract2\"],\n    \"io\": {\n        \"fn\": \"merge.generic\",\n        \"fn_args\": {\n            \"from_nodes\": [\"extract1\", \"extract2\"],\n            \"target\": {\"key\": \"merged_data\"}\n        }\n    }\n}\n</code></pre> <p>Built-in coordinator functions:</p> <ul> <li><code>merge.generic</code>: Merge artifacts from multiple nodes</li> <li><code>copy.artifacts</code>: Copy artifacts between nodes</li> <li><code>transform.metadata</code>: Transform artifact metadata</li> </ul>"},{"location":"guide/coordinators/#error-handling","title":"Error Handling","text":""},{"location":"guide/coordinators/#retry-policies","title":"Retry Policies","text":"<pre><code>{\n    \"node_id\": \"flaky_processor\",\n    \"retry_policy\": {\n        \"max\": 3,                    # Maximum retry attempts\n        \"backoff_sec\": 300,          # Backoff between retries\n        \"permanent_on\": [            # Errors that shouldn't retry\n            \"bad_input\",\n            \"schema_mismatch\"\n        ]\n    }\n}\n</code></pre>"},{"location":"guide/coordinators/#cascade-cancellation","title":"Cascade Cancellation","text":"<p>When a node fails permanently, the coordinator can cancel downstream nodes:</p> <pre><code># This happens automatically for permanent failures\n# You can also trigger manual cancellation:\n\nawait coordinator.cascade_cancel(\n    task_id=\"abc123\",\n    reason=\"upstream_failure\"\n)\n</code></pre>"},{"location":"guide/coordinators/#scaling-coordinators","title":"Scaling Coordinators","text":""},{"location":"guide/coordinators/#multiple-coordinators","title":"Multiple Coordinators","text":"<p>You can run multiple coordinators for high availability:</p> <pre><code># Coordinator 1\ncoordinator1 = Coordinator(db=db, cfg=config, worker_types=[\"processor\"])\n\n# Coordinator 2\ncoordinator2 = Coordinator(db=db, cfg=config, worker_types=[\"analyzer\"])\n\n# They coordinate through MongoDB and don't conflict\nawait coordinator1.start()\nawait coordinator2.start()\n</code></pre>"},{"location":"guide/coordinators/#load-balancing","title":"Load Balancing","text":"<p>Coordinators automatically distribute work:</p> <ul> <li>Task scheduling is coordinated through MongoDB</li> <li>Workers are discovered dynamically</li> <li>No single point of failure</li> </ul>"},{"location":"guide/coordinators/#monitoring-and-observability","title":"Monitoring and Observability","text":""},{"location":"guide/coordinators/#health-checks","title":"Health Checks","text":"<pre><code>async def check_coordinator_health(coordinator):\n    # Check if coordinator is running\n    if not coordinator._running:\n        return \"stopped\"\n\n    # Check active workers\n    workers = await coordinator.db.worker_registry.find(\n        {\"status\": \"online\"}\n    ).to_list(100)\n\n    # Check pending tasks\n    pending = await coordinator.db.tasks.count_documents(\n        {\"status\": {\"$in\": [\"queued\", \"running\"]}}\n    )\n\n    return {\n        \"status\": \"healthy\",\n        \"active_workers\": len(workers),\n        \"pending_tasks\": pending\n    }\n</code></pre>"},{"location":"guide/coordinators/#metrics-collection","title":"Metrics Collection","text":"<pre><code>async def collect_coordinator_metrics(coordinator):\n    # Task metrics\n    tasks_by_status = await coordinator.db.tasks.aggregate([\n        {\"$group\": {\"_id\": \"$status\", \"count\": {\"$sum\": 1}}}\n    ]).to_list(10)\n\n    # Worker metrics\n    workers_by_status = await coordinator.db.worker_registry.aggregate([\n        {\"$group\": {\"_id\": \"$status\", \"count\": {\"$sum\": 1}}}\n    ]).to_list(10)\n\n    # Recent events\n    recent_events = await coordinator.db.worker_events.count_documents({\n        \"created_at\": {\"$gte\": datetime.utcnow() - timedelta(hours=1)}\n    })\n\n    return {\n        \"tasks\": dict(tasks_by_status),\n        \"workers\": dict(workers_by_status),\n        \"recent_events\": recent_events\n    }\n</code></pre>"},{"location":"guide/coordinators/#best-practices","title":"Best Practices","text":""},{"location":"guide/coordinators/#configuration-management","title":"Configuration Management","text":"<ol> <li> <p>Use environment-specific configs:    <pre><code>config = CoordinatorConfig.load(f\"configs/coordinator.{env}.json\")\n</code></pre></p> </li> <li> <p>Tune timing parameters based on your workload:</p> </li> <li>Short <code>scheduler_tick_sec</code> for low-latency</li> <li> <p>Longer <code>heartbeat_soft_sec</code> for stable workloads</p> </li> <li> <p>Set appropriate worker types to match your pipeline needs</p> </li> </ol>"},{"location":"guide/coordinators/#task-design","title":"Task Design","text":"<ol> <li>Keep DAGs focused: Don't create overly complex graphs</li> <li>Use streaming for large datasets</li> <li>Implement proper retry policies for reliability</li> <li>Add monitoring for long-running tasks</li> </ol>"},{"location":"guide/coordinators/#error-handling_1","title":"Error Handling","text":"<ol> <li>Classify errors properly in handlers</li> <li>Set reasonable retry limits to avoid infinite loops</li> <li>Monitor failure patterns to identify systemic issues</li> <li>Implement circuit breakers for external dependencies</li> </ol>"},{"location":"guide/coordinators/#performance","title":"Performance","text":"<ol> <li>Scale coordinators horizontally for high throughput</li> <li>Tune Kafka settings for your message volume</li> <li>Monitor MongoDB performance under load</li> <li>Use indexing for large task collections</li> </ol>"},{"location":"guide/coordinators/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guide/coordinators/#common-issues","title":"Common Issues","text":"<p>Coordinator not starting: - Check Kafka connectivity - Verify MongoDB connection - Ensure topics are created</p> <p>Tasks not being scheduled: - Check worker availability - Verify DAG dependencies are correct - Look for failed discovery queries</p> <p>High memory usage: - Tune outbox retention settings - Clean up old task documents - Monitor worker event collection size</p> <p>Slow scheduling: - Reduce <code>scheduler_tick_sec</code> - Add MongoDB indexes - Optimize DAG complexity</p>"},{"location":"guide/coordinators/#next-steps","title":"Next Steps","text":"<ul> <li>Configure Workers to process your tasks</li> <li>Design Task Graphs for complex workflows</li> <li>Handle Errors gracefully</li> <li>Monitor Performance in production</li> </ul>"},{"location":"guide/error-handling/","title":"Error handling &amp; retries (WIP)","text":"<p>Transient vs permanent errors, retry policy, backoff, fencing. Placeholder.</p>"},{"location":"guide/graphs/","title":"Task Graphs (WIP)","text":"<p>Overview of DAG nodes, edges, fan-in modes, and scheduling rules. Placeholder.</p>"},{"location":"guide/workers/","title":"Workers","text":"<p>Workers are the execution engine of FlowKit, responsible for processing individual tasks and reporting results back to coordinators.</p>"},{"location":"guide/workers/#overview","title":"Overview","text":"<p>A Worker:</p> <ul> <li>Executes Tasks: Runs custom handler logic for specific task types</li> <li>Manages State: Maintains local state for reliability and recovery</li> <li>Communicates Progress: Reports status updates via Kafka</li> <li>Handles Cancellation: Responds to cancellation signals gracefully</li> <li>Supports Multiple Roles: Can handle different task types simultaneously</li> </ul>"},{"location":"guide/workers/#basic-setup","title":"Basic Setup","text":"<pre><code>from flowkit.worker import Worker, WorkerConfig\nfrom flowkit.worker.handlers.base import Handler\nfrom motor.motor_asyncio import AsyncIOMotorClient\n\nclass MyHandler(Handler):\n    async def iter_batches(self, input_data):\n        # Generate batches from input\n        yield Batch(batch_uid=\"batch_1\", payload={\"data\": \"example\"})\n\n    async def process_batch(self, batch, ctx):\n        # Process the batch\n        return BatchResult(success=True, metrics={\"processed\": 1})\n\nasync def setup_worker():\n    # Database connection\n    client = AsyncIOMotorClient(\"mongodb://localhost:27017\")\n    db = client.flowkit\n\n    # Configuration\n    config = WorkerConfig(\n        kafka_bootstrap=\"localhost:9092\",\n        roles=[\"processor\", \"analyzer\"]\n    )\n\n    # Create worker with handlers\n    worker = Worker(\n        db=db,\n        cfg=config,\n        handlers={\n            \"processor\": MyHandler(),\n            \"analyzer\": MyHandler()\n        }\n    )\n\n    await worker.start()\n    return worker\n</code></pre>"},{"location":"guide/workers/#configuration-options","title":"Configuration Options","text":""},{"location":"guide/workers/#core-settings","title":"Core Settings","text":"<pre><code>config = WorkerConfig(\n    # Kafka configuration\n    kafka_bootstrap=\"localhost:9092\",\n\n    # Worker identity\n    roles=[\"processor\", \"analyzer\"],    # Task types this worker handles\n    worker_id=None,                     # Auto-generated if None\n    worker_version=\"2.0.0\",            # Worker version for tracking\n\n    # Timing settings\n    lease_ttl_sec=60,                  # How long to hold task leases\n    hb_interval_sec=20,                # Heartbeat frequency\n    announce_interval_sec=60,          # Worker announcement frequency\n\n    # Local state management\n    state_dir=\"./.worker_state\",       # Directory for persistent state\n\n    # Performance tuning\n    dedup_cache_size=10000,           # Deduplication cache size\n    dedup_ttl_ms=3600_000,            # Dedup entry TTL\n    pull_poll_ms_default=300,         # Default polling interval\n    db_cancel_poll_ms=500,            # Cancellation check frequency\n)\n</code></pre>"},{"location":"guide/workers/#loading-from-file","title":"Loading from File","text":"<pre><code># Load from JSON file\nconfig = WorkerConfig.load(\"configs/worker.json\")\n\n# With overrides\nconfig = WorkerConfig.load(\n    \"configs/worker.json\",\n    overrides={\"roles\": [\"processor\"]}\n)\n</code></pre>"},{"location":"guide/workers/#handler-implementation","title":"Handler Implementation","text":""},{"location":"guide/workers/#handler-interface","title":"Handler Interface","text":"<p>All handlers must implement the <code>Handler</code> base class:</p> <pre><code>from flowkit.worker.handlers.base import Handler, Batch, BatchResult, RunContext\nfrom typing import AsyncIterator\n\nclass CustomHandler(Handler):\n\n    async def init(self, run_info: dict):\n        \"\"\"Initialize handler for a new task run.\"\"\"\n        self.task_id = run_info[\"task_id\"]\n        self.node_id = run_info[\"node_id\"]\n        # Setup resources, connections, etc.\n\n    async def iter_batches(self, input_data) -&gt; AsyncIterator[Batch]:\n        \"\"\"Generate batches from input data.\"\"\"\n        # Extract batch information from input_data\n        batch_size = input_data.get(\"input_inline\", {}).get(\"batch_size\", 10)\n\n        for i in range(0, 100, batch_size):\n            yield Batch(\n                batch_uid=f\"batch_{i}\",\n                payload={\"start\": i, \"end\": min(i + batch_size, 100)}\n            )\n\n    async def process_batch(self, batch: Batch, ctx: RunContext) -&gt; BatchResult:\n        \"\"\"Process a single batch.\"\"\"\n        start = batch.payload[\"start\"]\n        end = batch.payload[\"end\"]\n\n        # Check for cancellation\n        if ctx.cancel_flag.is_set():\n            raise asyncio.CancelledError()\n\n        # Do actual processing\n        result = await self.process_range(start, end)\n\n        # Save artifacts if needed\n        artifacts_ref = await ctx.artifacts_writer.upsert_partial(\n            batch.batch_uid,\n            {\"items_processed\": end - start}\n        )\n\n        return BatchResult(\n            success=True,\n            metrics={\"items_processed\": end - start},\n            artifacts_ref=artifacts_ref\n        )\n\n    async def finalize(self, ctx: RunContext) -&gt; FinalizationResult:\n        \"\"\"Clean up after all batches are processed.\"\"\"\n        # Final processing, cleanup, etc.\n        return FinalizationResult(\n            metrics={\"total_items\": ctx.total_processed}\n        )\n\n    def classify_error(self, error: Exception) -&gt; tuple[str, bool]:\n        \"\"\"Classify errors as permanent or transient.\"\"\"\n        if isinstance(error, ValueError):\n            return \"validation_error\", True  # Permanent\n        elif isinstance(error, ConnectionError):\n            return \"connection_error\", False  # Transient\n        else:\n            return \"unknown_error\", False\n</code></pre>"},{"location":"guide/workers/#batch-processing-patterns","title":"Batch Processing Patterns","text":""},{"location":"guide/workers/#simple-iterator","title":"Simple Iterator","text":"<pre><code>async def iter_batches(self, input_data) -&gt; AsyncIterator[Batch]:\n    items = input_data.get(\"input_inline\", {}).get(\"items\", [])\n\n    for i, item in enumerate(items):\n        yield Batch(\n            batch_uid=f\"item_{i}\",\n            payload={\"item\": item, \"index\": i}\n        )\n</code></pre>"},{"location":"guide/workers/#database-cursor","title":"Database Cursor","text":"<pre><code>async def iter_batches(self, input_data) -&gt; AsyncIterator[Batch]:\n    batch_size = input_data.get(\"input_inline\", {}).get(\"batch_size\", 100)\n\n    async for batch in self.db.collection.find().batch_size(batch_size):\n        yield Batch(\n            batch_uid=f\"db_batch_{batch[0]['_id']}\",\n            payload={\"records\": batch}\n        )\n</code></pre>"},{"location":"guide/workers/#file-processing","title":"File Processing","text":"<pre><code>async def iter_batches(self, input_data) -&gt; AsyncIterator[Batch]:\n    file_path = input_data.get(\"input_inline\", {}).get(\"file_path\")\n\n    with open(file_path, 'r') as f:\n        batch = []\n        for i, line in enumerate(f):\n            batch.append(line.strip())\n\n            if len(batch) &gt;= 1000:  # Process in chunks of 1000\n                yield Batch(\n                    batch_uid=f\"file_batch_{i//1000}\",\n                    payload={\"lines\": batch}\n                )\n                batch = []\n\n        # Final batch\n        if batch:\n            yield Batch(\n                batch_uid=f\"file_batch_final\",\n                payload={\"lines\": batch}\n            )\n</code></pre>"},{"location":"guide/workers/#input-adapters","title":"Input Adapters","text":"<p>Workers can use input adapters to pull data from previous stages:</p>"},{"location":"guide/workers/#pull-from-artifacts","title":"Pull from Artifacts","text":"<pre><code># In task definition\n\"io\": {\n    \"input_inline\": {\n        \"input_adapter\": \"pull.from_artifacts\",\n        \"input_args\": {\n            \"from_nodes\": [\"upstream_node\"],\n            \"poll_ms\": 500,\n            \"eof_on_task_done\": True\n        }\n    }\n}\n</code></pre>"},{"location":"guide/workers/#rechunk-data","title":"Rechunk Data","text":"<pre><code># Rechunk upstream data into different batch sizes\n\"io\": {\n    \"input_inline\": {\n        \"input_adapter\": \"pull.from_artifacts.rechunk:size\",\n        \"input_args\": {\n            \"from_nodes\": [\"upstream_node\"],\n            \"size\": 50,  # New batch size\n            \"meta_list_key\": \"items\"\n        }\n    }\n}\n</code></pre>"},{"location":"guide/workers/#state-management","title":"State Management","text":""},{"location":"guide/workers/#worker-state-architecture","title":"Worker State Architecture","text":"<p>\u95ee\u9898\u5206\u6790: \u5f53\u524d\u7684 worker_state \u6587\u4ef6\u7cfb\u7edf\u6709\u4ee5\u4e0b\u95ee\u9898\uff1a</p> <ol> <li>\u6587\u4ef6\u7cfb\u7edf\u4f9d\u8d56: \u9700\u8981\u521b\u5efa\u76ee\u5f55\uff0c\u53ef\u80fd\u5728\u5bb9\u5668\u73af\u5883\u4e2d\u6709\u6743\u9650\u95ee\u9898</li> <li>\u5355\u70b9\u6545\u969c: \u672c\u5730\u6587\u4ef6\u4e22\u5931\u4f1a\u5bfc\u81f4\u72b6\u6001\u4e22\u5931</li> <li>\u6269\u5c55\u6027\u9650\u5236: \u4e0d\u9002\u5408\u591a\u5b9e\u4f8b\u90e8\u7f72</li> </ol> <p>\u5efa\u8bae\u7684\u66f4\u597d\u67b6\u6784\u89e3\u51b3\u65b9\u6848:</p> <pre><code># \u65b0\u7684\u57fa\u4e8e\u6570\u636e\u5e93\u7684\u72b6\u6001\u7ba1\u7406\nclass DatabaseWorkerState:\n    \"\"\"Database-backed worker state for production reliability.\"\"\"\n\n    def __init__(self, db, worker_id: str):\n        self.db = db\n        self.worker_id = worker_id\n        self.collection = db.worker_states\n\n    async def read_active(self) -&gt; ActiveRun | None:\n        \"\"\"Read active task state from database.\"\"\"\n        doc = await self.collection.find_one({\"worker_id\": self.worker_id})\n        if doc and doc.get(\"active_run\"):\n            return ActiveRun(**doc[\"active_run\"])\n        return None\n\n    async def write_active(self, active_run: ActiveRun | None):\n        \"\"\"Write active task state to database.\"\"\"\n        await self.collection.update_one(\n            {\"worker_id\": self.worker_id},\n            {\n                \"$set\": {\n                    \"worker_id\": self.worker_id,\n                    \"active_run\": active_run.__dict__ if active_run else None,\n                    \"updated_at\": datetime.utcnow()\n                }\n            },\n            upsert=True\n        )\n\n    async def write_checkpoint(self, checkpoint: dict):\n        \"\"\"Write processing checkpoint.\"\"\"\n        await self.collection.update_one(\n            {\"worker_id\": self.worker_id},\n            {\"$set\": {\"active_run.checkpoint\": checkpoint}},\n            upsert=True\n        )\n</code></pre> <p>\u4f18\u52bf: - \u2705 \u65e0\u6587\u4ef6\u7cfb\u7edf\u4f9d\u8d56: \u5b8c\u5168\u57fa\u4e8e\u6570\u636e\u5e93 - \u2705 \u9ad8\u53ef\u7528: \u6570\u636e\u5e93\u96c6\u7fa4\u63d0\u4f9b\u5197\u4f59 - \u2705 \u6269\u5c55\u6027: \u652f\u6301\u591a\u5b9e\u4f8b\u90e8\u7f72 - \u2705 \u76d1\u63a7\u53cb\u597d: \u53ef\u67e5\u8be2\u6240\u6709 worker \u72b6\u6001 - \u2705 \u4e8b\u52a1\u6027: \u539f\u5b50\u6027\u72b6\u6001\u66f4\u65b0</p>"},{"location":"guide/workers/#state-recovery","title":"State Recovery","text":"<p>Workers automatically recover from failures:</p> <pre><code># On worker startup, check for existing state\nactive_run = await worker.state.read_active()\nif active_run:\n    print(f\"Recovering task: {active_run.task_id}\")\n    # Worker will resume the task automatically\n</code></pre>"},{"location":"guide/workers/#checkpointing","title":"Checkpointing","text":"<pre><code>async def process_batch(self, batch: Batch, ctx: RunContext) -&gt; BatchResult:\n    # Save progress checkpoint\n    checkpoint = {\n        \"processed_items\": batch.payload[\"count\"],\n        \"last_id\": batch.payload[\"last_id\"]\n    }\n    await ctx.save_checkpoint(checkpoint)\n\n    # Continue processing...\n</code></pre>"},{"location":"guide/workers/#cancellation-handling","title":"Cancellation Handling","text":""},{"location":"guide/workers/#graceful-cancellation","title":"Graceful Cancellation","text":"<pre><code>async def process_batch(self, batch: Batch, ctx: RunContext) -&gt; BatchResult:\n    # Check cancellation at key points\n    if ctx.cancel_flag.is_set():\n        reason = ctx.cancel_meta.get(\"reason\", \"unknown\")\n        print(f\"Task cancelled: {reason}\")\n        raise asyncio.CancelledError()\n\n    # Long-running operation with periodic checks\n    for item in batch.payload[\"items\"]:\n        if ctx.cancel_flag.is_set():\n            raise asyncio.CancelledError()\n\n        await process_item(item)\n\n    return BatchResult(success=True)\n</code></pre>"},{"location":"guide/workers/#cancellation-sources","title":"Cancellation Sources","text":"<p>Cancellation can come from:</p> <ol> <li>Coordinator signals: Explicit cancellation commands</li> <li>Database flags: Task marked as cancelled in DB</li> <li>Timeout: Lease expiration</li> <li>Worker shutdown: Graceful shutdown process</li> </ol>"},{"location":"guide/workers/#monitoring-and-health","title":"Monitoring and Health","text":""},{"location":"guide/workers/#worker-announcements","title":"Worker Announcements","text":"<p>Workers periodically announce their presence:</p> <pre><code># Automatic announcements include:\n{\n    \"worker_id\": \"worker-abc123\",\n    \"type\": \"processor,analyzer\",  # Comma-separated roles\n    \"capabilities\": {\"roles\": [\"processor\", \"analyzer\"]},\n    \"version\": \"2.0.0\",\n    \"capacity\": {\"tasks\": 1},\n    \"status\": \"online\"\n}\n</code></pre>"},{"location":"guide/workers/#health-checks","title":"Health Checks","text":"<pre><code>async def check_worker_health(worker):\n    return {\n        \"worker_id\": worker.worker_id,\n        \"roles\": worker.cfg.roles,\n        \"busy\": worker._busy,\n        \"active_task\": worker.active.task_id if worker.active else None,\n        \"last_heartbeat\": worker.last_heartbeat_time\n    }\n</code></pre>"},{"location":"guide/workers/#metrics-collection","title":"Metrics Collection","text":"<pre><code>async def process_batch(self, batch: Batch, ctx: RunContext) -&gt; BatchResult:\n    start_time = time.time()\n\n    # Do processing...\n\n    processing_time = time.time() - start_time\n\n    return BatchResult(\n        success=True,\n        metrics={\n            \"items_processed\": len(batch.payload[\"items\"]),\n            \"processing_time_ms\": int(processing_time * 1000),\n            \"memory_used_mb\": get_memory_usage()\n        }\n    )\n</code></pre>"},{"location":"guide/workers/#scaling-workers","title":"Scaling Workers","text":""},{"location":"guide/workers/#horizontal-scaling","title":"Horizontal Scaling","text":"<pre><code># Run multiple workers with same roles\nworkers = []\nfor i in range(5):\n    worker = Worker(\n        db=db,\n        cfg=WorkerConfig(roles=[\"processor\"]),\n        handlers={\"processor\": ProcessorHandler()}\n    )\n    await worker.start()\n    workers.append(worker)\n\n# Tasks are automatically distributed across workers\n</code></pre>"},{"location":"guide/workers/#role-specialization","title":"Role Specialization","text":"<pre><code># Specialized workers for different task types\ncpu_worker = Worker(cfg=WorkerConfig(roles=[\"cpu_intensive\"]))\nio_worker = Worker(cfg=WorkerConfig(roles=[\"io_intensive\"]))\ngpu_worker = Worker(cfg=WorkerConfig(roles=[\"gpu_processing\"]))\n</code></pre>"},{"location":"guide/workers/#best-practices","title":"Best Practices","text":""},{"location":"guide/workers/#handler-design","title":"Handler Design","text":"<ol> <li>Keep handlers stateless where possible</li> <li>Implement proper error classification</li> <li>Use checkpointing for long-running tasks</li> <li>Handle cancellation gracefully</li> <li>Report meaningful metrics</li> </ol>"},{"location":"guide/workers/#performance","title":"Performance","text":"<ol> <li>Tune batch sizes for your workload</li> <li>Use async I/O for external calls</li> <li>Implement connection pooling for databases</li> <li>Monitor memory usage in long-running tasks</li> </ol>"},{"location":"guide/workers/#reliability","title":"Reliability","text":"<ol> <li>Implement retry logic for transient failures</li> <li>Use idempotent operations where possible</li> <li>Save state frequently for recovery</li> <li>Test failure scenarios thoroughly</li> </ol>"},{"location":"guide/workers/#resource-management","title":"Resource Management","text":"<pre><code>class DatabaseHandler(Handler):\n    async def init(self, run_info):\n        # Create connection pool\n        self.pool = await create_connection_pool()\n\n    async def cleanup(self):\n        # Clean up resources\n        await self.pool.close()\n\n    async def process_batch(self, batch, ctx):\n        async with self.pool.acquire() as conn:\n            # Use connection for processing\n            pass\n</code></pre>"},{"location":"guide/workers/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guide/workers/#common-issues","title":"Common Issues","text":"<p>Worker not receiving tasks: - Check Kafka topic subscription - Verify worker roles match task types - Check for consumer group conflicts</p> <p>Tasks timing out: - Increase <code>lease_ttl_sec</code> - Check heartbeat frequency - Monitor processing times</p> <p>Memory leaks: - Implement proper cleanup in handlers - Monitor batch processing memory usage - Use memory profiling tools</p> <p>State corruption: - Check database connectivity - Verify state serialization/deserialization - Implement state validation</p>"},{"location":"guide/workers/#next-steps","title":"Next Steps","text":"<ul> <li>Design Task Graphs for your workflows</li> <li>Configure Error Handling policies</li> <li>Monitor Performance in production</li> <li>Test Workers thoroughly</li> </ul>"},{"location":"reference/bus/","title":"Bus API Reference","text":""},{"location":"reference/bus/#kafka-bus","title":"Kafka Bus","text":""},{"location":"reference/bus/#flowkit.bus.kafka","title":"flowkit.bus.kafka","text":""},{"location":"reference/bus/#flowkit.bus.kafka-classes","title":"Classes","text":""},{"location":"reference/bus/#flowkit.bus.kafka.KafkaBus","title":"KafkaBus","text":"<pre><code>KafkaBus(cfg: CoordinatorConfig)\n</code></pre> <p>Thin wrapper around AIOKafka with a minimal reply correlator (by corr_id).</p> Source code in <code>src/flowkit/bus/kafka.py</code> <pre><code>def __init__(self, cfg: CoordinatorConfig) -&gt; None:\n    self.cfg = cfg\n    self._producer: AIOKafkaProducer | None = None\n    self._consumers: list[AIOKafkaConsumer] = []\n    self._replies: dict[str, list[Envelope]] = {}\n    self._reply_events: dict[str, asyncio.Event] = {}\n    self.bootstrap = cfg.kafka_bootstrap\n</code></pre>"},{"location":"reference/coordinator/","title":"Coordinator API Reference","text":""},{"location":"reference/coordinator/#coordinator-class","title":"Coordinator Class","text":""},{"location":"reference/coordinator/#flowkit.coordinator.runner.Coordinator","title":"flowkit.coordinator.runner.Coordinator","text":"<pre><code>Coordinator(\n    *,\n    db,\n    cfg: CoordinatorConfig | None = None,\n    worker_types: list[str] | None = None,\n    clock: Clock | None = None,\n    adapters: dict[str, Any] | None = None,\n)\n</code></pre> <p>Orchestrates DAG execution across workers via Kafka topics. <code>db</code> is injected (e.g., Motor client). <code>clock</code> is injectable for tests.</p> Source code in <code>src/flowkit/coordinator/runner.py</code> <pre><code>def __init__(\n    self,\n    *,\n    db,\n    cfg: CoordinatorConfig | None = None,\n    worker_types: list[str] | None = None,\n    clock: Clock | None = None,\n    adapters: dict[str, Any] | None = None,\n) -&gt; None:\n    self.db = db\n    self.cfg = copy.deepcopy(cfg) if cfg is not None else CoordinatorConfig.load()\n    if worker_types:\n        self.cfg.worker_types = list(worker_types)\n\n    self.clock: Clock = clock or SystemClock()\n    self.bus = KafkaBus(self.cfg)\n    self.outbox = OutboxDispatcher(db=db, bus=self.bus, cfg=self.cfg, clock=self.clock)\n    self.adapters = adapters or dict(default_adapters(db=db, clock=self.clock))\n\n    self._tasks: set[asyncio.Task] = set()\n    self._running = False\n\n    self._announce_consumer: AIOKafkaConsumer | None = None\n    self._status_consumers: dict[str, AIOKafkaConsumer] = {}\n    self._query_reply_consumer: AIOKafkaConsumer | None = None\n\n    self._gid = f\"coord.{uuid.uuid4().hex[:6]}\"\n</code></pre>"},{"location":"reference/coordinator/#flowkit.coordinator.runner.Coordinator-functions","title":"Functions","text":""},{"location":"reference/coordinator/#flowkit.coordinator.runner.Coordinator.start","title":"start  <code>async</code>","text":"<pre><code>start() -&gt; None\n</code></pre> Source code in <code>src/flowkit/coordinator/runner.py</code> <pre><code>async def start(self) -&gt; None:\n    await self._ensure_indexes()\n    await self.bus.start()\n    await self._start_consumers()\n    await self.outbox.start()\n    self._running = True\n    self._spawn(self._scheduler_loop())\n    self._spawn(self._heartbeat_monitor())\n    self._spawn(self._finalizer_loop())\n    self._spawn(self._resume_inflight())\n</code></pre>"},{"location":"reference/coordinator/#flowkit.coordinator.runner.Coordinator.stop","title":"stop  <code>async</code>","text":"<pre><code>stop() -&gt; None\n</code></pre> Source code in <code>src/flowkit/coordinator/runner.py</code> <pre><code>async def stop(self) -&gt; None:\n    self._running = False\n    for t in list(self._tasks):\n        t.cancel()\n    self._tasks.clear()\n    await self.outbox.stop()\n    await self.bus.stop()\n</code></pre>"},{"location":"reference/coordinator/#flowkit.coordinator.runner.Coordinator.create_task","title":"create_task  <code>async</code>","text":"<pre><code>create_task(\n    *, params: dict[str, Any], graph: dict[str, Any]\n) -&gt; str\n</code></pre> Source code in <code>src/flowkit/coordinator/runner.py</code> <pre><code>async def create_task(self, *, params: dict[str, Any], graph: dict[str, Any]) -&gt; str:\n    task_id = str(uuid.uuid4())\n    graph.setdefault(\"nodes\", [])\n    graph.setdefault(\"edges\", [])\n    graph.setdefault(\"edges_ex\", [])\n    doc = TaskDoc(\n        id=task_id,\n        pipeline_id=task_id,\n        status=RunState.queued,\n        params=params,\n        graph=graph,\n        status_history=[{\"from\": None, \"to\": RunState.queued, \"at\": self.clock.now_dt()}],\n        started_at=self.clock.now_dt().isoformat(),\n        last_event_recv_ms=self.clock.now_ms(),\n    ).model_dump(mode=\"json\")\n    await self.db.tasks.insert_one(doc)\n    return task_id\n</code></pre>"},{"location":"reference/coordinator/#adapters","title":"Adapters","text":""},{"location":"reference/coordinator/#flowkit.coordinator.adapters","title":"flowkit.coordinator.adapters","text":""},{"location":"reference/coordinator/#flowkit.coordinator.adapters-classes","title":"Classes","text":""},{"location":"reference/coordinator/#flowkit.coordinator.adapters.CoordinatorAdapters","title":"CoordinatorAdapters","text":"<pre><code>CoordinatorAdapters(*, db, clock: Clock | None = None)\n</code></pre> <p>Generic functions the coordinator can call (in coordinator_fn nodes). They operate via injected <code>db</code>. Keep side-effects idempotent.</p> Source code in <code>src/flowkit/coordinator/adapters.py</code> <pre><code>def __init__(self, *, db, clock: Clock | None = None) -&gt; None:\n    self.db = db\n    self.clock = clock or SystemClock()\n</code></pre>"},{"location":"reference/core/","title":"Core API Reference","text":""},{"location":"reference/core/#configuration-classes","title":"Configuration Classes","text":""},{"location":"reference/core/#flowkit.core.config.CoordinatorConfig","title":"flowkit.core.config.CoordinatorConfig  <code>dataclass</code>","text":"<pre><code>CoordinatorConfig(\n    kafka_bootstrap: str = 'kafka:9092',\n    worker_types: list[str] = (\n        lambda: [\n            'indexer',\n            'enricher',\n            'grouper',\n            'analyzer',\n        ]\n    )(),\n    topic_cmd_fmt: str = 'cmd.{type}.v1',\n    topic_status_fmt: str = 'status.{type}.v1',\n    topic_worker_announce: str = 'workers.announce.v1',\n    topic_query: str = 'query.tasks.v1',\n    topic_reply: str = 'reply.tasks.v1',\n    topic_signals: str = 'signals.v1',\n    heartbeat_soft_sec: int = 300,\n    heartbeat_hard_sec: int = 3600,\n    lease_ttl_sec: int = 45,\n    discovery_window_sec: int = 8,\n    cancel_grace_sec: int = 30,\n    scheduler_tick_sec: float = 1.0,\n    finalizer_tick_sec: float = 5.0,\n    hb_monitor_tick_sec: float = 10.0,\n    outbox_dispatch_tick_sec: float = 0.25,\n    outbox_max_retry: int = 12,\n    outbox_backoff_min_ms: int = 250,\n    outbox_backoff_max_ms: int = 60000,\n    hb_soft_ms: int = 0,\n    hb_hard_ms: int = 0,\n    lease_ttl_ms: int = 0,\n    discovery_window_ms: int = 0,\n    cancel_grace_ms: int = 0,\n    scheduler_tick_ms: int = 0,\n    finalizer_tick_ms: int = 0,\n    hb_monitor_tick_ms: int = 0,\n    outbox_dispatch_tick_ms: int = 0,\n)\n</code></pre>"},{"location":"reference/core/#flowkit.core.config.WorkerConfig","title":"flowkit.core.config.WorkerConfig  <code>dataclass</code>","text":"<pre><code>WorkerConfig(\n    kafka_bootstrap: str = 'kafka:9092',\n    topic_cmd_fmt: str = 'cmd.{type}.v1',\n    topic_status_fmt: str = 'status.{type}.v1',\n    topic_worker_announce: str = 'workers.announce.v1',\n    topic_query: str = 'query.tasks.v1',\n    topic_reply: str = 'reply.tasks.v1',\n    topic_signals: str = 'signals.v1',\n    roles: list[str] = (lambda: ['echo'])(),\n    worker_id: str | None = None,\n    worker_version: str = '2.0.0',\n    lease_ttl_sec: int = 60,\n    hb_interval_sec: int = 20,\n    announce_interval_sec: int = 60,\n    dedup_cache_size: int = 10000,\n    dedup_ttl_ms: int = 3600000,\n    pull_poll_ms_default: int = 300,\n    pull_empty_backoff_ms_max: int = 4000,\n    db_cancel_poll_ms: int = 500,\n    state_dir: str = './.worker_state',\n    lease_ttl_ms: int = 60000,\n    hb_interval_ms: int = 20000,\n    announce_interval_ms: int = 60000,\n)\n</code></pre>"},{"location":"reference/core/#time-and-clock","title":"Time and Clock","text":""},{"location":"reference/core/#flowkit.core.time","title":"flowkit.core.time","text":""},{"location":"reference/core/#flowkit.core.time-classes","title":"Classes","text":""},{"location":"reference/core/#flowkit.core.time.SystemClock","title":"SystemClock","text":"<p>Default production/test clock.</p>"},{"location":"reference/core/#flowkit.core.time.ManualClock","title":"ManualClock","text":"<pre><code>ManualClock(start_ms: int = 0)\n</code></pre> <p>               Bases: <code>SystemClock</code></p> <p>Simple controllable clock for tests. - Wall time is advanced manually (affects persistence fields). - Monotonic time mirrors wall unless overridden.</p> Source code in <code>src/flowkit/core/time.py</code> <pre><code>def __init__(self, start_ms: int = 0) -&gt; None:\n    self._wall = start_ms\n    self._mono = start_ms\n</code></pre>"},{"location":"reference/core/#utilities","title":"Utilities","text":""},{"location":"reference/core/#flowkit.core.utils","title":"flowkit.core.utils","text":""},{"location":"reference/protocol/","title":"Protocol API Reference","text":""},{"location":"reference/protocol/#message-types-and-enums","title":"Message Types and Enums","text":""},{"location":"reference/protocol/#flowkit.protocol.messages","title":"flowkit.protocol.messages","text":""},{"location":"reference/testing/","title":"Testing Reference (WIP)","text":"<p>Helpers, fixtures, and in-memory Kafka/DB utilities. Placeholder.</p>"},{"location":"reference/worker/","title":"Worker API Reference","text":""},{"location":"reference/worker/#worker-class","title":"Worker Class","text":""},{"location":"reference/worker/#flowkit.worker.runner.Worker","title":"flowkit.worker.runner.Worker","text":"<pre><code>Worker(\n    *,\n    db,\n    cfg: WorkerConfig | None = None,\n    clock: Clock | None = None,\n    roles: list[str] | None = None,\n    handlers: dict[str, RoleHandler] | None = None,\n)\n</code></pre> <p>Stream-aware worker with cooperative cancellation and resilient batching. - Kafka I/O (producers/consumers per role) - Discovery (TASK_DISCOVER \u2192 TASK_SNAPSHOT) - Control-plane CANCEL via signals topic - DB-backed state for resume/takeover</p> Source code in <code>src/flowkit/worker/runner.py</code> <pre><code>def __init__(\n    self,\n    *,\n    db,\n    cfg: WorkerConfig | None = None,\n    clock: Clock | None = None,\n    roles: list[str] | None = None,\n    handlers: dict[str, RoleHandler] | None = None,\n) -&gt; None:\n    self.db = db\n    self.cfg = copy.deepcopy(cfg) if cfg is not None else WorkerConfig.load()\n    if roles:\n        self.cfg.roles = list(roles)\n    self.clock: Clock = clock or SystemClock()\n\n    # identity\n    self.worker_id = self.cfg.worker_id or f\"w-{uuid.uuid4().hex[:8]}\"\n    self.worker_version = self.cfg.worker_version\n\n    # adapters registry\n    self.input_adapters = build_input_adapters(db=db, clock=self.clock, cfg=self.cfg)\n\n    # handlers registry (user injects custom handlers; Echo kept as example)\n    self.handlers: dict[str, RoleHandler] = handlers or {}\n    if \"echo\" in (self.cfg.roles or []):\n        self.handlers.setdefault(\"echo\", EchoHandler())\n\n    # Kafka\n    self._producer: AIOKafkaProducer | None = None\n    self._cmd_consumers: dict[str, AIOKafkaConsumer] = {}\n    self._query_consumer: AIOKafkaConsumer | None = None\n    self._signals_consumer: AIOKafkaConsumer | None = None\n\n    # run-state\n    self._busy = False\n    self._busy_lock = asyncio.Lock()\n    self._cancel_flag = asyncio.Event()\n    self._cancel_meta: dict[str, Any] = {\"reason\": None, \"deadline_ts_ms\": None}\n    self._stopping = False\n\n    self.state = LocalStateManager(db=self.db, clock=self.clock, worker_id=self.worker_id)\n    self.active: ActiveRun | None = None\n\n    # dedup of command envelopes\n    self._dedup: OrderedDict[str, int] = OrderedDict()\n    self._dedup_lock = asyncio.Lock()\n\n    self._main_tasks: set[asyncio.Task] = set()\n</code></pre>"},{"location":"reference/worker/#flowkit.worker.runner.Worker-functions","title":"Functions","text":""},{"location":"reference/worker/#flowkit.worker.runner.Worker.start","title":"start  <code>async</code>","text":"<pre><code>start() -&gt; None\n</code></pre> Source code in <code>src/flowkit/worker/runner.py</code> <pre><code>async def start(self) -&gt; None:\n    await self._ensure_indexes()\n    self._producer = AIOKafkaProducer(\n        bootstrap_servers=self.cfg.kafka_bootstrap, value_serializer=dumps, enable_idempotence=True\n    )\n    await self._producer.start()\n\n    await self.state.refresh()\n    self.active = self.state.read_active()\n    if self.active and self.active.step_type not in self.cfg.roles:\n        self.active = None\n        await self.state.write_active(None)\n\n    await self._send_announce(\n        EventKind.WORKER_ONLINE,\n        extra={\n            \"worker_id\": self.worker_id,\n            \"type\": \",\".join(self.cfg.roles),\n            \"capabilities\": {\"roles\": self.cfg.roles},\n            \"version\": self.worker_version,\n            \"capacity\": {\"tasks\": 1},\n            \"resume\": self.active.__dict__ if self.active else None,\n        },\n    )\n\n    # command consumers per role\n    for role in self.cfg.roles:\n        topic = self.cfg.topic_cmd(role)\n        c = AIOKafkaConsumer(\n            topic,\n            bootstrap_servers=self.cfg.kafka_bootstrap,\n            value_deserializer=loads,\n            enable_auto_commit=False,\n            auto_offset_reset=\"latest\",\n            group_id=f\"workers.{role}.v1\",\n        )\n        await c.start()\n        self._cmd_consumers[role] = c\n        self._spawn(self._cmd_loop(role, c))\n\n    # query consumer (discovery)\n    self._query_consumer = AIOKafkaConsumer(\n        self.cfg.topic_query,\n        bootstrap_servers=self.cfg.kafka_bootstrap,\n        value_deserializer=loads,\n        enable_auto_commit=False,\n        auto_offset_reset=\"latest\",\n        group_id=\"workers.query.v1\",\n    )\n    await self._query_consumer.start()\n    self._spawn(self._query_loop(self._query_consumer))\n\n    # signals consumer (control plane; unique group per worker)\n    self._signals_consumer = AIOKafkaConsumer(\n        self.cfg.topic_signals,\n        bootstrap_servers=self.cfg.kafka_bootstrap,\n        value_deserializer=loads,\n        enable_auto_commit=False,\n        auto_offset_reset=\"latest\",\n        group_id=f\"workers.signals.{self.worker_id}\",\n    )\n    await self._signals_consumer.start()\n    self._spawn(self._signals_loop(self._signals_consumer))\n\n    # periodic announce\n    self._spawn(self._periodic_announce())\n\n    if self.active:\n        _json_log(self.clock, event=\"recovery_present\", task_id=self.active.task_id, node_id=self.active.node_id)\n</code></pre>"},{"location":"reference/worker/#flowkit.worker.runner.Worker.stop","title":"stop  <code>async</code>","text":"<pre><code>stop() -&gt; None\n</code></pre> Source code in <code>src/flowkit/worker/runner.py</code> <pre><code>async def stop(self) -&gt; None:\n    self._stopping = True\n    for t in list(self._main_tasks):\n        t.cancel()\n    self._main_tasks.clear()\n\n    if self._query_consumer:\n        try:\n            await self._query_consumer.stop()\n        except Exception:\n            pass\n    if self._signals_consumer:\n        try:\n            await self._signals_consumer.stop()\n        except Exception:\n            pass\n    for c in self._cmd_consumers.values():\n        try:\n            await c.stop()\n        except Exception:\n            pass\n    self._cmd_consumers.clear()\n\n    if self._producer:\n        try:\n            await self._send_announce(EventKind.WORKER_OFFLINE, extra={\"worker_id\": self.worker_id})\n            await self._producer.stop()\n        except Exception:\n            pass\n    self._producer = None\n</code></pre>"},{"location":"reference/worker/#handlers","title":"Handlers","text":""},{"location":"reference/worker/#flowkit.worker.handlers.base.RoleHandler","title":"flowkit.worker.handlers.base.RoleHandler","text":""},{"location":"reference/worker/#flowkit.worker.handlers.base.Batch","title":"flowkit.worker.handlers.base.Batch","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"reference/worker/#flowkit.worker.handlers.base.BatchResult","title":"flowkit.worker.handlers.base.BatchResult","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"reference/worker/#worker-context","title":"Worker Context","text":""},{"location":"reference/worker/#flowkit.worker.context","title":"flowkit.worker.context","text":""},{"location":"reference/worker/#flowkit.worker.context-classes","title":"Classes","text":""},{"location":"reference/worker/#flowkit.worker.context.RunContext","title":"RunContext","text":"<pre><code>RunContext(\n    *,\n    cancel_flag: Event,\n    cancel_meta: dict[str, Any],\n    artifacts_writer,\n    clock: Clock,\n    task_id: str,\n    node_id: str,\n    attempt_epoch: int,\n    worker_id: str,\n)\n</code></pre> <p>Cancellation-aware runtime utilities for handlers: - shared cancel flag + meta (reason, deadline ts) - cancellable awaits - subprocess group termination with escalation - resource cleanup registry</p> Source code in <code>src/flowkit/worker/context.py</code> <pre><code>def __init__(\n    self,\n    *,\n    cancel_flag: asyncio.Event,\n    cancel_meta: dict[str, Any],\n    artifacts_writer,\n    clock: Clock,\n    task_id: str,\n    node_id: str,\n    attempt_epoch: int,\n    worker_id: str,\n):\n    self._cancel_flag = cancel_flag\n    self._cancel_meta = cancel_meta\n    self.artifacts = artifacts_writer\n    self.clock = clock\n\n    self.task_id = task_id\n    self.node_id = node_id\n    self.attempt_epoch = attempt_epoch\n    self.worker_id = worker_id\n\n    self.kv: dict[str, Any] = {}\n    self._cleanup_callbacks: list[Callable[[], Any]] = []\n    self._subprocesses: list[Any] = []\n    self._temp_paths: list[str] = []\n</code></pre>"},{"location":"reference/worker/#worker-state","title":"Worker State","text":""},{"location":"reference/worker/#flowkit.worker.state","title":"flowkit.worker.state","text":""},{"location":"reference/worker/#flowkit.worker.state-classes","title":"Classes","text":""},{"location":"reference/worker/#flowkit.worker.state.LocalStateManager","title":"LocalStateManager","text":"<pre><code>LocalStateManager(*, db, clock: Clock, worker_id: str)\n</code></pre> <p>DB-backed state manager for crash-resume and cross-host takeover.</p> Source code in <code>src/flowkit/worker/state.py</code> <pre><code>def __init__(self, *, db, clock: Clock, worker_id: str) -&gt; None:\n    self.db = db\n    self.clock = clock\n    self.worker_id = worker_id\n    self._lock = asyncio.Lock()\n    self._cache: dict[str, Any] = {}\n</code></pre>"},{"location":"reference/worker/#flowkit.worker.state.LocalStateManager-functions","title":"Functions","text":""},{"location":"reference/worker/#flowkit.worker.state.LocalStateManager.refresh","title":"refresh  <code>async</code>","text":"<pre><code>refresh() -&gt; None\n</code></pre> <p>Pull the latest state from DB into in-memory cache.</p> Source code in <code>src/flowkit/worker/state.py</code> <pre><code>async def refresh(self) -&gt; None:\n    \"\"\"Pull the latest state from DB into in-memory cache.\"\"\"\n    doc = await self.db.worker_state.find_one({\"_id\": self.worker_id})\n    self._cache = doc or {}\n</code></pre>"},{"location":"reference/worker/#flowkit.worker.state.LocalStateManager.read_active","title":"read_active","text":"<pre><code>read_active() -&gt; ActiveRun | None\n</code></pre> <p>Read active run from cache.</p> Source code in <code>src/flowkit/worker/state.py</code> <pre><code>def read_active(self) -&gt; ActiveRun | None:\n    \"\"\"Read active run from cache.\"\"\"\n    d = (self._cache or {}).get(\"active_run\")\n    return ActiveRun(**d) if d else None\n</code></pre>"},{"location":"reference/worker/#flowkit.worker.state.LocalStateManager.write_active","title":"write_active  <code>async</code>","text":"<pre><code>write_active(ar: ActiveRun | None) -&gt; None\n</code></pre> <p>Atomically persist active run to DB and update cache.</p> Source code in <code>src/flowkit/worker/state.py</code> <pre><code>async def write_active(self, ar: ActiveRun | None) -&gt; None:\n    \"\"\"Atomically persist active run to DB and update cache.\"\"\"\n    async with self._lock:\n        payload = {\n            \"active_run\": asdict(ar) if ar else None,\n            \"updated_at\": self.clock.now_dt(),  # must be datetime for TTL index\n        }\n        await self.db.worker_state.update_one(\n            {\"_id\": self.worker_id},\n            {\"$set\": payload},\n            upsert=True,\n        )\n        self._cache.update(payload)\n</code></pre>"},{"location":"reference/worker/#artifacts","title":"Artifacts","text":""},{"location":"reference/worker/#flowkit.worker.artifacts","title":"flowkit.worker.artifacts","text":""}]}