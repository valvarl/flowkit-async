{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"FlowKit","text":"<p>FlowKit is a powerful flow orchestration toolkit that enables distributed task execution using a coordinator/worker architecture with Kafka as the message backplane.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Distributed Task Execution: Coordinate complex workflows across multiple workers</li> <li>Kafka-based Messaging: Reliable, scalable communication between components</li> <li>MongoDB Persistence: Durable task state and artifact storage</li> <li>Flexible DAG Support: Define complex task dependencies and fan-in/fan-out patterns</li> <li>Fault Tolerance: Built-in retry mechanisms, worker failure detection, and task recovery</li> <li>Streaming Processing: Support for streaming data through pipeline stages</li> <li>Testing Infrastructure: Comprehensive test helpers for building reliable workflows</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>import asyncio\nfrom flowkit import Coordinator, CoordinatorConfig, WorkerConfig\nfrom flowkit.worker import Worker\nfrom your_handlers import MyHandler\n\n# Start a coordinator\nasync def main():\n    # Configure and start coordinator\n    coord_config = CoordinatorConfig()\n    coordinator = Coordinator(db=your_mongo_db, cfg=coord_config)\n    await coordinator.start()\n\n    # Configure and start worker\n    worker_config = WorkerConfig(roles=[\"processor\"])\n    worker = Worker(\n        db=your_mongo_db,\n        cfg=worker_config,\n        handlers={\"processor\": MyHandler()}\n    )\n    await worker.start()\n\n    # Create a task with a simple graph\n    graph = {\n        \"nodes\": [\n            {\n                \"node_id\": \"process_data\",\n                \"type\": \"processor\",\n                \"io\": {\"input_inline\": {\"data\": \"hello world\"}}\n            }\n        ],\n        \"edges\": []\n    }\n\n    task_id = await coordinator.create_task(params={}, graph=graph)\n    print(f\"Created task: {task_id}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"#architecture-overview","title":"Architecture Overview","text":"<p>FlowKit consists of three main components:</p> <ol> <li>Coordinator: Orchestrates task execution, manages DAG scheduling, and monitors worker health</li> <li>Worker: Executes individual tasks and reports progress back to coordinators</li> <li>Message Bus: Kafka-based communication layer for reliable message delivery</li> </ol> <pre><code>graph TB\n    C[Coordinator] --&gt; K[Kafka]\n    W1[Worker 1] --&gt; K\n    W2[Worker 2] --&gt; K\n    W3[Worker N] --&gt; K\n    C --&gt; M[(MongoDB)]\n    W1 --&gt; M\n    W2 --&gt; M\n    W3 --&gt; M</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":"<ol> <li>Install FlowKit</li> <li>Follow the Quick Start guide</li> <li>Learn the basic concepts</li> <li>Explore examples</li> </ol>"},{"location":"#use-cases","title":"Use Cases","text":"<ul> <li>ETL Pipelines: Extract, transform, and load data across multiple stages</li> <li>ML Workflows: Orchestrate model training, validation, and deployment</li> <li>Data Processing: Handle large-scale data processing with fault tolerance</li> <li>Microservice Orchestration: Coordinate complex business processes</li> <li>Event-Driven Architecture: Build reactive systems with reliable message handling</li> </ul>"},{"location":"development/architecture/","title":"Architecture (WIP)","text":"<p>Coordinator/Worker/Outbox/Protocol components and flows. Placeholder.</p>"},{"location":"development/contributing/","title":"Contributing to FlowKit\u2011Async","text":"<p>Thank you for taking the time to contribute! This guide sets the ground rules for code, tests, docs, and CI so that changes stay reliable and fast.</p> <p>Short on time? See the TL;DR checklist below.</p>"},{"location":"development/contributing/#tldr-checklist","title":"TL;DR checklist","text":"<ul> <li>Use Python 3.11 or 3.12.</li> <li>Install dev deps: <code>pip install -e .[test]</code>.</li> <li>Run locally before pushing:</li> <li><code>ruff check src tests</code> (lint)</li> <li><code>black --check src tests</code> (format)</li> <li><code>mypy src</code> (types)</li> <li><code>pytest -n auto -q</code> (tests, parallel)</li> <li>Write Google\u2011style docstrings for public APIs; don\u2019t duplicate types already in annotations.</li> <li>Prefer precise typing (<code>collections.abc</code>, <code>dict[str, Any]</code> only at edges).</li> <li>Async code must be cancellation\u2011safe and non\u2011blocking.</li> <li>Keep PRs small and focused; link issues; add tests &amp; docs.</li> </ul>"},{"location":"development/contributing/#local-setup","title":"Local setup","text":"<pre><code>python -m venv .venv\nsource .venv/bin/activate            # Windows: .venv\\Scripts\\activate\npython -m pip install --upgrade pip\npip install -e .[test]\n# optional but recommended\npip install pre-commit &amp;&amp; pre-commit install\n</code></pre>"},{"location":"development/contributing/#useful-commands","title":"Useful commands","text":"<pre><code>ruff check src tests\nblack --check src tests\nmypy src\npytest -n auto -q\npytest -n auto --count=10 -q  # stress/flakiness check\nmkdocs serve                   # docs preview on http://localhost:8000\n</code></pre>"},{"location":"development/contributing/#branching-commits-and-prs","title":"Branching, commits, and PRs","text":"<ul> <li>Branch names: <code>feature/&lt;slug&gt;</code>, <code>fix/&lt;slug&gt;</code>, <code>chore/&lt;slug&gt;</code>.</li> <li>Keep PRs &lt; ~400 lines of diff when possible; split large changes.</li> <li>Reference issues (<code>Fixes #123</code>) and include tests.</li> <li>Commit messages: Conventional Commits recommended (not strictly required):</li> <li><code>feat(worker): cooperative cancellation for adapter</code></li> <li><code>fix(coordinator): fence duplicate lease renewals</code></li> <li><code>test(chaos): restart coordinator mid-stream</code></li> <li><code>docs(getting-started): clarify quickstart prerequisites</code></li> </ul>"},{"location":"development/contributing/#code-style","title":"Code style","text":""},{"location":"development/contributing/#docstrings-google-style","title":"Docstrings: Google style","text":"<p>Use Google\u2011style docstrings for public modules, classes, and functions. Don\u2019t repeat types already expressed in annotations.</p> <pre><code>from typing import Iterable\n\ndef stable_hash(items: Iterable[str]) -&gt; str:\n    \"\"\"Return a content\u2011stable hex digest for an iterable of strings.\n\n    Args:\n        items: Strings to hash in order.\n\n    Returns:\n        Hexadecimal SHA1 digest.\n    \"\"\"\n    ...\n</code></pre> <p>Sections you may use: Args, Returns, Raises, Yields, Examples, Notes. Keep docstrings concise and focused on behavior and invariants rather than implementation.</p>"},{"location":"development/contributing/#typing-policy","title":"Typing policy","text":"<ul> <li>Target Python 3.11+ syntax (<code>list[str]</code>, <code>X | None</code>, <code>typing.Self</code>).</li> <li>Prefer <code>collections.abc</code> for abstract containers: <code>Iterable</code>, <code>Mapping</code>, <code>Sequence</code>.</li> <li>Use <code>dict[str, Any]</code> only at system boundaries (I/O, JSON, DB). Convert to typed structures internally.</li> <li>For protocol payloads and wire messages use Pydantic v2 models (e.g., <code>Envelope</code>) to validate and serialize.</li> <li>For internal immutable data consider <code>@dataclass(frozen=True)</code>.</li> <li>Avoid <code>Any</code> unless there is a hard boundary. If used, document why.</li> <li>Always annotate async functions and return types explicitly.</li> </ul>"},{"location":"development/contributing/#formatting-linting","title":"Formatting &amp; linting","text":"<ul> <li>Black enforces formatting; Ruff enforces lint rules.</li> <li>Ruff is the source of truth for style checks; follow <code>ruff.toml</code> in the repo.</li> <li>Keep imports sorted (Ruff\u2019s <code>I</code> rules) and avoid unused code (<code>F401</code>, etc.).</li> <li>Lines should generally be \u2264 120 characters (CI enforces lints; docs may be stricter).</li> </ul>"},{"location":"development/contributing/#logging","title":"Logging","text":"<ul> <li>Prefer structured logs. Workers use <code>_json_log(clock, **kv)</code> for consistent, parseable output.</li> <li>Tests may use <code>tests.helpers.dbg()</code> for human\u2011readable traces.</li> </ul>"},{"location":"development/contributing/#async-cancellation","title":"Async &amp; cancellation","text":"<ul> <li>Never block the event loop. Use <code>await</code> for I/O and <code>asyncio.to_thread</code> for CPU\u2011bound work (sparingly).</li> <li>Handle <code>asyncio.CancelledError</code> by propagating (re\u2011raise) unless you are performing cleanup.</li> <li>Long\u2011running loops must check cancel flags (<code>ctx.cancel_flag.is_set()</code>) and yield (<code>await asyncio.sleep(\u2026)</code>).</li> <li>When catching broad exceptions, classify and surface them via handler error APIs (see <code>RoleHandler.classify_error</code>).</li> </ul>"},{"location":"development/contributing/#tests","title":"Tests","text":"<p>We use pytest with asyncio, randomized order, and parallelism.</p>"},{"location":"development/contributing/#quick-start","title":"Quick start","text":"<pre><code>pytest -n auto -q\n</code></pre>"},{"location":"development/contributing/#conventions","title":"Conventions","text":"<ul> <li>Name files <code>test_*.py</code> and mark async tests with <code>@pytest.mark.asyncio</code>.</li> <li>Use the helpers in <code>tests.helpers</code>:</li> <li>Graph helpers: <code>prime_graph</code>, <code>wait_task_finished</code>, <code>wait_node_running</code>, <code>node_by_id</code>.</li> <li>In-memory DB &amp; bus fixtures for isolated tests.</li> <li>Chaos/resiliency scenarios are marked with <code>@pytest.mark.chaos</code> and may use small sleeps; keep them deterministic otherwise.</li> <li>Every new feature/change should include unit tests; integration/chaos tests as applicable.</li> </ul>"},{"location":"development/contributing/#coverage","title":"Coverage","text":"<p>Coverage is reported in CI. Try to keep coverage steady or improving; focus on critical paths (coordinator scheduling, worker heartbeats, adapters, and cancellation).</p>"},{"location":"development/contributing/#ci-what-must-pass","title":"CI (what must pass)","text":"<p>The repository ships a CI workflow that runs on Linux with Python 3.11 and 3.12:</p> <ul> <li>Lint: <code>ruff</code>, <code>black --check</code>, <code>mypy</code>.</li> <li>Tests: <code>pytest -n auto --count=10</code> with timeouts and JUnit + coverage reports.</li> <li>Optional flake-hunt mode can repeat tests in random order to chase flakes.</li> <li>Docs build must succeed (<code>mkdocs build --strict</code>) in the docs workflow.</li> </ul> <p>If CI is red, please reproduce locally and push a fix or mark a flaky test with <code>@pytest.mark.flaky(reruns=...)</code> as a temporary measure with a tracking issue.</p>"},{"location":"development/contributing/#documentation","title":"Documentation","text":"<p>Docs live under <code>docs/</code> and are built with MkDocs (Material theme).</p> <ul> <li>Keep reference pages in sync when changing public APIs (protocol messages, coordinator/worker interfaces).</li> <li>Prefer short, task\u2011oriented pages. Put long rationale into <code>development/architecture.md</code>.</li> <li>Run <code>mkdocs serve</code> locally to preview. Do not commit the built <code>site/</code> folder; GitHub Actions deploys Pages automatically.</li> </ul>"},{"location":"development/contributing/#security-secrets","title":"Security &amp; secrets","text":"<ul> <li>Never commit credentials or tokens. Use environment variables in examples.</li> <li>Treat logs as potentially sensitive; avoid dumping full payloads unless sanitized.</li> </ul>"},{"location":"development/contributing/#design-principles-projectspecific","title":"Design principles (project\u2011specific)","text":"<ul> <li>Explicit states: transitions go through <code>RunState</code> and are visible in the task document.</li> <li>Exactly\u2011once intent for outbox/bus: deduplicate via <code>fp</code> (fingerprint), idempotent Kafka producers where applicable.</li> <li>Cooperative cancellation: make workers stop promptly and safely when signaled (signals topic, DB flags).</li> <li>Resilient batching: partial artifacts per batch, final \u201ccomplete\u201d artifact on finalize.</li> </ul>"},{"location":"development/contributing/#getting-help","title":"Getting help","text":"<ul> <li>Open a Draft PR early to discuss design.</li> <li>Use Discussions/Issues for proposals and bug reports.</li> <li>Ping maintainers in the PR if you need a review on a deadline.</li> </ul> <p>Thanks again for contributing \u2764\ufe0f</p>"},{"location":"development/github-pages/","title":"GitHub Pages Deployment Guide","text":"<p>This page explains reliable ways to publish the documentation site to GitHub Pages for the <code>valvarl/flowkit-async</code> project. It uses MkDocs (Material theme) and the official actions/deploy-pages workflow.</p> <p>If you are working in a fork, replace <code>valvarl</code> and <code>flowkit-async</code> with your own GitHub user/org and repository name.</p>"},{"location":"development/github-pages/#1-automatic-deployment-with-github-actions-recommended","title":"1) Automatic deployment with GitHub Actions (recommended)","text":"<p>The workflow below builds the site with MkDocs and publishes it to GitHub Pages on each push to the default branch. It also supports manual runs via workflow_dispatch.</p>"},{"location":"development/github-pages/#onetime-repo-settings","title":"One\u2011time repo settings","text":"<ol> <li>Open Settings \u2192 Pages.</li> <li>Under Build and deployment, set Source = GitHub Actions.</li> </ol>"},{"location":"development/github-pages/#add-workflow-githubworkflowsdocsyml","title":"Add workflow: <code>.github/workflows/docs.yml</code>","text":"<pre><code>name: Deploy Documentation\n\non:\n  push:\n    branches: [ main, master ]\n    paths:\n      - \"docs/**\"\n      - \"mkdocs.y*ml\"\n      - \"src/**\"\n      - \"pyproject.toml\"\n      - \"requirements*.txt\"\n  workflow_dispatch: {}\n\npermissions:\n  contents: read\n  pages: write\n  id-token: write\n\nconcurrency:\n  group: \"pages\"\n  cancel-in-progress: true\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Setup Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: \"3.12\"\n\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          # Install your package (if your docs import it) and doc toolchain\n          pip install -e .\n          pip install mkdocs mkdocs-material mkdocstrings[python]\n\n      - name: Build with MkDocs\n        run: |\n          mkdocs build --clean --strict\n\n      - name: Configure Pages\n        uses: actions/configure-pages@v5\n\n      - name: Upload artifact\n        uses: actions/upload-pages-artifact@v3\n        with:\n          path: ./site\n\n  deploy:\n    environment:\n      name: github-pages\n      url: ${{ steps.deployment.outputs.page_url }}\n    needs: build\n    runs-on: ubuntu-latest\n    steps:\n      - name: Deploy to GitHub Pages\n        id: deployment\n        uses: actions/deploy-pages@v4\n</code></pre> <p>After the first successful run, your docs will be available at:</p> <pre><code>https://valvarl.github.io/flowkit-async/\n</code></pre> <p>Tip: If your default branch is <code>develop</code> or <code>main</code> is protected, adjust the <code>on.push.branches</code> list accordingly.</p>"},{"location":"development/github-pages/#2-manual-deployment-alternative","title":"2) Manual deployment (alternative)","text":"<p>If you prefer to push directly to the <code>gh-pages</code> branch without Actions:</p>"},{"location":"development/github-pages/#prerequisites","title":"Prerequisites","text":"<pre><code>pip install mkdocs mkdocs-material mkdocstrings[python]\n</code></pre>"},{"location":"development/github-pages/#deploy","title":"Deploy","text":"<pre><code># Build and deploy to gh-pages (will create the branch if missing)\nmkdocs gh-deploy\n\n# With a custom message\nmkdocs gh-deploy -m \"Update documentation\"\n\n# Force re-deploy the current build (use with care)\nmkdocs gh-deploy --force\n</code></pre> <p>In Settings \u2192 Pages, set Source = Deploy from a branch and choose <code>gh-pages</code> / <code>/ (root)</code>.</p> <p>You should not use both the GitHub Actions deployment and <code>gh-deploy</code> at the same time. Pick one strategy to avoid confusion.</p>"},{"location":"development/github-pages/#3-local-development","title":"3) Local development","text":"<p>Use MkDocs\u2019 live-reload server while writing docs:</p> <pre><code># Start dev server on :8000\nmkdocs serve\n\n# Custom host/port\nmkdocs serve --dev-addr=0.0.0.0:8080\n\n# Watch extra paths (e.g., your package code)\nmkdocs serve --watch src --watch docs\n</code></pre> <p>The site will be available at http://localhost:8000 and will reload on changes.</p> <p>Build locally the same way Actions does:</p> <pre><code>mkdocs build --clean --strict\n</code></pre>"},{"location":"development/github-pages/#4-mkdocs-configuration-template","title":"4) MkDocs configuration template","text":"<p>Edit <code>mkdocs.yml</code> (or <code>mkdocs.yaml</code>) as needed. A minimal example aligned with this repository:</p> <pre><code>site_name: FlowKit Async\nsite_url: https://valvarl.github.io/flowkit-async\nrepo_url: https://github.com/valvarl/flowkit-async\nrepo_name: valvarl/flowkit-async\n\ntheme:\n  name: material\n  features:\n    - navigation.tracking\n    - navigation.expand\n    - content.code.copy\n\nnav:\n  - Home: index.md\n  - Getting Started:\n      - Quickstart: getting-started/quickstart.md\n  - Reference:\n      - Core: reference/core.md\n\nmarkdown_extensions:\n  - admonition\n  - toc:\n      permalink: true\n  - tables\n  - codehilite\n\nplugins:\n  - search\n  - mkdocstrings:\n      default_handler: python\n</code></pre> <p>Keep <code>site_url</code> in sync with your actual Pages URL. For forks, replace the owner/repo.</p>"},{"location":"development/github-pages/#custom-domain-optional","title":"Custom domain (optional)","text":"<ol> <li>Add a <code>CNAME</code> file inside the <code>docs/</code> directory so MkDocs copies it into <code>site/</code>:    <pre><code>echo \"docs.yourdomain.com\" &gt; docs/CNAME\n</code></pre></li> <li>Create a DNS CNAME record pointing <code>docs.yourdomain.com</code> \u2192 <code>valvarl.github.io</code> (or your username).</li> <li>Set <code>site_url</code> in <code>mkdocs.yml</code>:    <pre><code>site_url: https://docs.yourdomain.com\n</code></pre></li> </ol>"},{"location":"development/github-pages/#5-troubleshooting","title":"5) Troubleshooting","text":"<p>Pages shows 404 or outdated content - Make sure Settings \u2192 Pages \u2192 Source is set to GitHub Actions (for the workflow above) or <code>gh-pages</code> branch (for manual <code>gh-deploy</code>). - Check the latest workflow run logs; verify that <code>upload-pages-artifact</code> and <code>deploy-pages</code> completed successfully. - Confirm that <code>mkdocs build</code> produced <code>site/</code> with your expected pages.</p> <p>Build fails in Actions - Reproduce locally with:   <pre><code>mkdocs build --clean --strict\n</code></pre> - Ensure all doc dependencies are installed; if your docstrings import your package, install it with <code>pip install -e .</code>. - Validate configuration:   <pre><code>mkdocs config\n</code></pre></p> <p>Permission errors when deploying - The workflow must request:   <pre><code>permissions:\n  contents: read\n  pages: write\n  id-token: write\n</code></pre> - Also verify repo settings allow GitHub Pages for this branch/workflow.</p> <p>Link or image issues - Use relative links within <code>docs/</code> (MkDocs resolves them during build). - Place images under <code>docs/images/</code> and reference them relatively: <code>![Alt](images/diagram.png)</code>.</p>"},{"location":"development/github-pages/#6-best-practices","title":"6) Best practices","text":"<ol> <li>Preview locally before pushing:    <pre><code>mkdocs serve\n</code></pre></li> <li>Run strict builds in CI to catch warnings as errors:    <pre><code>mkdocs build --strict\n</code></pre></li> <li>Keep docs changes small and focused; review them in PRs.</li> <li>Avoid secrets in docs and examples; prefer environment variables where needed.</li> <li>Optimize images (lossless) to keep the site fast.</li> </ol>"},{"location":"development/github-pages/#7-url-recap","title":"7) URL recap","text":"<ul> <li>Site root: <code>https://valvarl.github.io/flowkit-async/</code></li> <li>If you add sections like \u201cGetting Started\u201d or \u201cReference\u201d, they will appear under that root according to your <code>nav</code> in <code>mkdocs.yml</code>.</li> </ul>"},{"location":"development/testing/","title":"Test Suite Reference","text":"<p>This page is generated from pytest docstrings at build time. If you see this placeholder, enable the generator plugin and rebuild docs.</p>"},{"location":"development/testing/#how-it-works","title":"How it works","text":"<p>We use mkdocs-gen-files to scan <code>tests/</code> and pull docstrings from test modules, classes, and functions, then render them as documentation. See <code>docs/_scripts/gen_tests_doc.py</code>.</p>"},{"location":"development/testing/#writing-good-test-docstrings","title":"Writing good test docstrings","text":"<pre><code>def test_example():\n    \"\"\"\n    Brief summary explaining what the test validates and why.\n    Mention key behaviors, invariants, or failure modes.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"examples/complex-workflows/","title":"Complex Workflows (WIP)","text":"<p>Multi-branch pipelines, fan-in variants, streaming patterns. Placeholder.</p>"},{"location":"examples/simple-pipeline/","title":"Simple Pipeline Example","text":"<p>This example demonstrates a basic three-stage ETL pipeline: Extract \u2192 Transform \u2192 Load.</p>"},{"location":"examples/simple-pipeline/#complete-example","title":"Complete Example","text":"<pre><code>import asyncio\nfrom typing import AsyncIterator\nfrom motor.motor_asyncio import AsyncIOMotorClient\n\nfrom flowkit import Coordinator, CoordinatorConfig, WorkerConfig\nfrom flowkit.worker import Worker\nfrom flowkit.worker.handlers.base import Handler, Batch, BatchResult, RunContext\n\n# Mock data source\nSAMPLE_DATA = [\n    {\"id\": 1, \"name\": \"Alice\", \"age\": 30, \"city\": \"New York\"},\n    {\"id\": 2, \"name\": \"Bob\", \"age\": 25, \"city\": \"San Francisco\"},\n    {\"id\": 3, \"name\": \"Charlie\", \"age\": 35, \"city\": \"Chicago\"},\n    {\"id\": 4, \"name\": \"Diana\", \"age\": 28, \"city\": \"Seattle\"},\n    {\"id\": 5, \"name\": \"Eve\", \"age\": 32, \"city\": \"Boston\"},\n]\n\nclass ExtractHandler(Handler):\n    \"\"\"Extract data from source.\"\"\"\n\n    async def iter_batches(self, input_data) -&gt; AsyncIterator[Batch]:\n        batch_size = input_data.get(\"input_inline\", {}).get(\"batch_size\", 2)\n\n        for i in range(0, len(SAMPLE_DATA), batch_size):\n            batch_data = SAMPLE_DATA[i:i + batch_size]\n            yield Batch(\n                batch_uid=f\"extract_batch_{i // batch_size}\",\n                payload={\"records\": batch_data}\n            )\n\n    async def process_batch(self, batch: Batch, ctx: RunContext) -&gt; BatchResult:\n        records = batch.payload[\"records\"]\n        print(f\"\ud83d\udce5 Extracted {len(records)} records\")\n\n        # Simulate extraction work\n        await asyncio.sleep(0.1)\n\n        return BatchResult(\n            success=True,\n            metrics={\"records_extracted\": len(records)},\n            artifacts_ref={\"data\": records, \"stage\": \"extract\"}\n        )\n\nclass TransformHandler(Handler):\n    \"\"\"Transform extracted data.\"\"\"\n\n    async def iter_batches(self, input_data) -&gt; AsyncIterator[Batch]:\n        # Use input adapter to pull from extract stage\n        input_args = input_data.get(\"input_inline\", {}).get(\"input_args\", {})\n        from_nodes = input_args.get(\"from_nodes\", [])\n\n        # In real implementation, this would pull from artifacts DB\n        # For demo, we'll simulate pulling transformed data\n        if \"extract\" in from_nodes:\n            # Simulate pulling from artifacts\n            for i in range(3):  # Mock 3 batches from extract\n                yield Batch(\n                    batch_uid=f\"transform_batch_{i}\",\n                    payload={\"batch_id\": i, \"from_extract\": True}\n                )\n\n    async def process_batch(self, batch: Batch, ctx: RunContext) -&gt; BatchResult:\n        print(f\"\ud83d\udd04 Transforming batch {batch.batch_uid}\")\n\n        # Simulate transformation: add computed fields\n        transformed_data = {\n            \"batch_id\": batch.payload.get(\"batch_id\"),\n            \"transformation\": \"added_full_name_and_category\",\n            \"processed_at\": ctx.clock.now_dt().isoformat()\n        }\n\n        await asyncio.sleep(0.15)  # Simulate work\n\n        return BatchResult(\n            success=True,\n            metrics={\"records_transformed\": 2},  # Simulated\n            artifacts_ref={\"transformed_data\": transformed_data, \"stage\": \"transform\"}\n        )\n\nclass LoadHandler(Handler):\n    \"\"\"Load transformed data to destination.\"\"\"\n\n    async def iter_batches(self, input_data) -&gt; AsyncIterator[Batch]:\n        # Pull from transform stage\n        input_args = input_data.get(\"input_inline\", {}).get(\"input_args\", {})\n        from_nodes = input_args.get(\"from_nodes\", [])\n\n        if \"transform\" in from_nodes:\n            # Simulate final loading batch\n            yield Batch(\n                batch_uid=\"load_final\",\n                payload={\"load_all\": True, \"from_transform\": True}\n            )\n\n    async def process_batch(self, batch: Batch, ctx: RunContext) -&gt; BatchResult:\n        print(f\"\ud83d\udcbe Loading data to destination\")\n\n        # Simulate loading to database/warehouse\n        await asyncio.sleep(0.2)\n\n        return BatchResult(\n            success=True,\n            metrics={\"records_loaded\": 5, \"load_time_ms\": 200},\n            artifacts_ref={\"load_status\": \"completed\", \"stage\": \"load\"}\n        )\n\nasync def run_etl_pipeline():\n    \"\"\"Run the complete ETL pipeline.\"\"\"\n\n    # Database setup\n    client = AsyncIOMotorClient(\"mongodb://localhost:27017\")\n    db = client.flowkit_examples\n\n    # Configuration\n    coord_config = CoordinatorConfig(\n        kafka_bootstrap=\"localhost:9092\",\n        worker_types=[\"extractor\", \"transformer\", \"loader\"],\n        scheduler_tick_sec=0.1  # Fast scheduling for demo\n    )\n\n    worker_config = WorkerConfig(\n        kafka_bootstrap=\"localhost:9092\",\n        roles=[\"extractor\", \"transformer\", \"loader\"]\n    )\n\n    # Start coordinator\n    coordinator = Coordinator(db=db, cfg=coord_config)\n    await coordinator.start()\n    print(\"\ud83d\ude80 Coordinator started\")\n\n    # Start worker with all handlers\n    worker = Worker(\n        db=db,\n        cfg=worker_config,\n        handlers={\n            \"extractor\": ExtractHandler(),\n            \"transformer\": TransformHandler(),\n            \"loader\": LoadHandler()\n        }\n    )\n    await worker.start()\n    print(\"\ud83d\ude80 Worker started\")\n\n    try:\n        # Define ETL graph\n        graph = {\n            \"nodes\": [\n                {\n                    \"node_id\": \"extract\",\n                    \"type\": \"extractor\",\n                    \"depends_on\": [],\n                    \"io\": {\n                        \"input_inline\": {\"batch_size\": 2}\n                    }\n                },\n                {\n                    \"node_id\": \"transform\",\n                    \"type\": \"transformer\",\n                    \"depends_on\": [\"extract\"],\n                    \"io\": {\n                        \"start_when\": \"first_batch\",  # Start as soon as extract produces data\n                        \"input_inline\": {\n                            \"input_adapter\": \"pull.from_artifacts\",\n                            \"input_args\": {\"from_nodes\": [\"extract\"]}\n                        }\n                    }\n                },\n                {\n                    \"node_id\": \"load\",\n                    \"type\": \"loader\",\n                    \"depends_on\": [\"transform\"],\n                    \"fan_in\": \"all\",  # Wait for transform to complete\n                    \"io\": {\n                        \"input_inline\": {\n                            \"input_adapter\": \"pull.from_artifacts\",\n                            \"input_args\": {\"from_nodes\": [\"transform\"]}\n                        }\n                    }\n                }\n            ],\n            \"edges\": [\n                [\"extract\", \"transform\"],\n                [\"transform\", \"load\"]\n            ]\n        }\n\n        # Create and run task\n        task_id = await coordinator.create_task(\n            params={\"pipeline_type\": \"etl_demo\"},\n            graph=graph\n        )\n        print(f\"\ud83d\udccb Created ETL task: {task_id}\")\n\n        # Monitor progress\n        print(\"\\\n\u23f3 Waiting for pipeline completion...\")\n        for i in range(30):  # Wait up to 30 seconds\n            task_doc = await db.tasks.find_one({\"id\": task_id})\n            status = task_doc.get(\"status\")\n\n            if status == \"finished\":\n                print(f\"\u2705 Pipeline completed successfully!\")\n                break\n            elif status == \"failed\":\n                print(f\"\u274c Pipeline failed\")\n                break\n\n            await asyncio.sleep(1)\n\n        # Show final results\n        task_doc = await db.tasks.find_one({\"id\": task_id})\n        nodes = {n[\"node_id\"]: n[\"status\"] for n in task_doc[\"graph\"][\"nodes\"]}\n        print(f\"\\\n\ud83d\udcca Final node statuses: {nodes}\")\n\n        # Show artifacts\n        artifacts = await db.artifacts.find({\"task_id\": task_id}).to_list(10)\n        print(f\"\ud83d\udce6 Total artifacts created: {len(artifacts)}\")\n\n        # Show metrics\n        metrics = await db.metrics_raw.find({\"task_id\": task_id}).to_list(20)\n        total_extracted = sum(m.get(\"metrics\", {}).get(\"records_extracted\", 0) for m in metrics)\n        total_transformed = sum(m.get(\"metrics\", {}).get(\"records_transformed\", 0) for m in metrics)\n        total_loaded = sum(m.get(\"metrics\", {}).get(\"records_loaded\", 0) for m in metrics)\n\n        print(f\"\\\n\ud83d\udcc8 Pipeline Metrics:\")\n        print(f\"  Records extracted: {total_extracted}\")\n        print(f\"  Records transformed: {total_transformed}\")\n        print(f\"  Records loaded: {total_loaded}\")\n\n    finally:\n        # Cleanup\n        await worker.stop()\n        await coordinator.stop()\n        client.close()\n        print(\"\\\n\u2705 Pipeline shutdown complete\")\n\nif __name__ == \"__main__\":\n    asyncio.run(run_etl_pipeline())\n</code></pre>"},{"location":"examples/simple-pipeline/#key-concepts-demonstrated","title":"Key Concepts Demonstrated","text":""},{"location":"examples/simple-pipeline/#1-handler-implementation","title":"1. Handler Implementation","text":"<p>Each stage implements the <code>Handler</code> interface with: - <code>iter_batches()</code>: Generates batches for processing - <code>process_batch()</code>: Processes individual batches - Returns <code>BatchResult</code> with success status and metrics</p>"},{"location":"examples/simple-pipeline/#2-input-adapters","title":"2. Input Adapters","text":"<p>The transform and load stages use <code>pull.from_artifacts</code> to get data from upstream stages:</p> <pre><code>\"input_inline\": {\n    \"input_adapter\": \"pull.from_artifacts\",\n    \"input_args\": {\"from_nodes\": [\"extract\"]}\n}\n</code></pre>"},{"location":"examples/simple-pipeline/#3-streaming-processing","title":"3. Streaming Processing","text":"<p>The transform stage starts as soon as extract produces its first batch:</p> <pre><code>\"io\": {\n    \"start_when\": \"first_batch\"\n}\n</code></pre>"},{"location":"examples/simple-pipeline/#4-dag-dependencies","title":"4. DAG Dependencies","text":"<p>The graph clearly defines the flow: extract \u2192 transform \u2192 load</p>"},{"location":"examples/simple-pipeline/#5-metrics-collection","title":"5. Metrics Collection","text":"<p>Each handler reports metrics that are aggregated for monitoring</p>"},{"location":"examples/simple-pipeline/#running-the-example","title":"Running the Example","text":"<ol> <li> <p>Start Kafka and MongoDB: <pre><code># Terminal 1: Kafka\ndocker run -d --name kafka -p 9092:9092 confluentinc/cp-kafka:latest\n\n# Terminal 2: MongoDB\ndocker run -d --name mongodb -p 27017:27017 mongo:6.0\n</code></pre></p> </li> <li> <p>Run the pipeline: <pre><code>python simple_pipeline.py\n</code></pre></p> </li> </ol> <p>Expected output: <pre><code>\ud83d\ude80 Coordinator started\n\ud83d\ude80 Worker started\n\ud83d\udccb Created ETL task: abc123...\n\u23f3 Waiting for pipeline completion...\n\ud83d\udce5 Extracted 2 records\n\ud83d\udce5 Extracted 2 records\n\ud83d\udce5 Extracted 1 records\n\ud83d\udd04 Transforming batch transform_batch_0\n\ud83d\udd04 Transforming batch transform_batch_1\n\ud83d\udd04 Transforming batch transform_batch_2\n\ud83d\udcbe Loading data to destination\n\u2705 Pipeline completed successfully!\n\ud83d\udcca Final node statuses: {'extract': 'finished', 'transform': 'finished', 'load': 'finished'}\n\ud83d\udce6 Total artifacts created: 7\n\ud83d\udcc8 Pipeline Metrics:\n  Records extracted: 5\n  Records transformed: 6\n  Records loaded: 5\n\u2705 Pipeline shutdown complete\n</code></pre></p>"},{"location":"examples/simple-pipeline/#next-steps","title":"Next Steps","text":"<ul> <li>Complex Workflows - Multi-path DAGs with fan-in/fan-out</li> <li>Testing Scenarios - How to test your pipelines</li> <li>Error Handling - Robust error handling patterns</li> </ul>"},{"location":"examples/testing/","title":"Testing Scenarios (WIP)","text":"<p>Chaos tests, resilience cases, concurrency &amp; lease checks. Placeholder.</p>"},{"location":"getting-started/concepts/","title":"Basic Concepts","text":"<p>Understanding FlowKit's core concepts will help you build robust, scalable workflows.</p>"},{"location":"getting-started/concepts/#core-components","title":"Core Components","text":""},{"location":"getting-started/concepts/#coordinator","title":"Coordinator","text":"<p>The Coordinator is the brain of FlowKit. It:</p> <ul> <li>Schedules tasks based on DAG dependencies</li> <li>Monitors worker health and task progress</li> <li>Handles task retries and failure recovery</li> <li>Manages task state transitions</li> <li>Coordinates multiple workers across different types</li> </ul> <pre><code>from flowkit import Coordinator, CoordinatorConfig\n\ncoordinator = Coordinator(\n    db=mongodb_instance,\n    cfg=CoordinatorConfig(worker_types=[\"indexer\", \"processor\"])\n)\n</code></pre>"},{"location":"getting-started/concepts/#worker","title":"Worker","text":"<p>A Worker executes individual tasks. Workers:</p> <ul> <li>Pull tasks from Kafka topics</li> <li>Execute custom handler logic</li> <li>Report progress and results back to coordinators</li> <li>Handle task cancellation and cleanup</li> <li>Support multiple roles/task types</li> </ul> <pre><code>from flowkit.worker import Worker, WorkerConfig\n\nworker = Worker(\n    db=mongodb_instance,\n    cfg=WorkerConfig(roles=[\"processor\"]),\n    handlers={\"processor\": MyProcessorHandler()}\n)\n</code></pre>"},{"location":"getting-started/concepts/#task-graph-dag","title":"Task Graph (DAG)","text":"<p>Tasks are organized into Directed Acyclic Graphs (DAGs) that define:</p> <ul> <li>Nodes: Individual processing steps</li> <li>Edges: Dependencies between nodes</li> <li>Fan-in/Fan-out: How data flows between stages</li> </ul> <pre><code>graph = {\n    \"nodes\": [\n        {\"node_id\": \"extract\", \"type\": \"extractor\", \"depends_on\": []},\n        {\"node_id\": \"transform\", \"type\": \"transformer\", \"depends_on\": [\"extract\"]},\n        {\"node_id\": \"load\", \"type\": \"loader\", \"depends_on\": [\"transform\"]}\n    ],\n    \"edges\": [[\"extract\", \"transform\"], [\"transform\", \"load\"]]\n}\n</code></pre>"},{"location":"getting-started/concepts/#key-concepts","title":"Key Concepts","text":""},{"location":"getting-started/concepts/#handlers","title":"Handlers","text":"<p>Handlers define the actual processing logic for each task type:</p> <pre><code>from flowkit.worker.handlers.base import Handler, Batch, BatchResult\n\nclass MyHandler(Handler):\n    async def iter_batches(self, input_data) -&gt; AsyncIterator[Batch]:\n        # Generate batches from input\n        for item in input_data:\n            yield Batch(batch_uid=item.id, payload=item.data)\n\n    async def process_batch(self, batch: Batch, ctx: RunContext) -&gt; BatchResult:\n        # Process single batch\n        result = process_data(batch.payload)\n        return BatchResult(success=True, artifacts_ref=result)\n\n    async def finalize(self, ctx: RunContext) -&gt; FinalizationResult:\n        # Clean up after all batches processed\n        return FinalizationResult(metrics={\"total_processed\": ctx.processed_count})\n</code></pre>"},{"location":"getting-started/concepts/#artifacts","title":"Artifacts","text":"<p>Artifacts are the data outputs from each processing stage:</p> <ul> <li>Stored in MongoDB with metadata</li> <li>Can be partial (streaming) or complete</li> <li>Referenced by downstream stages</li> <li>Include processing metrics and status</li> </ul>"},{"location":"getting-started/concepts/#input-adapters","title":"Input Adapters","text":"<p>Input Adapters define how data flows between stages:</p> <pre><code># Pull from upstream artifacts\n\"input_inline\": {\n    \"input_adapter\": \"pull.from_artifacts\",\n    \"input_args\": {\"from_nodes\": [\"upstream_node\"]}\n}\n\n# Rechunk data with different batch sizes\n\"input_inline\": {\n    \"input_adapter\": \"pull.from_artifacts.rechunk:size\",\n    \"input_args\": {\"from_nodes\": [\"upstream\"], \"size\": 10}\n}\n</code></pre>"},{"location":"getting-started/concepts/#task-states","title":"Task States","text":"<p>Tasks progress through several states:</p> <ul> <li>queued: Waiting to be scheduled</li> <li>running: Currently being processed</li> <li>deferred: Temporarily paused (e.g., for retry)</li> <li>finished: Completed successfully</li> <li>failed: Terminated with error</li> <li>cancelling: Being cancelled</li> </ul>"},{"location":"getting-started/concepts/#flow-control-patterns","title":"Flow Control Patterns","text":""},{"location":"getting-started/concepts/#fan-in-strategies","title":"Fan-in Strategies","text":"<p>Control when a node starts based on dependencies:</p> <pre><code>{\n    \"node_id\": \"combiner\",\n    \"depends_on\": [\"node1\", \"node2\", \"node3\"],\n    \"fan_in\": \"all\"        # Wait for all dependencies (default)\n    # \"fan_in\": \"any\"      # Start when any dependency completes\n    # \"fan_in\": \"count:2\"  # Start when 2 dependencies complete\n}\n</code></pre>"},{"location":"getting-started/concepts/#streaming-vs-batch-processing","title":"Streaming vs Batch Processing","text":"<pre><code># Start processing as soon as first batch is available\n{\n    \"node_id\": \"processor\",\n    \"io\": {\"start_when\": \"first_batch\"}\n}\n\n# Wait for upstream to completely finish\n{\n    \"node_id\": \"processor\",\n    \"io\": {\"start_when\": \"ready\"}  # default\n}\n</code></pre>"},{"location":"getting-started/concepts/#coordinator-functions","title":"Coordinator Functions","text":"<p>Execute logic directly in the coordinator (no worker needed):</p> <pre><code>{\n    \"node_id\": \"merger\",\n    \"type\": \"coordinator_fn\",\n    \"io\": {\n        \"fn\": \"merge.generic\",\n        \"fn_args\": {\n            \"from_nodes\": [\"node1\", \"node2\"],\n            \"target\": {\"key\": \"merged_result\"}\n        }\n    }\n}\n</code></pre>"},{"location":"getting-started/concepts/#error-handling","title":"Error Handling","text":""},{"location":"getting-started/concepts/#retry-policies","title":"Retry Policies","text":"<pre><code>{\n    \"node_id\": \"flaky_task\",\n    \"retry_policy\": {\n        \"max\": 3,\n        \"backoff_sec\": 300,\n        \"permanent_on\": [\"bad_input\", \"schema_mismatch\"]\n    }\n}\n</code></pre>"},{"location":"getting-started/concepts/#error-classification","title":"Error Classification","text":"<p>Handlers can classify errors as permanent or transient:</p> <pre><code>def classify_error(self, error: Exception) -&gt; tuple[str, bool]:\n    if isinstance(error, ValidationError):\n        return \"validation_failed\", True  # Permanent - don't retry\n    else:\n        return \"temporary_failure\", False  # Transient - retry\n</code></pre>"},{"location":"getting-started/concepts/#message-flow","title":"Message Flow","text":"<ol> <li>Coordinator publishes task commands to Kafka topics</li> <li>Workers consume from role-specific topics (e.g., <code>cmd.processor.v1</code>)</li> <li>Workers publish status updates to status topics</li> <li>Coordinator consumes status updates and updates task state</li> <li>Artifacts are stored in MongoDB for inter-stage communication</li> </ol>"},{"location":"getting-started/concepts/#scalability-patterns","title":"Scalability Patterns","text":""},{"location":"getting-started/concepts/#horizontal-scaling","title":"Horizontal Scaling","text":"<ul> <li>Run multiple coordinators (they coordinate through MongoDB)</li> <li>Run multiple workers of the same type for parallel processing</li> <li>Workers auto-discover tasks and distribute load</li> </ul>"},{"location":"getting-started/concepts/#partitioning","title":"Partitioning","text":"<ul> <li>Use Kafka partitioning for parallel processing</li> <li>Workers process partitions independently</li> <li>Results are merged by downstream stages</li> </ul>"},{"location":"getting-started/concepts/#resource-management","title":"Resource Management","text":"<ul> <li>Configure worker capacity limits</li> <li>Set concurrency limits per worker type</li> <li>Use backpressure mechanisms for flow control</li> </ul>"},{"location":"getting-started/concepts/#next-steps","title":"Next Steps","text":"<ul> <li>Set up Coordinators</li> <li>Create Custom Workers</li> <li>Design Task Graphs</li> <li>Handle Errors Gracefully</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.11+</li> <li>Apache Kafka</li> <li>MongoDB</li> <li>Docker (optional, for development)</li> </ul>"},{"location":"getting-started/installation/#install-flowkit","title":"Install FlowKit","text":""},{"location":"getting-started/installation/#from-pypi","title":"From PyPI","text":"<pre><code>pip install flowkit\n</code></pre>"},{"location":"getting-started/installation/#from-source","title":"From Source","text":"<pre><code>git clone https://github.com/your-org/flowkit.git\ncd flowkit\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>For development with all dependencies:</p> <pre><code>git clone https://github.com/your-org/flowkit.git\ncd flowkit\npip install -e \".[dev,test]\"\n</code></pre>"},{"location":"getting-started/installation/#infrastructure-setup","title":"Infrastructure Setup","text":""},{"location":"getting-started/installation/#apache-kafka","title":"Apache Kafka","text":"<p>FlowKit requires Kafka for message coordination. You can run Kafka locally using Docker:</p> <pre><code># Start Zookeeper and Kafka\ndocker run -d --name zookeeper -p 2181:2181 zookeeper:3.8\ndocker run -d --name kafka -p 9092:9092 \\\n    --link zookeeper:zookeeper \\\n    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \\\n    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092 \\\n    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \\\n    confluentinc/cp-kafka:latest\n</code></pre> <p>Or use the provided Docker Compose file:</p> <pre><code>cd flowkit\ndocker-compose up -d kafka\n</code></pre>"},{"location":"getting-started/installation/#mongodb","title":"MongoDB","text":"<p>FlowKit uses MongoDB for task state and artifact persistence:</p> <pre><code># Run MongoDB with Docker\ndocker run -d --name mongodb -p 27017:27017 mongo:6.0\n</code></pre> <p>Or with Docker Compose:</p> <pre><code>cd flowkit\ndocker-compose up -d mongodb\n</code></pre>"},{"location":"getting-started/installation/#configuration","title":"Configuration","text":"<p>Create configuration files for your coordinator and workers:</p>"},{"location":"getting-started/installation/#coordinator-configuration-coordinatorjson","title":"Coordinator Configuration (<code>coordinator.json</code>)","text":"<pre><code>{\n    \"kafka_bootstrap\": \"localhost:9092\",\n    \"worker_types\": [\"indexer\", \"processor\", \"analyzer\"],\n    \"heartbeat_soft_sec\": 300,\n    \"heartbeat_hard_sec\": 3600,\n    \"lease_ttl_sec\": 60\n}\n</code></pre>"},{"location":"getting-started/installation/#worker-configuration-workerjson","title":"Worker Configuration (<code>worker.json</code>)","text":"<pre><code>{\n    \"kafka_bootstrap\": \"localhost:9092\",\n    \"roles\": [\"processor\"],\n    \"worker_id\": null,\n    \"lease_ttl_sec\": 60,\n    \"hb_interval_sec\": 20\n}\n</code></pre>"},{"location":"getting-started/installation/#verification","title":"Verification","text":"<p>Test your installation:</p> <pre><code>import flowkit\nprint(f\"FlowKit version: {flowkit.__version__}\")\n\n# Test coordinator and worker imports\nfrom flowkit import Coordinator, CoordinatorConfig, WorkerConfig\nfrom flowkit.worker import Worker\nprint(\"FlowKit installed successfully!\")\n</code></pre>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>This guide will walk you through creating your first FlowKit pipeline in just a few minutes.</p>"},{"location":"getting-started/quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>FlowKit installed (Installation Guide)</li> <li>Kafka running on <code>localhost:9092</code></li> <li>MongoDB running on <code>localhost:27017</code></li> </ul>"},{"location":"getting-started/quickstart/#step-1-create-a-simple-handler","title":"Step 1: Create a Simple Handler","text":"<p>First, create a custom handler that will process your data:</p> <pre><code># handlers.py\nimport asyncio\nfrom typing import AsyncIterator\nfrom flowkit.worker.handlers.base import Handler, Batch, BatchResult, RunContext\n\nclass EchoHandler(Handler):\n    \"\"\"A simple handler that echoes input data.\"\"\"\n\n    async def iter_batches(self, input_data) -&gt; AsyncIterator[Batch]:\n        \"\"\"Generate batches from input data.\"\"\"\n        data = input_data.get(\"input_inline\", {}).get(\"message\", \"Hello World\")\n        yield Batch(\n            batch_uid=\"batch_1\",\n            payload={\"message\": data, \"processed_at\": \"now\"}\n        )\n\n    async def process_batch(self, batch: Batch, ctx: RunContext) -&gt; BatchResult:\n        \"\"\"Process a single batch.\"\"\"\n        message = batch.payload.get(\"message\", \"\")\n        processed_message = f\"Echo: {message.upper()}\"\n\n        print(f\"Processing: {processed_message}\")\n\n        # Simulate some work\n        await asyncio.sleep(0.1)\n\n        return BatchResult(\n            success=True,\n            metrics={\"messages_processed\": 1},\n            artifacts_ref={\"output\": processed_message}\n        )\n</code></pre>"},{"location":"getting-started/quickstart/#step-2-set-up-database-connection","title":"Step 2: Set Up Database Connection","text":"<pre><code># database.py\nfrom motor.motor_asyncio import AsyncIOMotorClient\n\nasync def get_database():\n    \"\"\"Get MongoDB database connection.\"\"\"\n    client = AsyncIOMotorClient(\"mongodb://localhost:27017\")\n    return client.flowkit_db\n</code></pre>"},{"location":"getting-started/quickstart/#step-3-create-your-first-pipeline","title":"Step 3: Create Your First Pipeline","text":"<pre><code># pipeline.py\nimport asyncio\nfrom flowkit import Coordinator, CoordinatorConfig, WorkerConfig\nfrom flowkit.worker import Worker\nfrom handlers import EchoHandler\nfrom database import get_database\n\nasync def run_pipeline():\n    # Get database connection\n    db = await get_database()\n\n    # Configure coordinator\n    coord_config = CoordinatorConfig(\n        kafka_bootstrap=\"localhost:9092\",\n        worker_types=[\"echo\"]\n    )\n\n    # Configure worker\n    worker_config = WorkerConfig(\n        kafka_bootstrap=\"localhost:9092\",\n        roles=[\"echo\"]\n    )\n\n    # Start coordinator\n    coordinator = Coordinator(db=db, cfg=coord_config)\n    await coordinator.start()\n    print(\"\u2705 Coordinator started\")\n\n    # Start worker\n    worker = Worker(\n        db=db,\n        cfg=worker_config,\n        handlers={\"echo\": EchoHandler()}\n    )\n    await worker.start()\n    print(\"\u2705 Worker started\")\n\n    try:\n        # Create a simple task graph\n        graph = {\n            \"nodes\": [\n                {\n                    \"node_id\": \"echo_task\",\n                    \"type\": \"echo\",\n                    \"depends_on\": [],\n                    \"io\": {\n                        \"input_inline\": {\n                            \"message\": \"Hello FlowKit!\"\n                        }\n                    }\n                }\n            ],\n            \"edges\": []\n        }\n\n        # Create and execute task\n        task_id = await coordinator.create_task(\n            params={\"pipeline_name\": \"quickstart\"},\n            graph=graph\n        )\n        print(f\"\u2705 Created task: {task_id}\")\n\n        # Wait for completion (in real apps, you'd monitor differently)\n        await asyncio.sleep(5)\n\n        # Check task status\n        task_doc = await db.tasks.find_one({\"id\": task_id})\n        print(f\"\u2705 Task status: {task_doc['status']}\")\n\n        # Check artifacts\n        artifacts = await db.artifacts.find({\"task_id\": task_id}).to_list(10)\n        print(f\"\u2705 Artifacts created: {len(artifacts)}\")\n\n    finally:\n        # Clean shutdown\n        await worker.stop()\n        await coordinator.stop()\n        print(\"\u2705 Pipeline shutdown complete\")\n\nif __name__ == \"__main__\":\n    asyncio.run(run_pipeline())\n</code></pre>"},{"location":"getting-started/quickstart/#step-4-run-your-pipeline","title":"Step 4: Run Your Pipeline","text":"<pre><code>python pipeline.py\n</code></pre> <p>Expected output: <pre><code>\u2705 Coordinator started\n\u2705 Worker started\n\u2705 Created task: 12345678-1234-1234-1234-123456789abc\nProcessing: Echo: HELLO FLOWKIT!\n\u2705 Task status: finished\n\u2705 Artifacts created: 1\n\u2705 Pipeline shutdown complete\n</code></pre></p>"},{"location":"getting-started/quickstart/#step-5-create-a-multi-stage-pipeline","title":"Step 5: Create a Multi-Stage Pipeline","text":"<p>Now let's create a more complex pipeline with multiple stages:</p> <pre><code># complex_pipeline.py\nimport asyncio\nfrom flowkit import Coordinator, CoordinatorConfig, WorkerConfig\nfrom flowkit.worker import Worker\nfrom handlers import EchoHandler\nfrom database import get_database\n\nclass ProcessorHandler(Handler):\n    \"\"\"Processes data from previous stage.\"\"\"\n\n    async def iter_batches(self, input_data) -&gt; AsyncIterator[Batch]:\n        # Use input adapter to pull from artifacts\n        input_adapter = input_data.get(\"input_inline\", {}).get(\"input_adapter\")\n        if input_adapter == \"pull.from_artifacts\":\n            # This would pull from upstream artifacts\n            # For simplicity, we'll simulate\n            yield Batch(\n                batch_uid=\"process_batch_1\",\n                payload={\"data\": \"processed_data\", \"stage\": \"processor\"}\n            )\n\n    async def process_batch(self, batch: Batch, ctx: RunContext) -&gt; BatchResult:\n        data = batch.payload.get(\"data\", \"\")\n        result = f\"Processed: {data}\"\n\n        print(f\"Stage 2 - {result}\")\n        await asyncio.sleep(0.1)\n\n        return BatchResult(\n            success=True,\n            metrics={\"items_processed\": 1},\n            artifacts_ref={\"result\": result}\n        )\n\nasync def run_complex_pipeline():\n    db = await get_database()\n\n    coord_config = CoordinatorConfig(\n        kafka_bootstrap=\"localhost:9092\",\n        worker_types=[\"echo\", \"processor\"]\n    )\n\n    worker_config = WorkerConfig(\n        kafka_bootstrap=\"localhost:9092\",\n        roles=[\"echo\", \"processor\"]\n    )\n\n    coordinator = Coordinator(db=db, cfg=coord_config)\n    await coordinator.start()\n\n    worker = Worker(\n        db=db,\n        cfg=worker_config,\n        handlers={\n            \"echo\": EchoHandler(),\n            \"processor\": ProcessorHandler()\n        }\n    )\n    await worker.start()\n\n    try:\n        # Multi-stage graph\n        graph = {\n            \"nodes\": [\n                {\n                    \"node_id\": \"stage1\",\n                    \"type\": \"echo\",\n                    \"depends_on\": [],\n                    \"io\": {\"input_inline\": {\"message\": \"Input data\"}}\n                },\n                {\n                    \"node_id\": \"stage2\",\n                    \"type\": \"processor\",\n                    \"depends_on\": [\"stage1\"],\n                    \"io\": {\n                        \"input_inline\": {\n                            \"input_adapter\": \"pull.from_artifacts\",\n                            \"input_args\": {\"from_nodes\": [\"stage1\"]}\n                        }\n                    }\n                }\n            ],\n            \"edges\": [[\"stage1\", \"stage2\"]]  # stage1 -&gt; stage2\n        }\n\n        task_id = await coordinator.create_task(params={}, graph=graph)\n        print(f\"\u2705 Multi-stage task created: {task_id}\")\n\n        await asyncio.sleep(10)  # Wait for completion\n\n        task_doc = await db.tasks.find_one({\"id\": task_id})\n        print(f\"\u2705 Final status: {task_doc['status']}\")\n\n    finally:\n        await worker.stop()\n        await coordinator.stop()\n\nif __name__ == \"__main__\":\n    asyncio.run(run_complex_pipeline())\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Basic Concepts</li> <li>Explore Task Graphs in detail</li> <li>Check out more Examples</li> <li>Read about Error Handling</li> </ul>"},{"location":"guide/configuration/","title":"Configuration (WIP)","text":"<p>Coordinator/Worker settings, environment overrides, and defaults. Placeholder.</p>"},{"location":"guide/coordinators/","title":"Coordinators","text":"<p>Coordinators are the orchestration layer of FlowKit, responsible for managing task execution, scheduling DAG nodes, and monitoring worker health.</p>"},{"location":"guide/coordinators/#overview","title":"Overview","text":"<p>A Coordinator:</p> <ul> <li>Schedules Tasks: Determines when nodes in a DAG are ready to execute</li> <li>Manages Workers: Tracks worker health and capacity</li> <li>Handles Failures: Implements retry policies and failure recovery</li> <li>Coordinates State: Maintains task state in MongoDB</li> <li>Routes Messages: Uses Kafka for reliable communication with workers</li> </ul>"},{"location":"guide/coordinators/#basic-setup","title":"Basic Setup","text":"<pre><code>from flowkit import Coordinator, CoordinatorConfig\nfrom motor.motor_asyncio import AsyncIOMotorClient\n\nasync def setup_coordinator():\n    # Database connection\n    client = AsyncIOMotorClient(\"mongodb://localhost:27017\")\n    db = client.flowkit\n\n    # Configuration\n    config = CoordinatorConfig(\n        kafka_bootstrap=\"localhost:9092\",\n        worker_types=[\"processor\", \"analyzer\", \"loader\"]\n    )\n\n    # Create and start coordinator\n    coordinator = Coordinator(db=db, cfg=config)\n    await coordinator.start()\n\n    return coordinator\n</code></pre>"},{"location":"guide/coordinators/#configuration-options","title":"Configuration Options","text":""},{"location":"guide/coordinators/#core-settings","title":"Core Settings","text":"<pre><code>config = CoordinatorConfig(\n    # Kafka configuration\n    kafka_bootstrap=\"localhost:9092\",\n    worker_types=[\"indexer\", \"processor\", \"analyzer\"],\n\n    # Topic naming patterns\n    topic_cmd_fmt=\"cmd.{type}.v1\",\n    topic_status_fmt=\"status.{type}.v1\",\n\n    # Timing settings (seconds)\n    heartbeat_soft_sec=300,      # Soft heartbeat timeout\n    heartbeat_hard_sec=3600,     # Hard heartbeat timeout\n    lease_ttl_sec=45,            # Worker lease duration\n    scheduler_tick_sec=1.0,      # Scheduling frequency\n\n    # Retry and backoff\n    cancel_grace_sec=30,         # Grace period for cancellation\n    outbox_max_retry=12,         # Max retry attempts\n    outbox_backoff_min_ms=250,   # Min backoff time\n    outbox_backoff_max_ms=60000, # Max backoff time\n)\n</code></pre>"},{"location":"guide/coordinators/#loading-from-file","title":"Loading from File","text":"<pre><code># Load from JSON file\nconfig = CoordinatorConfig.load(\"configs/coordinator.json\")\n\n# With overrides\nconfig = CoordinatorConfig.load(\n    \"configs/coordinator.json\",\n    overrides={\"scheduler_tick_sec\": 0.5}\n)\n\n# Environment variables\n# KAFKA_BOOTSTRAP_SERVERS, WORKER_TYPES are automatically loaded\n</code></pre>"},{"location":"guide/coordinators/#task-management","title":"Task Management","text":""},{"location":"guide/coordinators/#creating-tasks","title":"Creating Tasks","text":"<pre><code>async def create_pipeline_task(coordinator):\n    graph = {\n        \"nodes\": [\n            {\n                \"node_id\": \"extract\",\n                \"type\": \"extractor\",\n                \"depends_on\": [],\n                \"io\": {\"input_inline\": {\"source\": \"database\"}}\n            },\n            {\n                \"node_id\": \"transform\",\n                \"type\": \"processor\",\n                \"depends_on\": [\"extract\"],\n                \"io\": {\n                    \"input_inline\": {\n                        \"input_adapter\": \"pull.from_artifacts\",\n                        \"input_args\": {\"from_nodes\": [\"extract\"]}\n                    }\n                }\n            }\n        ],\n        \"edges\": [[\"extract\", \"transform\"]]\n    }\n\n    task_id = await coordinator.create_task(\n        params={\"pipeline_name\": \"etl_demo\"},\n        graph=graph\n    )\n\n    return task_id\n</code></pre>"},{"location":"guide/coordinators/#task-lifecycle","title":"Task Lifecycle","text":"<p>Tasks progress through these states:</p> <ol> <li>queued \u2192 Task created, waiting to be scheduled</li> <li>running \u2192 At least one node is executing</li> <li>finished \u2192 All nodes completed successfully</li> <li>failed \u2192 Task failed permanently</li> <li>deferred \u2192 Temporarily paused for retry</li> </ol>"},{"location":"guide/coordinators/#monitoring-tasks","title":"Monitoring Tasks","text":"<pre><code>async def monitor_task(coordinator, task_id):\n    # Get task document from database\n    task_doc = await coordinator.db.tasks.find_one({\"id\": task_id})\n\n    # Check overall status\n    print(f\"Task status: {task_doc['status']}\")\n\n    # Check individual node statuses\n    for node in task_doc[\"graph\"][\"nodes\"]:\n        print(f\"Node {node['node_id']}: {node['status']}\")\n\n    # Check artifacts\n    artifacts = await coordinator.db.artifacts.find(\n        {\"task_id\": task_id}\n    ).to_list(100)\n    print(f\"Artifacts created: {len(artifacts)}\")\n</code></pre>"},{"location":"guide/coordinators/#scheduling-behavior","title":"Scheduling Behavior","text":""},{"location":"guide/coordinators/#node-readiness","title":"Node Readiness","text":"<p>The coordinator schedules nodes when:</p> <ol> <li>Dependencies satisfied: All <code>depends_on</code> nodes are finished</li> <li>Fan-in condition met: Based on <code>fan_in</code> strategy</li> <li>Not already running: Node isn't currently being processed</li> <li>Retry conditions met: If deferred, retry time has passed</li> </ol>"},{"location":"guide/coordinators/#fan-in-strategies","title":"Fan-in Strategies","text":"<pre><code># Wait for all dependencies (default)\n{\n    \"node_id\": \"combiner\",\n    \"depends_on\": [\"node1\", \"node2\", \"node3\"],\n    \"fan_in\": \"all\"\n}\n\n# Start when any dependency completes\n{\n    \"node_id\": \"processor\",\n    \"depends_on\": [\"node1\", \"node2\"],\n    \"fan_in\": \"any\"\n}\n\n# Start when N dependencies complete\n{\n    \"node_id\": \"analyzer\",\n    \"depends_on\": [\"node1\", \"node2\", \"node3\"],\n    \"fan_in\": \"count:2\"\n}\n</code></pre>"},{"location":"guide/coordinators/#streaming-execution","title":"Streaming Execution","text":"<pre><code># Start as soon as upstream produces first batch\n{\n    \"node_id\": \"processor\",\n    \"depends_on\": [\"extractor\"],\n    \"io\": {\"start_when\": \"first_batch\"}\n}\n\n# Wait for upstream to completely finish (default)\n{\n    \"node_id\": \"loader\",\n    \"depends_on\": [\"processor\"],\n    \"io\": {\"start_when\": \"ready\"}\n}\n</code></pre>"},{"location":"guide/coordinators/#coordinator-functions","title":"Coordinator Functions","text":"<p>Coordinators can execute logic directly without workers:</p> <pre><code>{\n    \"node_id\": \"merger\",\n    \"type\": \"coordinator_fn\",\n    \"depends_on\": [\"extract1\", \"extract2\"],\n    \"io\": {\n        \"fn\": \"merge.generic\",\n        \"fn_args\": {\n            \"from_nodes\": [\"extract1\", \"extract2\"],\n            \"target\": {\"key\": \"merged_data\"}\n        }\n    }\n}\n</code></pre> <p>Built-in coordinator functions:</p> <ul> <li><code>merge.generic</code>: Merge artifacts from multiple nodes</li> <li><code>copy.artifacts</code>: Copy artifacts between nodes</li> <li><code>transform.metadata</code>: Transform artifact metadata</li> </ul>"},{"location":"guide/coordinators/#error-handling","title":"Error Handling","text":""},{"location":"guide/coordinators/#retry-policies","title":"Retry Policies","text":"<pre><code>{\n    \"node_id\": \"flaky_processor\",\n    \"retry_policy\": {\n        \"max\": 3,                    # Maximum retry attempts\n        \"backoff_sec\": 300,          # Backoff between retries\n        \"permanent_on\": [            # Errors that shouldn't retry\n            \"bad_input\",\n            \"schema_mismatch\"\n        ]\n    }\n}\n</code></pre>"},{"location":"guide/coordinators/#cascade-cancellation","title":"Cascade Cancellation","text":"<p>When a node fails permanently, the coordinator can cancel downstream nodes:</p> <pre><code># This happens automatically for permanent failures\n# You can also trigger manual cancellation:\n\nawait coordinator.cascade_cancel(\n    task_id=\"abc123\",\n    reason=\"upstream_failure\"\n)\n</code></pre>"},{"location":"guide/coordinators/#scaling-coordinators","title":"Scaling Coordinators","text":""},{"location":"guide/coordinators/#multiple-coordinators","title":"Multiple Coordinators","text":"<p>You can run multiple coordinators for high availability:</p> <pre><code># Coordinator 1\ncoordinator1 = Coordinator(db=db, cfg=config, worker_types=[\"processor\"])\n\n# Coordinator 2\ncoordinator2 = Coordinator(db=db, cfg=config, worker_types=[\"analyzer\"])\n\n# They coordinate through MongoDB and don't conflict\nawait coordinator1.start()\nawait coordinator2.start()\n</code></pre>"},{"location":"guide/coordinators/#load-balancing","title":"Load Balancing","text":"<p>Coordinators automatically distribute work:</p> <ul> <li>Task scheduling is coordinated through MongoDB</li> <li>Workers are discovered dynamically</li> <li>No single point of failure</li> </ul>"},{"location":"guide/coordinators/#monitoring-and-observability","title":"Monitoring and Observability","text":""},{"location":"guide/coordinators/#health-checks","title":"Health Checks","text":"<pre><code>async def check_coordinator_health(coordinator):\n    # Check if coordinator is running\n    if not coordinator._running:\n        return \"stopped\"\n\n    # Check active workers\n    workers = await coordinator.db.worker_registry.find(\n        {\"status\": \"online\"}\n    ).to_list(100)\n\n    # Check pending tasks\n    pending = await coordinator.db.tasks.count_documents(\n        {\"status\": {\"$in\": [\"queued\", \"running\"]}}\n    )\n\n    return {\n        \"status\": \"healthy\",\n        \"active_workers\": len(workers),\n        \"pending_tasks\": pending\n    }\n</code></pre>"},{"location":"guide/coordinators/#metrics-collection","title":"Metrics Collection","text":"<pre><code>async def collect_coordinator_metrics(coordinator):\n    # Task metrics\n    tasks_by_status = await coordinator.db.tasks.aggregate([\n        {\"$group\": {\"_id\": \"$status\", \"count\": {\"$sum\": 1}}}\n    ]).to_list(10)\n\n    # Worker metrics\n    workers_by_status = await coordinator.db.worker_registry.aggregate([\n        {\"$group\": {\"_id\": \"$status\", \"count\": {\"$sum\": 1}}}\n    ]).to_list(10)\n\n    # Recent events\n    recent_events = await coordinator.db.worker_events.count_documents({\n        \"created_at\": {\"$gte\": datetime.utcnow() - timedelta(hours=1)}\n    })\n\n    return {\n        \"tasks\": dict(tasks_by_status),\n        \"workers\": dict(workers_by_status),\n        \"recent_events\": recent_events\n    }\n</code></pre>"},{"location":"guide/coordinators/#best-practices","title":"Best Practices","text":""},{"location":"guide/coordinators/#configuration-management","title":"Configuration Management","text":"<ol> <li> <p>Use environment-specific configs:    <pre><code>config = CoordinatorConfig.load(f\"configs/coordinator.{env}.json\")\n</code></pre></p> </li> <li> <p>Tune timing parameters based on your workload:</p> </li> <li>Short <code>scheduler_tick_sec</code> for low-latency</li> <li> <p>Longer <code>heartbeat_soft_sec</code> for stable workloads</p> </li> <li> <p>Set appropriate worker types to match your pipeline needs</p> </li> </ol>"},{"location":"guide/coordinators/#task-design","title":"Task Design","text":"<ol> <li>Keep DAGs focused: Don't create overly complex graphs</li> <li>Use streaming for large datasets</li> <li>Implement proper retry policies for reliability</li> <li>Add monitoring for long-running tasks</li> </ol>"},{"location":"guide/coordinators/#error-handling_1","title":"Error Handling","text":"<ol> <li>Classify errors properly in handlers</li> <li>Set reasonable retry limits to avoid infinite loops</li> <li>Monitor failure patterns to identify systemic issues</li> <li>Implement circuit breakers for external dependencies</li> </ol>"},{"location":"guide/coordinators/#performance","title":"Performance","text":"<ol> <li>Scale coordinators horizontally for high throughput</li> <li>Tune Kafka settings for your message volume</li> <li>Monitor MongoDB performance under load</li> <li>Use indexing for large task collections</li> </ol>"},{"location":"guide/coordinators/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guide/coordinators/#common-issues","title":"Common Issues","text":"<p>Coordinator not starting: - Check Kafka connectivity - Verify MongoDB connection - Ensure topics are created</p> <p>Tasks not being scheduled: - Check worker availability - Verify DAG dependencies are correct - Look for failed discovery queries</p> <p>High memory usage: - Tune outbox retention settings - Clean up old task documents - Monitor worker event collection size</p> <p>Slow scheduling: - Reduce <code>scheduler_tick_sec</code> - Add MongoDB indexes - Optimize DAG complexity</p>"},{"location":"guide/coordinators/#next-steps","title":"Next Steps","text":"<ul> <li>Configure Workers to process your tasks</li> <li>Design Task Graphs for complex workflows</li> <li>Handle Errors gracefully</li> <li>Monitor Performance in production</li> </ul>"},{"location":"guide/error-handling/","title":"Error handling &amp; retries (WIP)","text":"<p>Transient vs permanent errors, retry policy, backoff, fencing. Placeholder.</p>"},{"location":"guide/graphs/","title":"Task Graphs (WIP)","text":"<p>Overview of DAG nodes, edges, fan-in modes, and scheduling rules. Placeholder.</p>"},{"location":"guide/workers/","title":"Workers","text":"<p>Workers are the execution engine of FlowKit, responsible for processing individual tasks and reporting results back to coordinators.</p>"},{"location":"guide/workers/#overview","title":"Overview","text":"<p>A Worker:</p> <ul> <li>Executes Tasks: Runs custom handler logic for specific task types</li> <li>Manages State: Maintains local state for reliability and recovery</li> <li>Communicates Progress: Reports status updates via Kafka</li> <li>Handles Cancellation: Responds to cancellation signals gracefully</li> <li>Supports Multiple Roles: Can handle different task types simultaneously</li> </ul>"},{"location":"guide/workers/#basic-setup","title":"Basic Setup","text":"<pre><code>from flowkit.worker import Worker, WorkerConfig\nfrom flowkit.worker.handlers.base import Handler\nfrom motor.motor_asyncio import AsyncIOMotorClient\n\nclass MyHandler(Handler):\n    async def iter_batches(self, input_data):\n        # Generate batches from input\n        yield Batch(batch_uid=\"batch_1\", payload={\"data\": \"example\"})\n\n    async def process_batch(self, batch, ctx):\n        # Process the batch\n        return BatchResult(success=True, metrics={\"processed\": 1})\n\nasync def setup_worker():\n    # Database connection\n    client = AsyncIOMotorClient(\"mongodb://localhost:27017\")\n    db = client.flowkit\n\n    # Configuration\n    config = WorkerConfig(\n        kafka_bootstrap=\"localhost:9092\",\n        roles=[\"processor\", \"analyzer\"]\n    )\n\n    # Create worker with handlers\n    worker = Worker(\n        db=db,\n        cfg=config,\n        handlers={\n            \"processor\": MyHandler(),\n            \"analyzer\": MyHandler()\n        }\n    )\n\n    await worker.start()\n    return worker\n</code></pre>"},{"location":"guide/workers/#configuration-options","title":"Configuration Options","text":""},{"location":"guide/workers/#core-settings","title":"Core Settings","text":"<pre><code>config = WorkerConfig(\n    # Kafka configuration\n    kafka_bootstrap=\"localhost:9092\",\n\n    # Worker identity\n    roles=[\"processor\", \"analyzer\"],    # Task types this worker handles\n    worker_id=None,                     # Auto-generated if None\n    worker_version=\"2.0.0\",            # Worker version for tracking\n\n    # Timing settings\n    lease_ttl_sec=60,                  # How long to hold task leases\n    hb_interval_sec=20,                # Heartbeat frequency\n    announce_interval_sec=60,          # Worker announcement frequency\n\n    # Performance tuning\n    dedup_cache_size=10000,           # Deduplication cache size\n    dedup_ttl_ms=3600_000,            # Dedup entry TTL\n    pull_poll_ms_default=300,         # Default polling interval\n    db_cancel_poll_ms=500,            # Cancellation check frequency\n)\n</code></pre>"},{"location":"guide/workers/#loading-from-file","title":"Loading from File","text":"<pre><code># Load from JSON file\nconfig = WorkerConfig.load(\"configs/worker.json\")\n\n# With overrides\nconfig = WorkerConfig.load(\n    \"configs/worker.json\",\n    overrides={\"roles\": [\"processor\"]}\n)\n</code></pre>"},{"location":"guide/workers/#handler-implementation","title":"Handler Implementation","text":""},{"location":"guide/workers/#handler-interface","title":"Handler Interface","text":"<p>All handlers must implement the <code>Handler</code> base class:</p> <pre><code>from flowkit.worker.handlers.base import Handler, Batch, BatchResult, RunContext\nfrom typing import AsyncIterator\n\nclass CustomHandler(Handler):\n\n    async def init(self, run_info: dict):\n        \"\"\"Initialize handler for a new task run.\"\"\"\n        self.task_id = run_info[\"task_id\"]\n        self.node_id = run_info[\"node_id\"]\n        # Setup resources, connections, etc.\n\n    async def iter_batches(self, input_data) -&gt; AsyncIterator[Batch]:\n        \"\"\"Generate batches from input data.\"\"\"\n        # Extract batch information from input_data\n        batch_size = input_data.get(\"input_inline\", {}).get(\"batch_size\", 10)\n\n        for i in range(0, 100, batch_size):\n            yield Batch(\n                batch_uid=f\"batch_{i}\",\n                payload={\"start\": i, \"end\": min(i + batch_size, 100)}\n            )\n\n    async def process_batch(self, batch: Batch, ctx: RunContext) -&gt; BatchResult:\n        \"\"\"Process a single batch.\"\"\"\n        start = batch.payload[\"start\"]\n        end = batch.payload[\"end\"]\n\n        # Check for cancellation\n        if ctx.cancel_flag.is_set():\n            raise asyncio.CancelledError()\n\n        # Do actual processing\n        result = await self.process_range(start, end)\n\n        # Save artifacts if needed\n        artifacts_ref = await ctx.artifacts_writer.upsert_partial(\n            batch.batch_uid,\n            {\"items_processed\": end - start}\n        )\n\n        return BatchResult(\n            success=True,\n            metrics={\"items_processed\": end - start},\n            artifacts_ref=artifacts_ref\n        )\n\n    async def finalize(self, ctx: RunContext) -&gt; FinalizationResult:\n        \"\"\"Clean up after all batches are processed.\"\"\"\n        # Final processing, cleanup, etc.\n        return FinalizationResult(\n            metrics={\"total_items\": ctx.total_processed}\n        )\n\n    def classify_error(self, error: Exception) -&gt; tuple[str, bool]:\n        \"\"\"Classify errors as permanent or transient.\"\"\"\n        if isinstance(error, ValueError):\n            return \"validation_error\", True  # Permanent\n        elif isinstance(error, ConnectionError):\n            return \"connection_error\", False  # Transient\n        else:\n            return \"unknown_error\", False\n</code></pre>"},{"location":"guide/workers/#batch-processing-patterns","title":"Batch Processing Patterns","text":""},{"location":"guide/workers/#simple-iterator","title":"Simple Iterator","text":"<pre><code>async def iter_batches(self, input_data) -&gt; AsyncIterator[Batch]:\n    items = input_data.get(\"input_inline\", {}).get(\"items\", [])\n\n    for i, item in enumerate(items):\n        yield Batch(\n            batch_uid=f\"item_{i}\",\n            payload={\"item\": item, \"index\": i}\n        )\n</code></pre>"},{"location":"guide/workers/#database-cursor","title":"Database Cursor","text":"<pre><code>async def iter_batches(self, input_data) -&gt; AsyncIterator[Batch]:\n    batch_size = input_data.get(\"input_inline\", {}).get(\"batch_size\", 100)\n\n    async for batch in self.db.collection.find().batch_size(batch_size):\n        yield Batch(\n            batch_uid=f\"db_batch_{batch[0]['_id']}\",\n            payload={\"records\": batch}\n        )\n</code></pre>"},{"location":"guide/workers/#file-processing","title":"File Processing","text":"<pre><code>async def iter_batches(self, input_data) -&gt; AsyncIterator[Batch]:\n    file_path = input_data.get(\"input_inline\", {}).get(\"file_path\")\n\n    with open(file_path, 'r') as f:\n        batch = []\n        for i, line in enumerate(f):\n            batch.append(line.strip())\n\n            if len(batch) &gt;= 1000:  # Process in chunks of 1000\n                yield Batch(\n                    batch_uid=f\"file_batch_{i//1000}\",\n                    payload={\"lines\": batch}\n                )\n                batch = []\n\n        # Final batch\n        if batch:\n            yield Batch(\n                batch_uid=f\"file_batch_final\",\n                payload={\"lines\": batch}\n            )\n</code></pre>"},{"location":"guide/workers/#input-adapters","title":"Input Adapters","text":"<p>Workers can use input adapters to pull data from previous stages:</p>"},{"location":"guide/workers/#pull-from-artifacts","title":"Pull from Artifacts","text":"<pre><code># In task definition\n\"io\": {\n    \"input_inline\": {\n        \"input_adapter\": \"pull.from_artifacts\",\n        \"input_args\": {\n            \"from_nodes\": [\"upstream_node\"],\n            \"poll_ms\": 500,\n            \"eof_on_task_done\": True\n        }\n    }\n}\n</code></pre>"},{"location":"guide/workers/#rechunk-data","title":"Rechunk Data","text":"<pre><code># Rechunk upstream data into different batch sizes\n\"io\": {\n    \"input_inline\": {\n        \"input_adapter\": \"pull.from_artifacts.rechunk:size\",\n        \"input_args\": {\n            \"from_nodes\": [\"upstream_node\"],\n            \"size\": 50,  # New batch size\n            \"meta_list_key\": \"items\"\n        }\n    }\n}\n</code></pre>"},{"location":"guide/workers/#state-management","title":"State Management","text":""},{"location":"guide/workers/#worker-state-architecture","title":"Worker State Architecture","text":"<p>\u95ee\u9898\u5206\u6790: \u5f53\u524d\u7684 worker_state \u6587\u4ef6\u7cfb\u7edf\u6709\u4ee5\u4e0b\u95ee\u9898\uff1a</p> <ol> <li>\u6587\u4ef6\u7cfb\u7edf\u4f9d\u8d56: \u9700\u8981\u521b\u5efa\u76ee\u5f55\uff0c\u53ef\u80fd\u5728\u5bb9\u5668\u73af\u5883\u4e2d\u6709\u6743\u9650\u95ee\u9898</li> <li>\u5355\u70b9\u6545\u969c: \u672c\u5730\u6587\u4ef6\u4e22\u5931\u4f1a\u5bfc\u81f4\u72b6\u6001\u4e22\u5931</li> <li>\u6269\u5c55\u6027\u9650\u5236: \u4e0d\u9002\u5408\u591a\u5b9e\u4f8b\u90e8\u7f72</li> </ol> <p>\u5efa\u8bae\u7684\u66f4\u597d\u67b6\u6784\u89e3\u51b3\u65b9\u6848:</p> <pre><code># \u65b0\u7684\u57fa\u4e8e\u6570\u636e\u5e93\u7684\u72b6\u6001\u7ba1\u7406\nclass DatabaseWorkerState:\n    \"\"\"Database-backed worker state for production reliability.\"\"\"\n\n    def __init__(self, db, worker_id: str):\n        self.db = db\n        self.worker_id = worker_id\n        self.collection = db.worker_states\n\n    async def read_active(self) -&gt; ActiveRun | None:\n        \"\"\"Read active task state from database.\"\"\"\n        doc = await self.collection.find_one({\"worker_id\": self.worker_id})\n        if doc and doc.get(\"active_run\"):\n            return ActiveRun(**doc[\"active_run\"])\n        return None\n\n    async def write_active(self, active_run: ActiveRun | None):\n        \"\"\"Write active task state to database.\"\"\"\n        await self.collection.update_one(\n            {\"worker_id\": self.worker_id},\n            {\n                \"$set\": {\n                    \"worker_id\": self.worker_id,\n                    \"active_run\": active_run.__dict__ if active_run else None,\n                    \"updated_at\": datetime.utcnow()\n                }\n            },\n            upsert=True\n        )\n\n    async def write_checkpoint(self, checkpoint: dict):\n        \"\"\"Write processing checkpoint.\"\"\"\n        await self.collection.update_one(\n            {\"worker_id\": self.worker_id},\n            {\"$set\": {\"active_run.checkpoint\": checkpoint}},\n            upsert=True\n        )\n</code></pre> <p>\u4f18\u52bf: - \u2705 \u65e0\u6587\u4ef6\u7cfb\u7edf\u4f9d\u8d56: \u5b8c\u5168\u57fa\u4e8e\u6570\u636e\u5e93 - \u2705 \u9ad8\u53ef\u7528: \u6570\u636e\u5e93\u96c6\u7fa4\u63d0\u4f9b\u5197\u4f59 - \u2705 \u6269\u5c55\u6027: \u652f\u6301\u591a\u5b9e\u4f8b\u90e8\u7f72 - \u2705 \u76d1\u63a7\u53cb\u597d: \u53ef\u67e5\u8be2\u6240\u6709 worker \u72b6\u6001 - \u2705 \u4e8b\u52a1\u6027: \u539f\u5b50\u6027\u72b6\u6001\u66f4\u65b0</p>"},{"location":"guide/workers/#state-recovery","title":"State Recovery","text":"<p>Workers automatically recover from failures:</p> <pre><code># On worker startup, check for existing state\nactive_run = await worker.state.read_active()\nif active_run:\n    print(f\"Recovering task: {active_run.task_id}\")\n    # Worker will resume the task automatically\n</code></pre>"},{"location":"guide/workers/#checkpointing","title":"Checkpointing","text":"<pre><code>async def process_batch(self, batch: Batch, ctx: RunContext) -&gt; BatchResult:\n    # Save progress checkpoint\n    checkpoint = {\n        \"processed_items\": batch.payload[\"count\"],\n        \"last_id\": batch.payload[\"last_id\"]\n    }\n    await ctx.save_checkpoint(checkpoint)\n\n    # Continue processing...\n</code></pre>"},{"location":"guide/workers/#cancellation-handling","title":"Cancellation Handling","text":""},{"location":"guide/workers/#graceful-cancellation","title":"Graceful Cancellation","text":"<pre><code>async def process_batch(self, batch: Batch, ctx: RunContext) -&gt; BatchResult:\n    # Check cancellation at key points\n    if ctx.cancel_flag.is_set():\n        reason = ctx.cancel_meta.get(\"reason\", \"unknown\")\n        print(f\"Task cancelled: {reason}\")\n        raise asyncio.CancelledError()\n\n    # Long-running operation with periodic checks\n    for item in batch.payload[\"items\"]:\n        if ctx.cancel_flag.is_set():\n            raise asyncio.CancelledError()\n\n        await process_item(item)\n\n    return BatchResult(success=True)\n</code></pre>"},{"location":"guide/workers/#cancellation-sources","title":"Cancellation Sources","text":"<p>Cancellation can come from:</p> <ol> <li>Coordinator signals: Explicit cancellation commands</li> <li>Database flags: Task marked as cancelled in DB</li> <li>Timeout: Lease expiration</li> <li>Worker shutdown: Graceful shutdown process</li> </ol>"},{"location":"guide/workers/#monitoring-and-health","title":"Monitoring and Health","text":""},{"location":"guide/workers/#worker-announcements","title":"Worker Announcements","text":"<p>Workers periodically announce their presence:</p> <pre><code># Automatic announcements include:\n{\n    \"worker_id\": \"worker-abc123\",\n    \"type\": \"processor,analyzer\",  # Comma-separated roles\n    \"capabilities\": {\"roles\": [\"processor\", \"analyzer\"]},\n    \"version\": \"2.0.0\",\n    \"capacity\": {\"tasks\": 1},\n    \"status\": \"online\"\n}\n</code></pre>"},{"location":"guide/workers/#health-checks","title":"Health Checks","text":"<pre><code>async def check_worker_health(worker):\n    return {\n        \"worker_id\": worker.worker_id,\n        \"roles\": worker.cfg.roles,\n        \"busy\": worker._busy,\n        \"active_task\": worker.active.task_id if worker.active else None,\n        \"last_heartbeat\": worker.last_heartbeat_time\n    }\n</code></pre>"},{"location":"guide/workers/#metrics-collection","title":"Metrics Collection","text":"<pre><code>async def process_batch(self, batch: Batch, ctx: RunContext) -&gt; BatchResult:\n    start_time = time.time()\n\n    # Do processing...\n\n    processing_time = time.time() - start_time\n\n    return BatchResult(\n        success=True,\n        metrics={\n            \"items_processed\": len(batch.payload[\"items\"]),\n            \"processing_time_ms\": int(processing_time * 1000),\n            \"memory_used_mb\": get_memory_usage()\n        }\n    )\n</code></pre>"},{"location":"guide/workers/#scaling-workers","title":"Scaling Workers","text":""},{"location":"guide/workers/#horizontal-scaling","title":"Horizontal Scaling","text":"<pre><code># Run multiple workers with same roles\nworkers = []\nfor i in range(5):\n    worker = Worker(\n        db=db,\n        cfg=WorkerConfig(roles=[\"processor\"]),\n        handlers={\"processor\": ProcessorHandler()}\n    )\n    await worker.start()\n    workers.append(worker)\n\n# Tasks are automatically distributed across workers\n</code></pre>"},{"location":"guide/workers/#role-specialization","title":"Role Specialization","text":"<pre><code># Specialized workers for different task types\ncpu_worker = Worker(cfg=WorkerConfig(roles=[\"cpu_intensive\"]))\nio_worker = Worker(cfg=WorkerConfig(roles=[\"io_intensive\"]))\ngpu_worker = Worker(cfg=WorkerConfig(roles=[\"gpu_processing\"]))\n</code></pre>"},{"location":"guide/workers/#best-practices","title":"Best Practices","text":""},{"location":"guide/workers/#handler-design","title":"Handler Design","text":"<ol> <li>Keep handlers stateless where possible</li> <li>Implement proper error classification</li> <li>Use checkpointing for long-running tasks</li> <li>Handle cancellation gracefully</li> <li>Report meaningful metrics</li> </ol>"},{"location":"guide/workers/#performance","title":"Performance","text":"<ol> <li>Tune batch sizes for your workload</li> <li>Use async I/O for external calls</li> <li>Implement connection pooling for databases</li> <li>Monitor memory usage in long-running tasks</li> </ol>"},{"location":"guide/workers/#reliability","title":"Reliability","text":"<ol> <li>Implement retry logic for transient failures</li> <li>Use idempotent operations where possible</li> <li>Save state frequently for recovery</li> <li>Test failure scenarios thoroughly</li> </ol>"},{"location":"guide/workers/#resource-management","title":"Resource Management","text":"<pre><code>class DatabaseHandler(Handler):\n    async def init(self, run_info):\n        # Create connection pool\n        self.pool = await create_connection_pool()\n\n    async def cleanup(self):\n        # Clean up resources\n        await self.pool.close()\n\n    async def process_batch(self, batch, ctx):\n        async with self.pool.acquire() as conn:\n            # Use connection for processing\n            pass\n</code></pre>"},{"location":"guide/workers/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guide/workers/#common-issues","title":"Common Issues","text":"<p>Worker not receiving tasks: - Check Kafka topic subscription - Verify worker roles match task types - Check for consumer group conflicts</p> <p>Tasks timing out: - Increase <code>lease_ttl_sec</code> - Check heartbeat frequency - Monitor processing times</p> <p>Memory leaks: - Implement proper cleanup in handlers - Monitor batch processing memory usage - Use memory profiling tools</p> <p>State corruption: - Check database connectivity - Verify state serialization/deserialization - Implement state validation</p>"},{"location":"guide/workers/#next-steps","title":"Next Steps","text":"<ul> <li>Design Task Graphs for your workflows</li> <li>Configure Error Handling policies</li> <li>Monitor Performance in production</li> <li>Test Workers thoroughly</li> </ul>"},{"location":"reference/bus/","title":"Bus API Reference","text":""},{"location":"reference/bus/#kafka-bus","title":"Kafka Bus","text":""},{"location":"reference/bus/#flowkit.bus.kafka","title":"flowkit.bus.kafka","text":""},{"location":"reference/bus/#flowkit.bus.kafka-classes","title":"Classes","text":""},{"location":"reference/bus/#flowkit.bus.kafka.KafkaBus","title":"KafkaBus","text":"<pre><code>KafkaBus(cfg: CoordinatorConfig)\n</code></pre> <p>Thin wrapper around AIOKafka with a minimal reply correlator (by corr_id).</p> Source code in <code>src/flowkit/bus/kafka.py</code> <pre><code>def __init__(self, cfg: CoordinatorConfig) -&gt; None:\n    self.cfg = cfg\n    self._producer: AIOKafkaProducer | None = None\n    self._consumers: list[AIOKafkaConsumer] = []\n    self._replies: dict[str, list[Envelope]] = {}\n    self._reply_events: dict[str, asyncio.Event] = {}\n    self.bootstrap = cfg.kafka_bootstrap\n</code></pre>"},{"location":"reference/coordinator/","title":"Coordinator API Reference","text":""},{"location":"reference/coordinator/#coordinator-class","title":"Coordinator Class","text":""},{"location":"reference/coordinator/#flowkit.coordinator.runner.Coordinator","title":"flowkit.coordinator.runner.Coordinator","text":"<pre><code>Coordinator(*, db, cfg: CoordinatorConfig | None = None, worker_types: list[str] | None = None, clock: Clock | None = None, adapters: dict[str, Any] | None = None)\n</code></pre> <p>Orchestrates DAG execution across workers via Kafka topics. <code>db</code> is injected (e.g., Motor client). <code>clock</code> is injectable for tests.</p> Source code in <code>src/flowkit/coordinator/runner.py</code> <pre><code>def __init__(\n    self,\n    *,\n    db,\n    cfg: CoordinatorConfig | None = None,\n    worker_types: list[str] | None = None,\n    clock: Clock | None = None,\n    adapters: dict[str, Any] | None = None,\n) -&gt; None:\n    self.db = db\n    self.cfg = copy.deepcopy(cfg) if cfg is not None else CoordinatorConfig.load()\n    if worker_types:\n        self.cfg.worker_types = list(worker_types)\n\n    self.clock: Clock = clock or SystemClock()\n    self.bus = KafkaBus(self.cfg)\n    self.outbox = OutboxDispatcher(db=db, bus=self.bus, cfg=self.cfg, clock=self.clock)\n    self.adapters = adapters or dict(default_adapters(db=db, clock=self.clock))\n\n    self._tasks: set[asyncio.Task] = set()\n    self._running = False\n\n    self._announce_consumer: AIOKafkaConsumer | None = None\n    self._status_consumers: dict[str, AIOKafkaConsumer] = {}\n    self._query_reply_consumer: AIOKafkaConsumer | None = None\n\n    self._gid = f\"coord.{uuid.uuid4().hex[:6]}\"\n\n    # logging\n    self.log = get_logger(\"coordinator\")\n    bind_context(role=\"coordinator\", gid=self._gid)\n    self.log.debug(\"coordinator.init\", event=\"coord.init\")\n</code></pre>"},{"location":"reference/coordinator/#flowkit.coordinator.runner.Coordinator-functions","title":"Functions","text":""},{"location":"reference/coordinator/#flowkit.coordinator.runner.Coordinator.start","title":"start  <code>async</code>","text":"<pre><code>start() -&gt; None\n</code></pre> Source code in <code>src/flowkit/coordinator/runner.py</code> <pre><code>async def start(self) -&gt; None:\n    # Helpful config print (silent by default unless tests enable stdout)\n    cfg_dump: Any\n    if hasattr(self.cfg, \"model_dump\"):  # pydantic v2\n        cfg_dump = self.cfg.model_dump()\n    elif hasattr(self.cfg, \"dict\"):  # pydantic v1\n        cfg_dump = self.cfg.dict()\n    else:\n        cfg_dump = getattr(self.cfg, \"__dict__\", str(self.cfg))\n    self.log.debug(\"coordinator.start\", event=\"coord.start\", cfg=cfg_dump)\n\n    await self._ensure_indexes()\n    await self.bus.start()\n    await self._start_consumers()\n    await self.outbox.start()\n    self._running = True\n    self._spawn(self._scheduler_loop(), name=\"scheduler\")\n    self._spawn(self._heartbeat_monitor(), name=\"hb-monitor\")\n    self._spawn(self._finalizer_loop(), name=\"finalizer\")\n    self._spawn(self._resume_inflight(), name=\"resume-inflight\")\n    self.log.debug(\"coordinator.started\", event=\"coord.started\", gid=self._gid)\n</code></pre>"},{"location":"reference/coordinator/#flowkit.coordinator.runner.Coordinator.stop","title":"stop  <code>async</code>","text":"<pre><code>stop() -&gt; None\n</code></pre> Source code in <code>src/flowkit/coordinator/runner.py</code> <pre><code>async def stop(self) -&gt; None:\n    self._running = False\n    for t in list(self._tasks):\n        t.cancel()\n    self._tasks.clear()\n    with swallow(\n        logger=self.log, code=\"outbox.stop\", msg=\"outbox stop failed\", level=logging.ERROR, expected=False\n    ):\n        await self.outbox.stop()\n    with swallow(logger=self.log, code=\"bus.stop\", msg=\"bus stop failed\", level=logging.ERROR, expected=False):\n        await self.bus.stop()\n    self.log.debug(\"coordinator.stopped\", event=\"coord.stopped\")\n</code></pre>"},{"location":"reference/coordinator/#flowkit.coordinator.runner.Coordinator.create_task","title":"create_task  <code>async</code>","text":"<pre><code>create_task(*, params: dict[str, Any], graph: dict[str, Any]) -&gt; str\n</code></pre> Source code in <code>src/flowkit/coordinator/runner.py</code> <pre><code>async def create_task(self, *, params: dict[str, Any], graph: dict[str, Any]) -&gt; str:\n    task_id = str(uuid.uuid4())\n    graph.setdefault(\"nodes\", [])\n    graph.setdefault(\"edges\", [])\n    graph.setdefault(\"edges_ex\", [])\n    doc = TaskDoc(\n        id=task_id,\n        pipeline_id=task_id,\n        status=RunState.queued,\n        params=params,\n        graph=graph,\n        status_history=[{\"from\": None, \"to\": RunState.queued, \"at\": self.clock.now_dt()}],\n        started_at=self.clock.now_dt().isoformat(),\n        last_event_recv_ms=self.clock.now_ms(),\n    ).model_dump(mode=\"json\")\n    await self.db.tasks.insert_one(doc)\n    self.log.debug(\"task.created\", event=\"coord.task.created\", task_id=task_id)\n    return task_id\n</code></pre>"},{"location":"reference/coordinator/#adapters","title":"Adapters","text":""},{"location":"reference/coordinator/#flowkit.coordinator.adapters","title":"flowkit.coordinator.adapters","text":""},{"location":"reference/coordinator/#flowkit.coordinator.adapters-classes","title":"Classes","text":""},{"location":"reference/coordinator/#flowkit.coordinator.adapters.CoordinatorAdapters","title":"CoordinatorAdapters","text":"<pre><code>CoordinatorAdapters(*, db, clock: Clock | None = None)\n</code></pre> <p>Generic functions the coordinator can call (in coordinator_fn nodes). They operate via injected <code>db</code>. Keep side-effects idempotent.</p> Source code in <code>src/flowkit/coordinator/adapters.py</code> <pre><code>def __init__(self, *, db, clock: Clock | None = None) -&gt; None:\n    self.db = db\n    self.clock = clock or SystemClock()\n</code></pre>"},{"location":"reference/core/","title":"Core API Reference","text":""},{"location":"reference/core/#configuration-classes","title":"Configuration Classes","text":""},{"location":"reference/core/#flowkit.core.config.CoordinatorConfig","title":"flowkit.core.config.CoordinatorConfig  <code>dataclass</code>","text":"<pre><code>CoordinatorConfig(kafka_bootstrap: str = 'kafka:9092', worker_types: list[str] = (lambda: ['indexer', 'enricher', 'grouper', 'analyzer'])(), topic_cmd_fmt: str = 'cmd.{type}.v1', topic_status_fmt: str = 'status.{type}.v1', topic_worker_announce: str = 'workers.announce.v1', topic_query: str = 'query.tasks.v1', topic_reply: str = 'reply.tasks.v1', topic_signals: str = 'signals.v1', heartbeat_soft_sec: int = 300, heartbeat_hard_sec: int = 3600, lease_ttl_sec: int = 45, discovery_window_sec: int = 8, cancel_grace_sec: int = 30, scheduler_tick_sec: float = 1.0, finalizer_tick_sec: float = 5.0, hb_monitor_tick_sec: float = 10.0, outbox_dispatch_tick_sec: float = 0.25, outbox_max_retry: int = 12, outbox_backoff_min_ms: int = 250, outbox_backoff_max_ms: int = 60000, hb_soft_ms: int = 0, hb_hard_ms: int = 0, lease_ttl_ms: int = 0, discovery_window_ms: int = 0, cancel_grace_ms: int = 0, scheduler_tick_ms: int = 0, finalizer_tick_ms: int = 0, hb_monitor_tick_ms: int = 0, outbox_dispatch_tick_ms: int = 0)\n</code></pre>"},{"location":"reference/core/#flowkit.core.config.WorkerConfig","title":"flowkit.core.config.WorkerConfig  <code>dataclass</code>","text":"<pre><code>WorkerConfig(kafka_bootstrap: str = 'kafka:9092', topic_cmd_fmt: str = 'cmd.{type}.v1', topic_status_fmt: str = 'status.{type}.v1', topic_worker_announce: str = 'workers.announce.v1', topic_query: str = 'query.tasks.v1', topic_reply: str = 'reply.tasks.v1', topic_signals: str = 'signals.v1', roles: list[str] = (lambda: ['echo'])(), worker_id: str | None = None, worker_version: str = '2.0.0', lease_ttl_sec: int = 60, hb_interval_sec: int = 20, announce_interval_sec: int = 60, dedup_cache_size: int = 10000, dedup_ttl_ms: int = 3600000, pull_poll_ms_default: int = 300, pull_empty_backoff_ms_max: int = 4000, db_cancel_poll_ms: int = 500, lease_ttl_ms: int = 60000, hb_interval_ms: int = 20000, announce_interval_ms: int = 60000)\n</code></pre>"},{"location":"reference/core/#time-and-clock","title":"Time and Clock","text":""},{"location":"reference/core/#flowkit.core.time","title":"flowkit.core.time","text":""},{"location":"reference/core/#flowkit.core.time-classes","title":"Classes","text":""},{"location":"reference/core/#flowkit.core.time.SystemClock","title":"SystemClock","text":"<p>Default production/test clock.</p>"},{"location":"reference/core/#flowkit.core.time.ManualClock","title":"ManualClock","text":"<pre><code>ManualClock(start_ms: int = 0)\n</code></pre> <p>               Bases: <code>SystemClock</code></p> <p>Simple controllable clock for tests. - Wall time is advanced manually (affects persistence fields). - Monotonic time mirrors wall unless overridden.</p> Source code in <code>src/flowkit/core/time.py</code> <pre><code>def __init__(self, start_ms: int = 0) -&gt; None:\n    self._wall = start_ms\n    self._mono = start_ms\n</code></pre>"},{"location":"reference/core/#utilities","title":"Utilities","text":""},{"location":"reference/core/#flowkit.core.utils","title":"flowkit.core.utils","text":""},{"location":"reference/protocol/","title":"Protocol API Reference","text":""},{"location":"reference/protocol/#message-types-and-enums","title":"Message Types and Enums","text":""},{"location":"reference/protocol/#flowkit.protocol.messages","title":"flowkit.protocol.messages","text":""},{"location":"reference/testing/","title":"Test Suite Reference","text":"<p>This page is auto-generated from pytest docstrings at build time. Do not edit manually. See <code>docs/_scripts/gen_tests_doc.py</code>.</p> <p>Modules: 10 \u00a0\u00a0 Tests: 58</p>"},{"location":"reference/testing/#how-it-works","title":"How it works","text":"<p>The generator scans <code>tests/</code> for <code>test_*.py</code> modules, reads docstrings of modules, test classes, and test functions (including <code>async def</code>), and renders them here. The first sentence becomes a short summary in the index; the full docstring appears in Details.</p>"},{"location":"reference/testing/#writing-good-test-docstrings","title":"Writing good test docstrings","text":"<pre><code>async def test_outbox_retry_backoff():\n    \"\"\"\n    First two sends fail \u2192 item moves to *retry* with exponential backoff;\n    on the 3rd attempt the item becomes *sent*.\n    \"\"\"\n</code></pre>"},{"location":"reference/testing/#modules","title":"Modules","text":"<code>tests/test_artifacts_data.py</code> 2 tests (marks: asyncio) <ul> <li>test_merge_generic_creates_complete_artifact \u2014 coordinator_fn: merge.generic combines results of nodes 'a' and 'b'.</li> <li>test_partial_shards_and_stream_counts \u2014 Source w1 (indexer) emits batches with batch_uid \u2192 worker creates partial artifacts.</li> </ul> <code>tests/test_cancel_and_restart.py</code> 5 tests (marks: asyncio) <ul> <li>test_cancel_before_any_start_keeps_all_nodes_idle \u2014 Cancel the task before any node can start: no node must enter 'running'.</li> <li>test_cancel_on_deferred_prevents_retry \u2014 Node 'flaky' fails on first attempt \u2192 becomes deferred with backoff.</li> <li>test_cascade_cancel_prevents_downstream \u2014 Cancel the task while upstream runs: downstream must not start.</li> <li>test_restart_higher_epoch_ignores_old_batch_ok \u2014 After a node restarts with a higher epoch, the coordinator must ignore a stale BATCH_OK from a lower epoch.</li> <li>test_restart_higher_epoch_ignores_old_events \u2014 After accepting epoch&gt;=1, re-inject an old event (epoch=0).</li> </ul> <code>tests/test_chaos_models.py</code> 4 tests (marks: asyncio) End-to-end streaming smoke tests under chaos. <ul> <li>test_chaos_coordinator_restart \u2014 Restart the Coordinator while the task is running.</li> <li>test_chaos_delays_and_duplications \u2014 Chaos mode: small broker/consumer jitter + message duplications (no drops).</li> <li>test_chaos_worker_restart_mid_stream \u2014 Restart the 'enricher' worker in the middle of the stream.</li> <li>test_e2e_streaming_with_kafka_chaos \u2014 With chaos enabled (broker/consumer jitter and message duplications, no drops), the full pipeline should still complete and produce artifacts at indexer/enricher/ocr stages.</li> </ul> <code>tests/test_concurrency_limits.py</code> 4 tests (marks: asyncio) <ul> <li>test_concurrent_tasks_respect_global_limit \u2014 (no docstring)</li> <li>test_max_global_running_limit \u2014 (no docstring)</li> <li>test_max_type_concurrency_limits \u2014 (no docstring)</li> <li>test_multi_workers_same_type_rr_distribution \u2014 (no docstring)</li> </ul> <code>tests/test_lease_heartbeat_resume.py</code> 11 tests (marks: asyncio, cfg) Heartbeat / grace-window / resume tests. <ul> <li>test_deferred_retry_ignores_grace_gate \u2014 A DEFERRED retry must not be throttled by discovery grace window.</li> <li>test_grace_gate_blocks_then_allows_after_window \u2014 Grace window should delay start initially and allow it after the window elapses.</li> <li>test_heartbeat_hard_marks_task_failed \u2014 If hard heartbeat window is exceeded, the task should be marked FAILED.</li> <li>test_heartbeat_soft_deferred_then_recovers \u2014 With a short soft heartbeat window the task becomes DEFERRED, then recovers and finishes.</li> <li>test_heartbeat_tolerates_clock_skew \u2014 Worker clock skew should not cause a hard timeout; lease deadlines must be non-decreasing.</li> <li>test_heartbeat_updates_lease_deadline_simple \u2014 Heartbeats should move the lease.deadline_ts_ms forward while the node runs.</li> <li>test_lease_expiry_cascades_cancel \u2014 Permanent fail upstream should cascade-cancel downstream nodes.</li> <li>test_no_task_resumed_on_worker_restart \u2014 On a cold worker restart there must be no TASK_RESUMED event emitted by the worker.</li> <li>test_resume_inflight_worker_restarts_with_local_state \u2014 Restart with the same worker_id: coordinator should adopt inflight work without new epoch.</li> <li>test_task_discover_complete_artifacts_skips_node_start \u2014 If artifacts are 'complete' during discovery, node should auto-finish without starting its handler.</li> <li>test_worker_restart_with_new_id_bumps_epoch \u2014 Restart with a new worker_id must bump attempt_epoch; stale heartbeats from the old epoch are ignored.</li> </ul> <code>tests/test_outbox_delivery.py</code> 6 tests (marks: asyncio, cfg, use_outbox) <ul> <li>test_outbox_backoff_caps_with_jitter \u2014 Exponential backoff is capped by max and jitter stays within a reasonable window.</li> <li>test_outbox_crash_between_send_and_mark_sent \u2014 Send succeeds, coordinator crashes before mark(sent) \u2192 after restart the outbox may resend, but there is only ONE effective delivery for (topic,key,dedup_id).</li> <li>test_outbox_dedup_survives_restart \u2014 Deduplication by (topic,key,dedup_id) persists across coordinator restart: re-enqueue of the same envelope after restart does not create a second outbox row or send.</li> <li>test_outbox_dlq_after_max_retries \u2014 After reaching max retries, the outbox row is moved to a terminal state with attempts &gt;= max.</li> <li>test_outbox_exactly_once_fp_uniqueness \u2014 Two enqueues with identical (topic,key,dedup_id) \u2192 single outbox row and exactly one real send.</li> <li>test_outbox_retry_backoff \u2014 First two _raw_send calls fail \u2192 outbox goes to 'retry' with exponential backoff (with jitter), then on the 3rd attempt becomes 'sent'.</li> </ul> <code>tests/test_pipeline_smoke.py</code> 1 test (marks: asyncio) End-to-end streaming smoke test. <ul> <li>test_e2e_streaming_with_kafka_sim \u2014 Full pipeline should complete and produce artifacts at indexer/enricher/ocr stages.</li> </ul> <code>tests/test_reliability_failures.py</code> 7 tests (marks: asyncio, cfg) Tests around source-like roles: idempotent metrics, retries, fencing, coordinator restart adoption, cascade cancel, and heartbeat/lease updates. <ul> <li>test_coordinator_restart_adopts_inflight_without_new_epoch \u2014 When the coordinator restarts, it should adopt in-flight work without incrementing the worker's attempt_epoch unnecessarily (i.e., source keeps epoch=1).</li> <li>test_explicit_cascade_cancel_moves_node_to_deferred \u2014 Explicit cascade cancel should move a running node to a cancelling/deferred/queued state within the configured cancel_grace window.</li> <li>test_heartbeat_updates_lease_deadline \u2014 Heartbeats from a worker must extend the lease deadline in the task document.</li> <li>test_idempotent_metrics_on_duplicate_events \u2014 Duplicated STATUS events (BATCH_OK/TASK_DONE) must not double-count metrics.</li> <li>test_permanent_fail_cascades_cancel_and_task_failed \u2014 Permanent failure in an upstream node should cause the task to fail, while dependents get cancelled/deferred/queued depending on race windows.</li> <li>test_status_fencing_ignores_stale_epoch \u2014 Status fencing must ignore events from a stale attempt_epoch.</li> <li>test_transient_failure_deferred_then_retry \u2014 A transient error should defer the node and succeed on retry according to retry_policy (max&gt;=2).</li> </ul> <code>tests/test_scheduler_fanin_routing.py</code> 7 tests (marks: asyncio, xfail) Fan-in behavior (ANY/ALL/COUNT:N) and coordinator_fn merge smoke tests. <ul> <li>test_coordinator_fn_merge_without_worker \u2014 coordinator_fn node should run without a worker and produce artifacts that a downstream analyzer can consume via pull.from_artifacts.</li> <li>test_edges_vs_routing_priority \u2014 If explicit graph edges are present and a node also has routing.on_success, edges should take precedence (routing target should not run).</li> <li>test_fanin_all_waits_all_parents \u2014 Fan-in ALL: without start_when hint, downstream should only start after both parents are finished.</li> <li>test_fanin_any_starts_early \u2014 Fan-in ANY: downstream should start as soon as at least one parent streams (start_when='first_batch'), even if other parents are not yet finished.</li> <li>test_fanin_count_n \u2014 Fan-in COUNT:N placeholder: downstream should start when at least N parents are ready.</li> <li>test_fanout_one_upstream_two_downstreams_mixed_start_when \u2014 One upstream \u2192 two downstreams: A has start_when=first_batch (starts early), B has no start_when (waits for completion).</li> <li>test_routing_on_failure_triggers_remediator_only \u2014 On upstream TASK_FAILED(permanent=True), only the 'on_failure' remediator should run; 'on_success' must not.</li> </ul> <code>tests/test_streaming_sync.py</code> 11 tests (marks: asyncio) Tests for streaming/async fan-in behavior and metric accounting. <ul> <li>test_after_upstream_complete_delays_start \u2014 Without start_when, the downstream must not start until the upstream is fully finished.</li> <li>test_metrics_cross_talk_guard \u2014 Two concurrent tasks (same node_ids across different task_ids) do not interfere: final 'count' values are isolated per task.</li> <li>test_metrics_dedup_persists_across_coord_restart \u2014 Metric deduplication survives a coordinator restart: duplicates of STATUS (BATCH_OK/TASK_DONE) sent during the restart must not double-count.</li> <li>test_metrics_idempotent_on_duplicate_status_events \u2014 Duplicate STATUS events (BATCH_OK / TASK_DONE) must not double-increment aggregated metrics.</li> <li>test_metrics_isolation_between_tasks \u2014 Two back-to-back tasks must keep metric aggregation isolated per task document.</li> <li>test_metrics_multistream_exact_sum \u2014 Three upstreams -&gt; one downstream: analyzer's aggregated 'count' must equal the sum of all totals.</li> <li>test_metrics_partial_batches_exact_count \u2014 With a remainder in the last upstream batch, analyzer's 'count' must still exactly equal total.</li> <li>test_metrics_single_stream_exact_count \u2014 Single upstream -&gt; single downstream: analyzer's aggregated 'count' must equal the total items.</li> <li>test_multistream_fanin_stream_to_one_downstream \u2014 Multi-stream fan-in: three upstream indexers stream into one analyzer.</li> <li>test_start_when_first_batch_starts_early \u2014 Downstream should start while upstream is still running when `start_when=first_batch` is set.</li> <li>test_status_out_of_order_does_not_break_aggregation \u2014 BATCH_OK STATUS events arrive out of order \u2192 the aggregated metric remains correct.</li> </ul>"},{"location":"reference/testing/#teststest_artifacts_datapy","title":"tests/test_artifacts_data.py","text":""},{"location":"reference/testing/#tests","title":"Tests","text":"Test Summary Marks Location test_merge_generic_creates_complete_artifact coordinator_fn: merge.generic combines results of nodes 'a' and 'b'. asyncio <code>tests/test_artifacts_data.py:158</code> test_partial_shards_and_stream_counts Source w1 (indexer) emits batches with batch_uid \u2192 worker creates partial artifacts. asyncio <code>tests/test_artifacts_data.py:90</code>"},{"location":"reference/testing/#details","title":"Details","text":""},{"location":"reference/testing/#test_merge_generic_creates_complete_artifact","title":"test_merge_generic_creates_complete_artifact","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_artifacts_data.py:158</code></p> <p>coordinator_fn: merge.generic combines results of nodes 'a' and 'b'.</p> <p>We verify:   * the task finishes   * merge node 'm' has a 'complete' artifact   * nodes 'a' and 'b' have artifacts (partial/complete \u2014 does not matter)</p>"},{"location":"reference/testing/#test_partial_shards_and_stream_counts","title":"test_partial_shards_and_stream_counts","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_artifacts_data.py:90</code></p> <p>Source w1 (indexer) emits batches with batch_uid \u2192 worker creates partial artifacts. Completion marks a 'complete' artifact. Analyzer reads via rechunk and accumulates a counter.</p> <p>We verify:   * number of partial shards at w1 == ceil(total / batch_size)   * a 'complete' artifact exists at w1   * w2 has node.stats.count == total</p>"},{"location":"reference/testing/#teststest_cancel_and_restartpy","title":"tests/test_cancel_and_restart.py","text":""},{"location":"reference/testing/#tests_1","title":"Tests","text":"Test Summary Marks Location test_cancel_before_any_start_keeps_all_nodes_idle Cancel the task before any node can start: no node must enter 'running'. asyncio <code>tests/test_cancel_and_restart.py:231</code> test_cancel_on_deferred_prevents_retry Node 'flaky' fails on first attempt \u2192 becomes deferred with backoff. asyncio <code>tests/test_cancel_and_restart.py:295</code> test_cascade_cancel_prevents_downstream Cancel the task while upstream runs: downstream must not start. asyncio <code>tests/test_cancel_and_restart.py:97</code> test_restart_higher_epoch_ignores_old_batch_ok After a node restarts with a higher epoch, the coordinator must ignore a stale BATCH_OK from a lower epoch. asyncio <code>tests/test_cancel_and_restart.py:366</code> test_restart_higher_epoch_ignores_old_events After accepting epoch&gt;=1, re-inject an old event (epoch=0). asyncio <code>tests/test_cancel_and_restart.py:169</code>"},{"location":"reference/testing/#details_1","title":"Details","text":""},{"location":"reference/testing/#test_cancel_before_any_start_keeps_all_nodes_idle","title":"test_cancel_before_any_start_keeps_all_nodes_idle","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_cancel_and_restart.py:231</code></p> <p>Cancel the task before any node can start: no node must enter 'running'.</p>"},{"location":"reference/testing/#test_cancel_on_deferred_prevents_retry","title":"test_cancel_on_deferred_prevents_retry","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_cancel_and_restart.py:295</code></p> <p>Node 'flaky' fails on first attempt \u2192 becomes deferred with backoff. Cancel the task right after TASK_FAILED(epoch=1) and ensure no higher epoch is accepted.</p>"},{"location":"reference/testing/#test_cascade_cancel_prevents_downstream","title":"test_cascade_cancel_prevents_downstream","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_cancel_and_restart.py:97</code></p> <p>Cancel the task while upstream runs: downstream must not start.</p>"},{"location":"reference/testing/#test_restart_higher_epoch_ignores_old_batch_ok","title":"test_restart_higher_epoch_ignores_old_batch_ok","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_cancel_and_restart.py:366</code></p> <p>After a node restarts with a higher epoch, the coordinator must ignore a stale BATCH_OK from a lower epoch. Also ensure injected stale event doesn't create duplicate metrics.</p>"},{"location":"reference/testing/#test_restart_higher_epoch_ignores_old_events","title":"test_restart_higher_epoch_ignores_old_events","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_cancel_and_restart.py:169</code></p> <p>After accepting epoch&gt;=1, re-inject an old event (epoch=0). Coordinator must ignore it by fencing.</p>"},{"location":"reference/testing/#teststest_chaos_modelspy","title":"tests/test_chaos_models.py","text":"<p>End-to-end streaming smoke tests under chaos.</p> <p>Pipelines covered:   - w1(indexer) -&gt; w2(enricher) -&gt; w5(ocr) -&gt; w4(analyzer)                     ___________/           /                          _____ w3(merge) _/</p> <p>Checks:   - all nodes finish successfully (including the merge node in the extended graph)   - artifacts are produced for indexer/enricher/ocr (streaming stages)   - resiliency under worker and coordinator restarts</p>"},{"location":"reference/testing/#tests_2","title":"Tests","text":"Test Summary Marks Location test_chaos_coordinator_restart Restart the Coordinator while the task is running. asyncio <code>tests/test_chaos_models.py:344</code> test_chaos_delays_and_duplications Chaos mode: small broker/consumer jitter + message duplications (no drops). asyncio <code>tests/test_chaos_models.py:277</code> test_chaos_worker_restart_mid_stream Restart the 'enricher' worker in the middle of the stream. asyncio <code>tests/test_chaos_models.py:304</code> test_e2e_streaming_with_kafka_chaos With chaos enabled (broker/consumer jitter and message duplications, no drops), the full pipeline should still complete and produce artifacts at indexer/enricher/ocr stages. asyncio <code>tests/test_chaos_models.py:245</code>"},{"location":"reference/testing/#details_2","title":"Details","text":""},{"location":"reference/testing/#test_chaos_coordinator_restart","title":"test_chaos_coordinator_restart","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_chaos_models.py:344</code></p> <p>Restart the Coordinator while the task is running. Expect the pipeline to recover and finish.</p>"},{"location":"reference/testing/#test_chaos_delays_and_duplications","title":"test_chaos_delays_and_duplications","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_chaos_models.py:277</code></p> <p>Chaos mode: small broker/consumer jitter + message duplications (no drops). Expect the pipeline to finish and produce artifacts for w1/w2/w5.</p>"},{"location":"reference/testing/#test_chaos_worker_restart_mid_stream","title":"test_chaos_worker_restart_mid_stream","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_chaos_models.py:304</code></p> <p>Restart the 'enricher' worker in the middle of the stream. Expect the coordinator to fence and the pipeline to still finish.</p>"},{"location":"reference/testing/#test_e2e_streaming_with_kafka_chaos","title":"test_e2e_streaming_with_kafka_chaos","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_chaos_models.py:245</code></p> <p>With chaos enabled (broker/consumer jitter and message duplications, no drops), the full pipeline should still complete and produce artifacts at indexer/enricher/ocr stages.</p>"},{"location":"reference/testing/#teststest_concurrency_limitspy","title":"tests/test_concurrency_limits.py","text":""},{"location":"reference/testing/#tests_3","title":"Tests","text":"Test Summary Marks Location test_concurrent_tasks_respect_global_limit (no docstring) asyncio <code>tests/test_concurrency_limits.py:302</code> test_max_global_running_limit (no docstring) asyncio <code>tests/test_concurrency_limits.py:153</code> test_max_type_concurrency_limits (no docstring) asyncio <code>tests/test_concurrency_limits.py:196</code> test_multi_workers_same_type_rr_distribution (no docstring) asyncio <code>tests/test_concurrency_limits.py:252</code>"},{"location":"reference/testing/#details_3","title":"Details","text":""},{"location":"reference/testing/#test_concurrent_tasks_respect_global_limit","title":"test_concurrent_tasks_respect_global_limit","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_concurrency_limits.py:302</code></p> <p>No docstring.</p>"},{"location":"reference/testing/#test_max_global_running_limit","title":"test_max_global_running_limit","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_concurrency_limits.py:153</code></p> <p>No docstring.</p>"},{"location":"reference/testing/#test_max_type_concurrency_limits","title":"test_max_type_concurrency_limits","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_concurrency_limits.py:196</code></p> <p>No docstring.</p>"},{"location":"reference/testing/#test_multi_workers_same_type_rr_distribution","title":"test_multi_workers_same_type_rr_distribution","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_concurrency_limits.py:252</code></p> <p>No docstring.</p>"},{"location":"reference/testing/#teststest_lease_heartbeat_resumepy","title":"tests/test_lease_heartbeat_resume.py","text":"<p>Heartbeat / grace-window / resume tests.</p> <p>Covers: - Soft heartbeat -&gt; deferred -&gt; recovery - Hard heartbeat -&gt; task failed - Resume with local worker state (no new epoch) - Discover w/ complete artifacts -&gt; skip start - Grace-gate delay, and that deferred retry ignores the gate - Heartbeat extends lease deadline - Restart with a new worker_id bumps attempt_epoch and fences stale heartbeats</p>"},{"location":"reference/testing/#tests_4","title":"Tests","text":"Test Summary Marks Location test_deferred_retry_ignores_grace_gate A DEFERRED retry must not be throttled by discovery grace window. cfg, asyncio <code>tests/test_lease_heartbeat_resume.py:258</code> test_grace_gate_blocks_then_allows_after_window Grace window should delay start initially and allow it after the window elapses. cfg, asyncio <code>tests/test_lease_heartbeat_resume.py:209</code> test_heartbeat_hard_marks_task_failed If hard heartbeat window is exceeded, the task should be marked FAILED. cfg, asyncio <code>tests/test_lease_heartbeat_resume.py:77</code> test_heartbeat_soft_deferred_then_recovers With a short soft heartbeat window the task becomes DEFERRED, then recovers and finishes. cfg, asyncio <code>tests/test_lease_heartbeat_resume.py:39</code> test_heartbeat_tolerates_clock_skew Worker clock skew should not cause a hard timeout; lease deadlines must be non-decreasing. asyncio, cfg <code>tests/test_lease_heartbeat_resume.py:556</code> test_heartbeat_updates_lease_deadline_simple Heartbeats should move the lease.deadline_ts_ms forward while the node runs. cfg, asyncio <code>tests/test_lease_heartbeat_resume.py:348</code> test_lease_expiry_cascades_cancel Permanent fail upstream should cascade-cancel downstream nodes. asyncio, cfg <code>tests/test_lease_heartbeat_resume.py:625</code> test_no_task_resumed_on_worker_restart On a cold worker restart there must be no TASK_RESUMED event emitted by the worker. cfg, asyncio <code>tests/test_lease_heartbeat_resume.py:292</code> test_resume_inflight_worker_restarts_with_local_state Restart with the same worker_id: coordinator should adopt inflight work without new epoch. cfg, asyncio <code>tests/test_lease_heartbeat_resume.py:113</code> test_task_discover_complete_artifacts_skips_node_start If artifacts are 'complete' during discovery, node should auto-finish without starting its handler. asyncio <code>tests/test_lease_heartbeat_resume.py:173</code> test_worker_restart_with_new_id_bumps_epoch Restart with a new worker_id must bump attempt_epoch; stale heartbeats from the old epoch are ignored. asyncio, cfg <code>tests/test_lease_heartbeat_resume.py:394</code>"},{"location":"reference/testing/#details_4","title":"Details","text":""},{"location":"reference/testing/#test_deferred_retry_ignores_grace_gate","title":"test_deferred_retry_ignores_grace_gate","text":"<p>marks: <code>cfg, asyncio</code> \u2022 location: <code>tests/test_lease_heartbeat_resume.py:258</code></p> <p>A DEFERRED retry must not be throttled by discovery grace window.</p>"},{"location":"reference/testing/#test_grace_gate_blocks_then_allows_after_window","title":"test_grace_gate_blocks_then_allows_after_window","text":"<p>marks: <code>cfg, asyncio</code> \u2022 location: <code>tests/test_lease_heartbeat_resume.py:209</code></p> <p>Grace window should delay start initially and allow it after the window elapses.</p>"},{"location":"reference/testing/#test_heartbeat_hard_marks_task_failed","title":"test_heartbeat_hard_marks_task_failed","text":"<p>marks: <code>cfg, asyncio</code> \u2022 location: <code>tests/test_lease_heartbeat_resume.py:77</code></p> <p>If hard heartbeat window is exceeded, the task should be marked FAILED.</p>"},{"location":"reference/testing/#test_heartbeat_soft_deferred_then_recovers","title":"test_heartbeat_soft_deferred_then_recovers","text":"<p>marks: <code>cfg, asyncio</code> \u2022 location: <code>tests/test_lease_heartbeat_resume.py:39</code></p> <p>With a short soft heartbeat window the task becomes DEFERRED, then recovers and finishes.</p>"},{"location":"reference/testing/#test_heartbeat_tolerates_clock_skew","title":"test_heartbeat_tolerates_clock_skew","text":"<p>marks: <code>asyncio, cfg</code> \u2022 location: <code>tests/test_lease_heartbeat_resume.py:556</code></p> <p>Worker clock skew should not cause a hard timeout; lease deadlines must be non-decreasing.</p>"},{"location":"reference/testing/#test_heartbeat_updates_lease_deadline_simple","title":"test_heartbeat_updates_lease_deadline_simple","text":"<p>marks: <code>cfg, asyncio</code> \u2022 location: <code>tests/test_lease_heartbeat_resume.py:348</code></p> <p>Heartbeats should move the lease.deadline_ts_ms forward while the node runs.</p>"},{"location":"reference/testing/#test_lease_expiry_cascades_cancel","title":"test_lease_expiry_cascades_cancel","text":"<p>marks: <code>asyncio, cfg</code> \u2022 location: <code>tests/test_lease_heartbeat_resume.py:625</code></p> <p>Permanent fail upstream should cascade-cancel downstream nodes.</p>"},{"location":"reference/testing/#test_no_task_resumed_on_worker_restart","title":"test_no_task_resumed_on_worker_restart","text":"<p>marks: <code>cfg, asyncio</code> \u2022 location: <code>tests/test_lease_heartbeat_resume.py:292</code></p> <p>On a cold worker restart there must be no TASK_RESUMED event emitted by the worker.</p>"},{"location":"reference/testing/#test_resume_inflight_worker_restarts_with_local_state","title":"test_resume_inflight_worker_restarts_with_local_state","text":"<p>marks: <code>cfg, asyncio</code> \u2022 location: <code>tests/test_lease_heartbeat_resume.py:113</code></p> <p>Restart with the same worker_id: coordinator should adopt inflight work without new epoch.</p>"},{"location":"reference/testing/#test_task_discover_complete_artifacts_skips_node_start","title":"test_task_discover_complete_artifacts_skips_node_start","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_lease_heartbeat_resume.py:173</code></p> <p>If artifacts are 'complete' during discovery, node should auto-finish without starting its handler.</p>"},{"location":"reference/testing/#test_worker_restart_with_new_id_bumps_epoch","title":"test_worker_restart_with_new_id_bumps_epoch","text":"<p>marks: <code>asyncio, cfg</code> \u2022 location: <code>tests/test_lease_heartbeat_resume.py:394</code></p> <p>Restart with a new worker_id must bump attempt_epoch; stale heartbeats from the old epoch are ignored.</p>"},{"location":"reference/testing/#teststest_outbox_deliverypy","title":"tests/test_outbox_delivery.py","text":""},{"location":"reference/testing/#tests_5","title":"Tests","text":"Test Summary Marks Location test_outbox_backoff_caps_with_jitter Exponential backoff is capped by max and jitter stays within a reasonable window. asyncio, use_outbox, cfg <code>tests/test_outbox_delivery.py:366</code> test_outbox_crash_between_send_and_mark_sent Send succeeds, coordinator crashes before mark(sent) \u2192 after restart the outbox may resend, but there is only ONE effective delivery for (topic,key,dedup_id). asyncio, use_outbox <code>tests/test_outbox_delivery.py:214</code> test_outbox_dedup_survives_restart Deduplication by (topic,key,dedup_id) persists across coordinator restart: re-enqueue of the same envelope after restart does not create a second outbox row or send. asyncio, use_outbox <code>tests/test_outbox_delivery.py:305</code> test_outbox_dlq_after_max_retries After reaching max retries, the outbox row is moved to a terminal state with attempts &gt;= max. asyncio, use_outbox, cfg <code>tests/test_outbox_delivery.py:419</code> test_outbox_exactly_once_fp_uniqueness Two enqueues with identical (topic,key,dedup_id) \u2192 single outbox row and exactly one real send. asyncio <code>tests/test_outbox_delivery.py:138</code> test_outbox_retry_backoff First two _raw_send calls fail \u2192 outbox goes to 'retry' with exponential backoff (with jitter), then on the 3<sup>rd</sup> attempt becomes 'sent'. asyncio <code>tests/test_outbox_delivery.py:46</code>"},{"location":"reference/testing/#details_5","title":"Details","text":""},{"location":"reference/testing/#test_outbox_backoff_caps_with_jitter","title":"test_outbox_backoff_caps_with_jitter","text":"<p>marks: <code>asyncio, use_outbox, cfg</code> \u2022 location: <code>tests/test_outbox_delivery.py:366</code></p> <p>Exponential backoff is capped by max and jitter stays within a reasonable window.</p>"},{"location":"reference/testing/#test_outbox_crash_between_send_and_mark_sent","title":"test_outbox_crash_between_send_and_mark_sent","text":"<p>marks: <code>asyncio, use_outbox</code> \u2022 location: <code>tests/test_outbox_delivery.py:214</code></p> <p>Send succeeds, coordinator crashes before mark(sent) \u2192 after restart the outbox may resend, but there is only ONE effective delivery for (topic,key,dedup_id).</p>"},{"location":"reference/testing/#test_outbox_dedup_survives_restart","title":"test_outbox_dedup_survives_restart","text":"<p>marks: <code>asyncio, use_outbox</code> \u2022 location: <code>tests/test_outbox_delivery.py:305</code></p> <p>Deduplication by (topic,key,dedup_id) persists across coordinator restart: re-enqueue of the same envelope after restart does not create a second outbox row or send.</p>"},{"location":"reference/testing/#test_outbox_dlq_after_max_retries","title":"test_outbox_dlq_after_max_retries","text":"<p>marks: <code>asyncio, use_outbox, cfg</code> \u2022 location: <code>tests/test_outbox_delivery.py:419</code></p> <p>After reaching max retries, the outbox row is moved to a terminal state with attempts &gt;= max.</p>"},{"location":"reference/testing/#test_outbox_exactly_once_fp_uniqueness","title":"test_outbox_exactly_once_fp_uniqueness","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_outbox_delivery.py:138</code></p> <p>Two enqueues with identical (topic,key,dedup_id) \u2192 single outbox row and exactly one real send.</p>"},{"location":"reference/testing/#test_outbox_retry_backoff","title":"test_outbox_retry_backoff","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_outbox_delivery.py:46</code></p> <p>First two _raw_send calls fail \u2192 outbox goes to 'retry' with exponential backoff (with jitter), then on the 3<sup>rd</sup> attempt becomes 'sent'.</p>"},{"location":"reference/testing/#teststest_pipeline_smokepy","title":"tests/test_pipeline_smoke.py","text":"<p>End-to-end streaming smoke test.</p> <p>Pipeline:   w1(indexer) -&gt; w2(enricher) -&gt; w5(ocr) -&gt; w4(analyzer)                  ___________/           /                       _____ w3(merge) _/</p> <p>Checks:   - all nodes finish successfully   - artifacts are produced for indexer/enricher/ocr</p>"},{"location":"reference/testing/#tests_6","title":"Tests","text":"Test Summary Marks Location test_e2e_streaming_with_kafka_sim Full pipeline should complete and produce artifacts at indexer/enricher/ocr stages. asyncio <code>tests/test_pipeline_smoke.py:136</code>"},{"location":"reference/testing/#details_6","title":"Details","text":""},{"location":"reference/testing/#test_e2e_streaming_with_kafka_sim","title":"test_e2e_streaming_with_kafka_sim","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_pipeline_smoke.py:136</code></p> <p>Full pipeline should complete and produce artifacts at indexer/enricher/ocr stages.</p>"},{"location":"reference/testing/#teststest_reliability_failurespy","title":"tests/test_reliability_failures.py","text":"<p>Tests around source-like roles: idempotent metrics, retries, fencing, coordinator restart adoption, cascade cancel, and heartbeat/lease updates.</p> <p>Design: - All runtime knobs come via coord_cfg/worker_cfg fixtures (see conftest.py). - For one-off tweaks, use @pytest.mark.cfg(coord={...}, worker={...}). - Workers are started with in-memory DB and handlers built from helper builders.</p>"},{"location":"reference/testing/#tests_7","title":"Tests","text":"Test Summary Marks Location test_coordinator_restart_adopts_inflight_without_new_epoch When the coordinator restarts, it should adopt in-flight work without incrementing the worker's attempt_epoch unnecessarily (i.e., source keeps epoch=1). asyncio <code>tests/test_reliability_failures.py:218</code> test_explicit_cascade_cancel_moves_node_to_deferred Explicit cascade cancel should move a running node to a cancelling/deferred/queued state within the configured cancel_grace window. cfg, asyncio <code>tests/test_reliability_failures.py:273</code> test_heartbeat_updates_lease_deadline Heartbeats from a worker must extend the lease deadline in the task document. cfg, asyncio <code>tests/test_reliability_failures.py:322</code> test_idempotent_metrics_on_duplicate_events Duplicated STATUS events (BATCH_OK/TASK_DONE) must not double-count metrics. asyncio <code>tests/test_reliability_failures.py:36</code> test_permanent_fail_cascades_cancel_and_task_failed Permanent failure in an upstream node should cause the task to fail, while dependents get cancelled/deferred/queued depending on race windows. cfg, asyncio <code>tests/test_reliability_failures.py:125</code> test_status_fencing_ignores_stale_epoch Status fencing must ignore events from a stale attempt_epoch. asyncio <code>tests/test_reliability_failures.py:169</code> test_transient_failure_deferred_then_retry A transient error should defer the node and succeed on retry according to retry_policy (max&gt;=2). asyncio <code>tests/test_reliability_failures.py:83</code>"},{"location":"reference/testing/#details_7","title":"Details","text":""},{"location":"reference/testing/#test_coordinator_restart_adopts_inflight_without_new_epoch","title":"test_coordinator_restart_adopts_inflight_without_new_epoch","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_reliability_failures.py:218</code></p> <p>When the coordinator restarts, it should adopt in-flight work without incrementing the worker's attempt_epoch unnecessarily (i.e., source keeps epoch=1).</p>"},{"location":"reference/testing/#test_explicit_cascade_cancel_moves_node_to_deferred","title":"test_explicit_cascade_cancel_moves_node_to_deferred","text":"<p>marks: <code>cfg, asyncio</code> \u2022 location: <code>tests/test_reliability_failures.py:273</code></p> <p>Explicit cascade cancel should move a running node to a cancelling/deferred/queued state within the configured cancel_grace window.</p>"},{"location":"reference/testing/#test_heartbeat_updates_lease_deadline","title":"test_heartbeat_updates_lease_deadline","text":"<p>marks: <code>cfg, asyncio</code> \u2022 location: <code>tests/test_reliability_failures.py:322</code></p> <p>Heartbeats from a worker must extend the lease deadline in the task document.</p>"},{"location":"reference/testing/#test_idempotent_metrics_on_duplicate_events","title":"test_idempotent_metrics_on_duplicate_events","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_reliability_failures.py:36</code></p> <p>Duplicated STATUS events (BATCH_OK/TASK_DONE) must not double-count metrics. We duplicate STATUS envelopes at the Kafka level; aggregator must remain stable.</p>"},{"location":"reference/testing/#test_permanent_fail_cascades_cancel_and_task_failed","title":"test_permanent_fail_cascades_cancel_and_task_failed","text":"<p>marks: <code>cfg, asyncio</code> \u2022 location: <code>tests/test_reliability_failures.py:125</code></p> <p>Permanent failure in an upstream node should cause the task to fail, while dependents get cancelled/deferred/queued depending on race windows.</p>"},{"location":"reference/testing/#test_status_fencing_ignores_stale_epoch","title":"test_status_fencing_ignores_stale_epoch","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_reliability_failures.py:169</code></p> <p>Status fencing must ignore events from a stale attempt_epoch. We finish the task, then send a forged event with attempt_epoch=0; stats must remain unchanged.</p>"},{"location":"reference/testing/#test_transient_failure_deferred_then_retry","title":"test_transient_failure_deferred_then_retry","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_reliability_failures.py:83</code></p> <p>A transient error should defer the node and succeed on retry according to retry_policy (max&gt;=2).</p>"},{"location":"reference/testing/#teststest_scheduler_fanin_routingpy","title":"tests/test_scheduler_fanin_routing.py","text":"<p>Fan-in behavior (ANY/ALL/COUNT:N) and coordinator_fn merge smoke tests.</p> <p>Covers:   - ANY fan-in: downstream starts as soon as any parent streams a first batch.   - ALL fan-in: downstream starts only after all parents are done (no start_when).   - COUNT:N fan-in: xfail placeholder until the coordinator supports it.   - Edges vs routing priority: xfail placeholder for precedence logic.   - coordinator_fn merge: runs without a worker and feeds downstream via artifacts.</p>"},{"location":"reference/testing/#tests_8","title":"Tests","text":"Test Summary Marks Location test_coordinator_fn_merge_without_worker coordinator_fn node should run without a worker and produce artifacts that a downstream analyzer can consume via pull.from_artifacts. asyncio <code>tests/test_scheduler_fanin_routing.py:294</code> test_edges_vs_routing_priority If explicit graph edges are present and a node also has routing.on_success, edges should take precedence (routing target should not run). asyncio, xfail <code>tests/test_scheduler_fanin_routing.py:230</code> test_fanin_all_waits_all_parents Fan-in ALL: without start_when hint, downstream should only start after both parents are finished. asyncio <code>tests/test_scheduler_fanin_routing.py:127</code> test_fanin_any_starts_early Fan-in ANY: downstream should start as soon as at least one parent streams (start_when='first_batch'), even if other parents are not yet finished. asyncio <code>tests/test_scheduler_fanin_routing.py:69</code> test_fanin_count_n Fan-in COUNT:N placeholder: downstream should start when at least N parents are ready. asyncio, xfail <code>tests/test_scheduler_fanin_routing.py:176</code> test_fanout_one_upstream_two_downstreams_mixed_start_when One upstream \u2192 two downstreams: A has start_when=first_batch (starts early), B has no start_when (waits for completion). asyncio <code>tests/test_scheduler_fanin_routing.py:431</code> test_routing_on_failure_triggers_remediator_only On upstream TASK_FAILED(permanent=True), only the 'on_failure' remediator should run; 'on_success' must not. asyncio, xfail <code>tests/test_scheduler_fanin_routing.py:358</code>"},{"location":"reference/testing/#details_8","title":"Details","text":""},{"location":"reference/testing/#test_coordinator_fn_merge_without_worker","title":"test_coordinator_fn_merge_without_worker","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_scheduler_fanin_routing.py:294</code></p> <p>coordinator_fn node should run without a worker and produce artifacts that a downstream analyzer can consume via pull.from_artifacts.</p>"},{"location":"reference/testing/#test_edges_vs_routing_priority","title":"test_edges_vs_routing_priority","text":"<p>marks: <code>asyncio, xfail</code> \u2022 location: <code>tests/test_scheduler_fanin_routing.py:230</code></p> <p>If explicit graph edges are present and a node also has routing.on_success, edges should take precedence (routing target should not run).</p>"},{"location":"reference/testing/#test_fanin_all_waits_all_parents","title":"test_fanin_all_waits_all_parents","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_scheduler_fanin_routing.py:127</code></p> <p>Fan-in ALL: without start_when hint, downstream should only start after both parents are finished.</p>"},{"location":"reference/testing/#test_fanin_any_starts_early","title":"test_fanin_any_starts_early","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_scheduler_fanin_routing.py:69</code></p> <p>Fan-in ANY: downstream should start as soon as at least one parent streams (start_when='first_batch'), even if other parents are not yet finished. We run a single 'indexer' worker so upstream parents execute sequentially.</p>"},{"location":"reference/testing/#test_fanin_count_n","title":"test_fanin_count_n","text":"<p>marks: <code>asyncio, xfail</code> \u2022 location: <code>tests/test_scheduler_fanin_routing.py:176</code></p> <p>Fan-in COUNT:N placeholder: downstream should start when at least N parents are ready. Marked xfail until coordinator supports 'count:n'.</p>"},{"location":"reference/testing/#test_fanout_one_upstream_two_downstreams_mixed_start_when","title":"test_fanout_one_upstream_two_downstreams_mixed_start_when","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_scheduler_fanin_routing.py:431</code></p> <p>One upstream \u2192 two downstreams: A has start_when=first_batch (starts early), B has no start_when (waits for completion).</p>"},{"location":"reference/testing/#test_routing_on_failure_triggers_remediator_only","title":"test_routing_on_failure_triggers_remediator_only","text":"<p>marks: <code>asyncio, xfail</code> \u2022 location: <code>tests/test_scheduler_fanin_routing.py:358</code></p> <p>On upstream TASK_FAILED(permanent=True), only the 'on_failure' remediator should run; 'on_success' must not.</p>"},{"location":"reference/testing/#teststest_streaming_syncpy","title":"tests/test_streaming_sync.py","text":"<p>Tests for streaming/async fan-in behavior and metric accounting.</p> <p>This suite validates:   1) Early start of downstream when <code>start_when=first_batch</code>.   2) Delayed start of downstream when no <code>start_when</code> hint is provided.   3) Multi-stream fan-in to a single downstream consumer.   4) Exactness and isolation of metric aggregation.   5) Idempotency of metrics on duplicated STATUS events (BATCH_OK / TASK_DONE).</p> <p>All tests rely on in-memory Kafka and DB, configured via fixtures from <code>conftest.py</code>.</p>"},{"location":"reference/testing/#tests_9","title":"Tests","text":"Test Summary Marks Location test_after_upstream_complete_delays_start Without start_when, the downstream must not start until the upstream is fully finished. asyncio <code>tests/test_streaming_sync.py:134</code> test_metrics_cross_talk_guard Two concurrent tasks (same node_ids across different task_ids) do not interfere: final 'count' values are isolated per task. asyncio <code>tests/test_streaming_sync.py:644</code> test_metrics_dedup_persists_across_coord_restart Metric deduplication survives a coordinator restart: duplicates of STATUS (BATCH_OK/TASK_DONE) sent during the restart must not double-count. asyncio <code>tests/test_streaming_sync.py:567</code> test_metrics_idempotent_on_duplicate_status_events Duplicate STATUS events (BATCH_OK / TASK_DONE) must not double-increment aggregated metrics. asyncio <code>tests/test_streaming_sync.py:433</code> test_metrics_isolation_between_tasks Two back-to-back tasks must keep metric aggregation isolated per task document. asyncio <code>tests/test_streaming_sync.py:388</code> test_metrics_multistream_exact_sum Three upstreams -&gt; one downstream: analyzer's aggregated 'count' must equal the sum of all totals. asyncio <code>tests/test_streaming_sync.py:300</code> test_metrics_partial_batches_exact_count With a remainder in the last upstream batch, analyzer's 'count' must still exactly equal total. asyncio <code>tests/test_streaming_sync.py:347</code> test_metrics_single_stream_exact_count Single upstream -&gt; single downstream: analyzer's aggregated 'count' must equal the total items. asyncio <code>tests/test_streaming_sync.py:259</code> test_multistream_fanin_stream_to_one_downstream Multi-stream fan-in: three upstream indexers stream into one analyzer. asyncio <code>tests/test_streaming_sync.py:188</code> test_start_when_first_batch_starts_early Downstream should start while upstream is still running when <code>start_when=first_batch</code> is set. asyncio <code>tests/test_streaming_sync.py:76</code> test_status_out_of_order_does_not_break_aggregation BATCH_OK STATUS events arrive out of order \u2192 the aggregated metric remains correct. asyncio <code>tests/test_streaming_sync.py:494</code>"},{"location":"reference/testing/#details_9","title":"Details","text":""},{"location":"reference/testing/#test_after_upstream_complete_delays_start","title":"test_after_upstream_complete_delays_start","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_streaming_sync.py:134</code></p> <p>Without start_when, the downstream must not start until the upstream is fully finished. We first assert a negative hold window while the upstream is running, then assert the downstream starts and finishes after the upstream completes.</p>"},{"location":"reference/testing/#test_metrics_cross_talk_guard","title":"test_metrics_cross_talk_guard","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_streaming_sync.py:644</code></p> <p>Two concurrent tasks (same node_ids across different task_ids) do not interfere: final 'count' values are isolated per task.</p>"},{"location":"reference/testing/#test_metrics_dedup_persists_across_coord_restart","title":"test_metrics_dedup_persists_across_coord_restart","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_streaming_sync.py:567</code></p> <p>Metric deduplication survives a coordinator restart: duplicates of STATUS (BATCH_OK/TASK_DONE) sent during the restart must not double-count. Skips if the fixture <code>coord</code> has no <code>restart</code> method`.</p>"},{"location":"reference/testing/#test_metrics_idempotent_on_duplicate_status_events","title":"test_metrics_idempotent_on_duplicate_status_events","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_streaming_sync.py:433</code></p> <p>Duplicate STATUS events (BATCH_OK / TASK_DONE) must not double-increment aggregated metrics.</p> <p>We monkeypatch AIOKafkaProducerMock.send_and_wait to produce the same STATUS event twice for status topics. The Coordinator should deduplicate by envelope key and keep metrics stable.</p>"},{"location":"reference/testing/#test_metrics_isolation_between_tasks","title":"test_metrics_isolation_between_tasks","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_streaming_sync.py:388</code></p> <p>Two back-to-back tasks must keep metric aggregation isolated per task document.</p>"},{"location":"reference/testing/#test_metrics_multistream_exact_sum","title":"test_metrics_multistream_exact_sum","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_streaming_sync.py:300</code></p> <p>Three upstreams -&gt; one downstream: analyzer's aggregated 'count' must equal the sum of all totals.</p>"},{"location":"reference/testing/#test_metrics_partial_batches_exact_count","title":"test_metrics_partial_batches_exact_count","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_streaming_sync.py:347</code></p> <p>With a remainder in the last upstream batch, analyzer's 'count' must still exactly equal total.</p>"},{"location":"reference/testing/#test_metrics_single_stream_exact_count","title":"test_metrics_single_stream_exact_count","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_streaming_sync.py:259</code></p> <p>Single upstream -&gt; single downstream: analyzer's aggregated 'count' must equal the total items.</p>"},{"location":"reference/testing/#test_multistream_fanin_stream_to_one_downstream","title":"test_multistream_fanin_stream_to_one_downstream","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_streaming_sync.py:188</code></p> <p>Multi-stream fan-in: three upstream indexers stream into one analyzer. Analyzer should start early on first batch and eventually see the full flow.</p>"},{"location":"reference/testing/#test_start_when_first_batch_starts_early","title":"test_start_when_first_batch_starts_early","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_streaming_sync.py:76</code></p> <p>Downstream should start while upstream is still running when <code>start_when=first_batch</code> is set.</p>"},{"location":"reference/testing/#test_status_out_of_order_does_not_break_aggregation","title":"test_status_out_of_order_does_not_break_aggregation","text":"<p>marks: <code>asyncio</code> \u2022 location: <code>tests/test_streaming_sync.py:494</code></p> <p>BATCH_OK STATUS events arrive out of order \u2192 the aggregated metric remains correct. We hold the first BATCH_OK so later ones arrive first.</p>"},{"location":"reference/worker/","title":"Worker API Reference","text":""},{"location":"reference/worker/#worker-class","title":"Worker Class","text":""},{"location":"reference/worker/#flowkit.worker.runner.Worker","title":"flowkit.worker.runner.Worker","text":"<pre><code>Worker(*, db, cfg: WorkerConfig | None = None, clock: Clock | None = None, roles: list[str] | None = None, handlers: dict[str, RoleHandler] | None = None)\n</code></pre> <p>Stream-aware worker with cooperative cancellation and resilient batching. - Kafka I/O (producers/consumers per role) - Discovery (TASK_DISCOVER \u2192 TASK_SNAPSHOT) - Control-plane CANCEL via signals topic - DB-backed state for resume/takeover</p> Source code in <code>src/flowkit/worker/runner.py</code> <pre><code>def __init__(\n    self,\n    *,\n    db,\n    cfg: WorkerConfig | None = None,\n    clock: Clock | None = None,\n    roles: list[str] | None = None,\n    handlers: dict[str, RoleHandler] | None = None,\n) -&gt; None:\n    self.db = db\n    self.cfg = copy.deepcopy(cfg) if cfg is not None else WorkerConfig.load()\n    if roles:\n        self.cfg.roles = list(roles)\n    self.clock: Clock = clock or SystemClock()\n\n    # identity\n    self.worker_id = self.cfg.worker_id or f\"w-{uuid.uuid4().hex[:8]}\"\n    self.worker_version = self.cfg.worker_version\n\n    # adapters registry\n    self.input_adapters = build_input_adapters(db=db, clock=self.clock, cfg=self.cfg)\n\n    # handlers registry (user injects custom handlers; Echo kept as example)\n    self.handlers: dict[str, RoleHandler] = handlers or {}\n    if \"echo\" in (self.cfg.roles or []):\n        self.handlers.setdefault(\"echo\", EchoHandler())\n\n    # Kafka\n    self._producer: AIOKafkaProducer | None = None\n    self._cmd_consumers: dict[str, AIOKafkaConsumer] = {}\n    self._query_consumer: AIOKafkaConsumer | None = None\n    self._signals_consumer: AIOKafkaConsumer | None = None\n\n    # run-state\n    self._busy = False\n    self._busy_lock = asyncio.Lock()\n    self._cancel_flag = asyncio.Event()\n    self._cancel_meta: dict[str, Any] = {\"reason\": None, \"deadline_ts_ms\": None}\n    self._stopping = False\n\n    self.state = LocalStateManager(db=self.db, clock=self.clock, worker_id=self.worker_id)\n    self.active: ActiveRun | None = None\n\n    # dedup of command envelopes\n    self._dedup: OrderedDict[str, int] = OrderedDict()\n    self._dedup_lock = asyncio.Lock()\n\n    self._main_tasks: set[asyncio.Task] = set()\n\n    # logging\n    self.log = get_logger(\"worker\")\n    bind_context(role=\"worker\", worker_id=self.worker_id, version=self.worker_version)\n    self.log.debug(\"worker.init\", event=\"worker.init\", roles=self.cfg.roles, version=self.worker_version)\n</code></pre>"},{"location":"reference/worker/#flowkit.worker.runner.Worker-functions","title":"Functions","text":""},{"location":"reference/worker/#flowkit.worker.runner.Worker.start","title":"start  <code>async</code>","text":"<pre><code>start() -&gt; None\n</code></pre> Source code in <code>src/flowkit/worker/runner.py</code> <pre><code>async def start(self) -&gt; None:\n    # Helpful config print (silent by default unless tests enable stdout)\n    cfg_dump: Any\n    if hasattr(self.cfg, \"model_dump\"):  # pydantic v2\n        cfg_dump = self.cfg.model_dump()\n    elif hasattr(self.cfg, \"dict\"):  # pydantic v1\n        cfg_dump = self.cfg.dict()\n    else:\n        cfg_dump = getattr(self.cfg, \"__dict__\", str(self.cfg))\n    self.log.debug(\"worker.start\", event=\"worker.start\", cfg=cfg_dump)\n\n    await self._ensure_indexes()\n    self._producer = AIOKafkaProducer(\n        bootstrap_servers=self.cfg.kafka_bootstrap, value_serializer=dumps, enable_idempotence=True\n    )\n    await self._producer.start()\n\n    await self.state.refresh()\n    self.active = self.state.read_active()\n    if self.active and self.active.step_type not in self.cfg.roles:\n        self.active = None\n        await self.state.write_active(None)\n\n    await self._send_announce(\n        EventKind.WORKER_ONLINE,\n        extra={\n            \"worker_id\": self.worker_id,\n            \"type\": \",\".join(self.cfg.roles),\n            \"capabilities\": {\"roles\": self.cfg.roles},\n            \"version\": self.worker_version,\n            \"capacity\": {\"tasks\": 1},\n            \"resume\": self.active.__dict__ if self.active else None,\n        },\n    )\n\n    # command consumers per role\n    for role in self.cfg.roles:\n        topic = self.cfg.topic_cmd(role)\n        c = AIOKafkaConsumer(\n            topic,\n            bootstrap_servers=self.cfg.kafka_bootstrap,\n            value_deserializer=loads,\n            enable_auto_commit=False,\n            auto_offset_reset=\"latest\",\n            group_id=f\"workers.{role}.v1\",\n        )\n        await c.start()\n        self._cmd_consumers[role] = c\n        self._spawn(self._cmd_loop(role, c))\n\n    # query consumer (discovery)\n    self._query_consumer = AIOKafkaConsumer(\n        self.cfg.topic_query,\n        bootstrap_servers=self.cfg.kafka_bootstrap,\n        value_deserializer=loads,\n        enable_auto_commit=False,\n        auto_offset_reset=\"latest\",\n        group_id=\"workers.query.v1\",\n    )\n    await self._query_consumer.start()\n    self._spawn(self._query_loop(self._query_consumer))\n\n    # signals consumer (control plane; unique group per worker)\n    self._signals_consumer = AIOKafkaConsumer(\n        self.cfg.topic_signals,\n        bootstrap_servers=self.cfg.kafka_bootstrap,\n        value_deserializer=loads,\n        enable_auto_commit=False,\n        auto_offset_reset=\"latest\",\n        group_id=f\"workers.signals.{self.worker_id}\",\n    )\n    await self._signals_consumer.start()\n    self._spawn(self._signals_loop(self._signals_consumer))\n\n    # periodic announce\n    self._spawn(self._periodic_announce())\n\n    if self.active:\n        self.log.debug(\n            \"worker.recovery_present\",\n            event=\"worker.recovery_present\",\n            task_id=self.active.task_id,\n            node_id=self.active.node_id,\n        )\n\n    self.log.debug(\"worker.started\", event=\"worker.started\", worker_id=self.worker_id)\n</code></pre>"},{"location":"reference/worker/#flowkit.worker.runner.Worker.stop","title":"stop  <code>async</code>","text":"<pre><code>stop() -&gt; None\n</code></pre> Source code in <code>src/flowkit/worker/runner.py</code> <pre><code>async def stop(self) -&gt; None:\n    self._stopping = True\n    for t in list(self._main_tasks):\n        t.cancel()\n    self._main_tasks.clear()\n\n    if self._query_consumer:\n        with swallow(\n            logger=self.log,\n            code=\"worker.query_consumer.stop\",\n            msg=\"query consumer stop failed\",\n            level=logging.WARNING,\n        ):\n            await self._query_consumer.stop()\n    if self._signals_consumer:\n        with swallow(\n            logger=self.log,\n            code=\"worker.signals_consumer.stop\",\n            msg=\"signals consumer stop failed\",\n            level=logging.WARNING,\n        ):\n            await self._signals_consumer.stop()\n    for c in self._cmd_consumers.values():\n        with swallow(\n            logger=self.log, code=\"worker.cmd_consumer.stop\", msg=\"cmd consumer stop failed\", level=logging.WARNING\n        ):\n            await c.stop()\n    self._cmd_consumers.clear()\n\n    if self._producer:\n        with swallow(\n            logger=self.log, code=\"worker.announce.offline\", msg=\"announce offline failed\", level=logging.WARNING\n        ):\n            await self._send_announce(EventKind.WORKER_OFFLINE, extra={\"worker_id\": self.worker_id})\n        with swallow(\n            logger=self.log, code=\"worker.producer.stop\", msg=\"producer stop failed\", level=logging.WARNING\n        ):\n            await self._producer.stop()\n    self._producer = None\n</code></pre>"},{"location":"reference/worker/#handlers","title":"Handlers","text":""},{"location":"reference/worker/#flowkit.worker.handlers.base.RoleHandler","title":"flowkit.worker.handlers.base.RoleHandler","text":""},{"location":"reference/worker/#flowkit.worker.handlers.base.Batch","title":"flowkit.worker.handlers.base.Batch","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"reference/worker/#flowkit.worker.handlers.base.BatchResult","title":"flowkit.worker.handlers.base.BatchResult","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"reference/worker/#worker-context","title":"Worker Context","text":""},{"location":"reference/worker/#flowkit.worker.context","title":"flowkit.worker.context","text":""},{"location":"reference/worker/#flowkit.worker.context-classes","title":"Classes","text":""},{"location":"reference/worker/#flowkit.worker.context.RunContext","title":"RunContext","text":"<pre><code>RunContext(*, cancel_flag: Event, cancel_meta: dict[str, Any], artifacts_writer, clock: Clock, task_id: str, node_id: str, attempt_epoch: int, worker_id: str)\n</code></pre> <p>Cancellation-aware runtime utilities for handlers: - shared cancel flag + meta (reason, deadline ts) - cancellable awaits - subprocess group termination with escalation - resource cleanup registry</p> Source code in <code>src/flowkit/worker/context.py</code> <pre><code>def __init__(\n    self,\n    *,\n    cancel_flag: asyncio.Event,\n    cancel_meta: dict[str, Any],\n    artifacts_writer,\n    clock: Clock,\n    task_id: str,\n    node_id: str,\n    attempt_epoch: int,\n    worker_id: str,\n):\n    self._cancel_flag = cancel_flag\n    self._cancel_meta = cancel_meta\n    self.artifacts = artifacts_writer\n    self.clock = clock\n\n    self.task_id = task_id\n    self.node_id = node_id\n    self.attempt_epoch = attempt_epoch\n    self.worker_id = worker_id\n\n    self.kv: dict[str, Any] = {}\n    self._cleanup_callbacks: list[Callable[[], Any]] = []\n    self._subprocesses: list[Any] = []\n    self._temp_paths: list[str] = []\n</code></pre>"},{"location":"reference/worker/#worker-state","title":"Worker State","text":""},{"location":"reference/worker/#flowkit.worker.state","title":"flowkit.worker.state","text":""},{"location":"reference/worker/#flowkit.worker.state-classes","title":"Classes","text":""},{"location":"reference/worker/#flowkit.worker.state.LocalStateManager","title":"LocalStateManager","text":"<pre><code>LocalStateManager(*, db, clock: Clock, worker_id: str)\n</code></pre> <p>DB-backed state manager for crash-resume and cross-host takeover.</p> Source code in <code>src/flowkit/worker/state.py</code> <pre><code>def __init__(self, *, db, clock: Clock, worker_id: str) -&gt; None:\n    self.db = db\n    self.clock = clock\n    self.worker_id = worker_id\n    self._lock = asyncio.Lock()\n    self._cache: dict[str, Any] = {}\n</code></pre>"},{"location":"reference/worker/#flowkit.worker.state.LocalStateManager-functions","title":"Functions","text":""},{"location":"reference/worker/#flowkit.worker.state.LocalStateManager.refresh","title":"refresh  <code>async</code>","text":"<pre><code>refresh() -&gt; None\n</code></pre> <p>Pull the latest state from DB into in-memory cache.</p> Source code in <code>src/flowkit/worker/state.py</code> <pre><code>async def refresh(self) -&gt; None:\n    \"\"\"Pull the latest state from DB into in-memory cache.\"\"\"\n    doc = await self.db.worker_state.find_one({\"_id\": self.worker_id})\n    self._cache = doc or {}\n</code></pre>"},{"location":"reference/worker/#flowkit.worker.state.LocalStateManager.read_active","title":"read_active","text":"<pre><code>read_active() -&gt; ActiveRun | None\n</code></pre> <p>Read active run from cache.</p> Source code in <code>src/flowkit/worker/state.py</code> <pre><code>def read_active(self) -&gt; ActiveRun | None:\n    \"\"\"Read active run from cache.\"\"\"\n    d = (self._cache or {}).get(\"active_run\")\n    return ActiveRun(**d) if d else None\n</code></pre>"},{"location":"reference/worker/#flowkit.worker.state.LocalStateManager.write_active","title":"write_active  <code>async</code>","text":"<pre><code>write_active(ar: ActiveRun | None) -&gt; None\n</code></pre> <p>Atomically persist active run to DB and update cache.</p> Source code in <code>src/flowkit/worker/state.py</code> <pre><code>async def write_active(self, ar: ActiveRun | None) -&gt; None:\n    \"\"\"Atomically persist active run to DB and update cache.\"\"\"\n    async with self._lock:\n        payload = {\n            \"active_run\": asdict(ar) if ar else None,\n            \"updated_at\": self.clock.now_dt(),  # must be datetime for TTL index\n        }\n        await self.db.worker_state.update_one(\n            {\"_id\": self.worker_id},\n            {\"$set\": payload},\n            upsert=True,\n        )\n        self._cache.update(payload)\n</code></pre>"},{"location":"reference/worker/#artifacts","title":"Artifacts","text":""},{"location":"reference/worker/#flowkit.worker.artifacts","title":"flowkit.worker.artifacts","text":""}]}